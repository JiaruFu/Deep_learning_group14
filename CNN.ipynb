{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import re \n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Need to load the large model to get the vectors\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# read the text file and add the column names\n",
    "read_file = pd.read_csv(r\"booksummaries.txt\", sep='\t', header=None)\n",
    "read_file.columns = ['ID', 'm number', 'book name', 'author name', 'date', 'label', 'summary']\n",
    "\n",
    "# clean data\n",
    "read_file['label'] = read_file['label'].str.replace(r'/m/\\S*\\s', '')\n",
    "read_file['label'] = read_file['label'].str.replace(r'{', '')\n",
    "read_file['label'] = read_file['label'].str.replace(r'}', '')\n",
    "read_file['label'] = read_file['label'].str.replace(r'}', '')\n",
    "read_file['label'] = read_file['label'].str.replace(r'\\\\u00e0\\s+clef', '')\n",
    "\n",
    "# select columns\n",
    "new_file = read_file.loc[:, ['book name', 'label', 'summary']]\n",
    "\n",
    "#delete the columns with no labels\n",
    "new_file.dropna(axis = 0, how = 'any', inplace = True)\n",
    "new_file = new_file.iloc[:, [0, 2, 1]]\n",
    "\n",
    "new_file = new_file.reset_index(drop=True)\n",
    "\n",
    "#output data as csv\n",
    "new_file.to_csv(r'./booksummries.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book name</th>\n",
       "      <th>summary</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Animal Farm</td>\n",
       "      <td>Old Major, the old boar on the Manor Farm, ca...</td>\n",
       "      <td>\"\"Roman \", \"\"Satire\", \"\"Children's literature\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Clockwork Orange</td>\n",
       "      <td>Alex, a teenager living in near-future Englan...</td>\n",
       "      <td>\"\"Science Fiction\", \"\"Novella\", \"\"Speculative ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Plague</td>\n",
       "      <td>The text of The Plague is divided into five p...</td>\n",
       "      <td>\"\"Existentialism\", \"\"Fiction\", \"\"Absurdist fic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Fire Upon the Deep</td>\n",
       "      <td>The novel posits that space around the Milky ...</td>\n",
       "      <td>\"\"Hard science fiction\", \"\"Science Fiction\", \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All Quiet on the Western Front</td>\n",
       "      <td>The book tells the story of Paul Bäumer, a Ge...</td>\n",
       "      <td>\"\"War novel\", \"\"Roman \"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        book name  \\\n",
       "0                     Animal Farm   \n",
       "1              A Clockwork Orange   \n",
       "2                      The Plague   \n",
       "3            A Fire Upon the Deep   \n",
       "4  All Quiet on the Western Front   \n",
       "\n",
       "                                             summary  \\\n",
       "0   Old Major, the old boar on the Manor Farm, ca...   \n",
       "1   Alex, a teenager living in near-future Englan...   \n",
       "2   The text of The Plague is divided into five p...   \n",
       "3   The novel posits that space around the Milky ...   \n",
       "4   The book tells the story of Paul Bäumer, a Ge...   \n",
       "\n",
       "                                               label  \n",
       "0  \"\"Roman \", \"\"Satire\", \"\"Children's literature\"...  \n",
       "1  \"\"Science Fiction\", \"\"Novella\", \"\"Speculative ...  \n",
       "2  \"\"Existentialism\", \"\"Fiction\", \"\"Absurdist fic...  \n",
       "3  \"\"Hard science fiction\", \"\"Science Fiction\", \"...  \n",
       "4                            \"\"War novel\", \"\"Roman \"  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(label_list):\n",
    "    has_fiction = False\n",
    "    has_spec_fiction = False\n",
    "    has_novel = False\n",
    "    has_spec_novel = False\n",
    "    for i in range(len(label_list)):\n",
    "        if 'novel' in label_list[i].lower():\n",
    "            if 'novel' == label_list[i].lower():\n",
    "                has_novel = True\n",
    "            else:\n",
    "                has_spec_novel = True\n",
    "        if 'fiction' in label_list[i].lower():\n",
    "            if 'fiction' == label_list[i].lower():\n",
    "                has_fiction = True\n",
    "            else:\n",
    "                has_spec_fiction = True\n",
    "        \n",
    "    if has_spec_fiction and has_spec_novel:\n",
    "        if has_fiction:\n",
    "            label_list.remove('fiction')\n",
    "        if has_novel:\n",
    "            label_list.remove('novel')\n",
    "    elif has_spec_fiction:\n",
    "        if has_fiction:\n",
    "            label_list.remove('fiction')\n",
    "        if has_novel:\n",
    "            label_list.remove('novel')\n",
    "    elif has_spec_novel:\n",
    "        if has_fiction:\n",
    "            label_list.remove('fiction')\n",
    "        if has_novel:\n",
    "            label_list.remove('novel')\n",
    "    elif has_fiction and has_novel:\n",
    "        label_list.remove('fiction')\n",
    "    return label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(new_file['label'])):\n",
    "    label = new_file['label'][index].replace('\"', ''). lower()\n",
    "    label_list = re.split(', ', label)  \n",
    "    label_list = text_process(label_list)\n",
    "    new_file.xs(index)['label']= label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output data as csv\n",
    "new_file.to_csv(r'./booksummries.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book name</th>\n",
       "      <th>summary</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12836</th>\n",
       "      <td>The Third Lynx</td>\n",
       "      <td>The story starts with former government agent...</td>\n",
       "      <td>[science fiction]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12837</th>\n",
       "      <td>Remote Control</td>\n",
       "      <td>The series follows the character of Nick Ston...</td>\n",
       "      <td>[thriller, fiction, suspense]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12838</th>\n",
       "      <td>Transfer of Power</td>\n",
       "      <td>The reader first meets Rapp while he is doing...</td>\n",
       "      <td>[thriller, fiction]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12839</th>\n",
       "      <td>Decoded</td>\n",
       "      <td>The book follows very rough chronological ord...</td>\n",
       "      <td>[autobiography]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12840</th>\n",
       "      <td>Poor Folk</td>\n",
       "      <td>Makar Devushkin and Varvara Dobroselova are s...</td>\n",
       "      <td>[epistolary novel, speculative fiction]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               book name                                            summary  \\\n",
       "12836     The Third Lynx   The story starts with former government agent...   \n",
       "12837     Remote Control   The series follows the character of Nick Ston...   \n",
       "12838  Transfer of Power   The reader first meets Rapp while he is doing...   \n",
       "12839            Decoded   The book follows very rough chronological ord...   \n",
       "12840          Poor Folk   Makar Devushkin and Varvara Dobroselova are s...   \n",
       "\n",
       "                                         label  \n",
       "12836                        [science fiction]  \n",
       "12837            [thriller, fiction, suspense]  \n",
       "12838                      [thriller, fiction]  \n",
       "12839                          [autobiography]  \n",
       "12840  [epistolary novel, speculative fiction]  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_file.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Encoding the Labels##\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = []\n",
    "for index in range(len(new_file['label'])):\n",
    "    object_label = new_file['label'][index]\n",
    "    for l in object_label:\n",
    "        if l not in label_list:\n",
    "            label_list.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = pd.DataFrame(np.zeros((12841, 227)), columns=label_list).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(new_file['label'])):\n",
    "    object_label = new_file['label'][index]\n",
    "    for l in object_label:\n",
    "        one_hot[l][index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roman</th>\n",
       "      <th>satire</th>\n",
       "      <th>children's literature</th>\n",
       "      <th>speculative fiction</th>\n",
       "      <th>science fiction</th>\n",
       "      <th>novella</th>\n",
       "      <th>utopian and dystopian fiction</th>\n",
       "      <th>existentialism</th>\n",
       "      <th>absurdist fiction</th>\n",
       "      <th>hard science fiction</th>\n",
       "      <th>...</th>\n",
       "      <th>encyclopedia</th>\n",
       "      <th>mashup</th>\n",
       "      <th>biopunk</th>\n",
       "      <th>popular culture</th>\n",
       "      <th>neuroscience</th>\n",
       "      <th>new york times best seller list</th>\n",
       "      <th>epic science fiction and fantasy</th>\n",
       "      <th>alien invasion</th>\n",
       "      <th>prose</th>\n",
       "      <th>pastiche</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12836</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12837</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12838</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12839</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12840</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12841 rows × 227 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       roman   satire  children's literature  speculative fiction  \\\n",
       "0           1       1                      1                    1   \n",
       "1           0       1                      0                    1   \n",
       "2           0       0                      0                    0   \n",
       "3           0       0                      0                    1   \n",
       "4           1       0                      0                    0   \n",
       "...       ...     ...                    ...                  ...   \n",
       "12836       0       0                      0                    0   \n",
       "12837       0       0                      0                    0   \n",
       "12838       0       0                      0                    0   \n",
       "12839       0       0                      0                    0   \n",
       "12840       0       0                      0                    1   \n",
       "\n",
       "       science fiction  novella  utopian and dystopian fiction  \\\n",
       "0                    0        0                              0   \n",
       "1                    1        1                              1   \n",
       "2                    0        0                              0   \n",
       "3                    1        0                              0   \n",
       "4                    0        0                              0   \n",
       "...                ...      ...                            ...   \n",
       "12836                1        0                              0   \n",
       "12837                0        0                              0   \n",
       "12838                0        0                              0   \n",
       "12839                0        0                              0   \n",
       "12840                0        0                              0   \n",
       "\n",
       "       existentialism  absurdist fiction  hard science fiction  ...  \\\n",
       "0                   0                  0                     0  ...   \n",
       "1                   0                  0                     0  ...   \n",
       "2                   1                  1                     0  ...   \n",
       "3                   0                  0                     1  ...   \n",
       "4                   0                  0                     0  ...   \n",
       "...               ...                ...                   ...  ...   \n",
       "12836               0                  0                     0  ...   \n",
       "12837               0                  0                     0  ...   \n",
       "12838               0                  0                     0  ...   \n",
       "12839               0                  0                     0  ...   \n",
       "12840               0                  0                     0  ...   \n",
       "\n",
       "       encyclopedia  mashup  biopunk  popular culture  neuroscience  \\\n",
       "0                 0       0        0                0             0   \n",
       "1                 0       0        0                0             0   \n",
       "2                 0       0        0                0             0   \n",
       "3                 0       0        0                0             0   \n",
       "4                 0       0        0                0             0   \n",
       "...             ...     ...      ...              ...           ...   \n",
       "12836             0       0        0                0             0   \n",
       "12837             0       0        0                0             0   \n",
       "12838             0       0        0                0             0   \n",
       "12839             0       0        0                0             0   \n",
       "12840             0       0        0                0             0   \n",
       "\n",
       "       new york times best seller list  epic science fiction and fantasy  \\\n",
       "0                                    0                                 0   \n",
       "1                                    0                                 0   \n",
       "2                                    0                                 0   \n",
       "3                                    0                                 0   \n",
       "4                                    0                                 0   \n",
       "...                                ...                               ...   \n",
       "12836                                0                                 0   \n",
       "12837                                0                                 0   \n",
       "12838                                0                                 0   \n",
       "12839                                0                                 0   \n",
       "12840                                0                                 0   \n",
       "\n",
       "       alien invasion  prose  pastiche  \n",
       "0                   0      0         0  \n",
       "1                   0      0         0  \n",
       "2                   0      0         0  \n",
       "3                   0      0         0  \n",
       "4                   0      0         0  \n",
       "...               ...    ...       ...  \n",
       "12836               0      0         0  \n",
       "12837               0      0         0  \n",
       "12838               0      0         0  \n",
       "12839               0      0         0  \n",
       "12840               0      0         0  \n",
       "\n",
       "[12841 rows x 227 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the words##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def words_process(new_file):\n",
    "    book_summaries = new_file['summary']\n",
    "    summary_list = [summary for summary in book_summaries]\n",
    "    summary_num = len(summary_list)\n",
    "    #summaries = ''.join(summary_list)\n",
    "    print(\"the total number of books: {}\\n\".format(summary_num))\n",
    "    \n",
    "    all_docs = []\n",
    "\n",
    "    for doc in summary_list:\n",
    "        # Tokenize the string into words\n",
    "        tokens = word_tokenize(doc)\n",
    "        # Remove non-alphabetic tokens, such as punctuation\n",
    "        words = [word.lower() for word in tokens if word.isalpha()]\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [word for word in words if not word in stop_words]\n",
    "        all_docs.append(words)\n",
    "        \n",
    "    return all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total number of books: 12841\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_words = words_process(new_file.iloc[:, :]) ####NEED TO CHANGE TO ALL!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## outliners ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Pre-Trained Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Word2Vec loading capabilities\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Creating the model\n",
    "embed_lookup = KeyedVectors.load_word2vec_format('word2vec_model/word2vec-slim/GoogleNews-vectors-negative300.bin', \n",
    "                                                 binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store pretrained vocab\n",
    "pretrained_words = []\n",
    "for word in embed_lookup.vocab:\n",
    "    pretrained_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocab: 3000000\n",
      "\n",
      "Word in vocab: in\n",
      "\n",
      "Length of embedding: 300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row_idx = 1\n",
    "\n",
    "# get word/embedding in that row\n",
    "word = pretrained_words[row_idx] # get words by index\n",
    "embedding = embed_lookup[word] # embeddings by word\n",
    "\n",
    "# vocab and embedding info\n",
    "print(\"Size of Vocab: {}\\n\".format(len(pretrained_words)))\n",
    "print('Word in vocab: {}\\n'.format(word))\n",
    "print('Length of embedding: {}\\n'.format(len(embedding)))\n",
    "#print('Associated embedding: \\n', embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_all_sum(embed_lookup):\n",
    "    tokenized_sum = []\n",
    "    for summ in all_words:\n",
    "        ints = []\n",
    "        for w in summ:\n",
    "            try:\n",
    "                idx = embed_lookup.vocab[w].index\n",
    "            except: \n",
    "                idx = 0\n",
    "            ints.append(idx)\n",
    "        tokenized_sum.append(ints)\n",
    "    return tokenized_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sum = tokenize_all_sum(embed_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[154, 338, 154, 86316, 63917, 2563, 926, 2418, 2563, 349, 8691, 5103, 32366, 7328, 2418, 8709, 2216, 146659, 338, 6119, 54, 533, 13250, 37391, 517953, 4315, 3929, 749, 2768, 5868, 2418, 14076, 817, 18500, 11132, 94620, 131280, 2563, 35097, 2801, 2563, 4922, 375, 60262, 396, 2418, 3325, 37391, 2687, 4016, 2418, 1772, 2009, 560, 18855, 2563, 623, 9841, 13250, 18603, 1784, 1274, 211, 3125, 605, 560, 1281, 22341, 764, 492, 517953, 920, 38308, 2563, 2766, 5325, 9223, 517953, 37391, 2894, 1274, 37391, 5513, 383, 857, 45282, 517953, 2766, 5148, 37391, 309, 15156, 614, 517953, 75114, 726, 6481, 2351, 2563, 5226, 1778, 956, 13250, 209, 2563, 527, 533, 13283, 945, 798371, 34858, 517953, 1059, 912, 45282, 931, 2418, 141, 3437, 3102, 2200, 870, 45282, 3148, 1896, 2418, 359, 45282, 66096, 517953, 798371, 5823, 2418, 37391, 3016, 1021, 34168, 0, 2470, 3064, 4406, 5160, 37391, 2540, 29160, 517953, 2219, 50487, 2563, 2766, 1423, 2418, 11092, 174391, 154, 2318, 13250, 2258, 325, 9174, 509, 36632, 12891, 33177, 576, 0, 37391, 66524, 517953, 798371, 29457, 274, 489, 517953, 638, 155, 13250, 28470, 375, 60262, 1090552, 1096, 146659, 2588, 20467, 66524, 517953, 1530, 9224, 4734, 251, 2418, 872, 4233, 254, 94620, 131280, 798371, 8622, 2418, 1268, 4829, 87799, 1052, 261, 2249, 94620, 1868397, 45, 0, 2470, 1020, 2563, 527, 16778, 11860, 3252, 5757, 45282, 459, 2418, 202, 1508, 267, 370, 131, 143, 13959, 32408, 2691, 865, 1481, 13959, 1206, 345, 3437, 3437, 20628, 345, 45282, 517953, 6560, 5040, 142, 13959, 16356, 11075, 6553, 254, 643, 482, 971843, 15879, 30560, 75, 1025, 112, 13283, 9616, 5040, 8314, 1018880, 2687, 9117, 2381, 2418, 2687, 20585, 798371, 613, 5040, 2793, 825, 2009, 721, 1140, 59619, 21980, 7013, 13959, 570, 1157, 201, 788, 643, 77, 901, 13250, 1234, 1505, 16856, 1635, 36992, 3015, 3874, 375, 60262, 1823, 649, 7880, 2418, 3325, 2418, 3325, 488, 517953, 2237, 2656, 447, 13250, 5103, 272, 12870, 517953, 328, 5554, 2418, 198, 517953, 5513, 4074, 5103, 0, 2251, 9539, 129908, 2240, 8816, 734, 6372, 726, 521, 2563, 63917, 2563, 2418, 127255, 3399, 2432, 1608, 13250, 3347, 2192, 8136, 624, 3371, 3853, 517953, 94620, 0, 2418, 0, 1608, 13250, 329, 87, 1608, 5103, 45, 888, 1567, 13250, 37391, 517953, 798371, 8448, 154, 338, 2122, 2769, 5868, 5462, 521, 1090552, 774, 517953, 798371, 19381, 51939, 5103, 3273, 2330, 6911, 7676, 832, 798371, 4578, 9240, 375, 60262, 1201, 817491, 61536, 167332, 121, 23141, 576, 555, 2437, 509, 68, 6518, 1979, 1412, 60262, 2341, 1047, 1472, 54, 4237, 4980, 2341, 1047, 1472, 134, 4237, 8933, 1269, 2801, 4034, 3015, 3874, 2801, 4034, 3629, 3126, 2801, 4034, 3554, 2330, 2801, 4034, 2176, 2801, 2418, 3325, 334, 517953, 13250, 11693, 8243, 60262, 645, 6157, 2801, 4034, 3554, 2330, 4668, 84951, 2801, 4034, 3629, 3126, 10383, 264, 1327, 60262, 3120, 726, 245227, 2801, 4034, 3629, 3126, 10383, 2801, 4034, 3554, 2330, 4668, 2801, 4034, 2176, 2801, 294, 953, 1702, 2588, 161863, 2418, 3325, 2418, 3325, 488, 134, 4237, 127, 54, 4237, 254, 13250, 411, 1275, 14156, 8127, 1412, 2578, 375, 60262, 2402, 350, 555, 405, 2801, 2563, 26608, 2418, 526, 5103, 1559, 2418, 486, 5103, 6040, 8053, 12207, 60262, 0, 7541, 1197, 450, 44923, 760, 72489, 9792]\n"
     ]
    }
   ],
   "source": [
    "# testing code and printing a tokenized review\n",
    "print(tokenized_sum[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2975"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_Length = 0  \n",
    "for x in tokenized_sum:\n",
    "    if len(x) > max_Length:\n",
    "        max_Length = len(x)\n",
    "max_Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(tokenized_sum, seq_length):\n",
    "    \n",
    "    # getting the correct rows x cols shape\n",
    "    features = np.zeros((len(tokenized_sum), seq_length), dtype=int)\n",
    " \n",
    "    # for each review, I grab that review and \n",
    "    for i, row in enumerate(tokenized_sum):\n",
    "        if len(row)>0:\n",
    "            features[i, -len(row):] =  np.array(row)[:seq_length]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     0     0 ... 13421 40364  5103]\n"
     ]
    }
   ],
   "source": [
    "features = pad_features(tokenized_sum, max_Length)\n",
    "\n",
    "assert len(features)==len(tokenized_sum), \"Features should have as many rows as reviews.\"\n",
    "assert len(features[0])==max_Length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "print(features[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training, Validation, and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(10272, 2975) \n",
      "Validation set: \t(1284, 2975) \n",
      "Test set: \t\t(1285, 2975)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(features)*split_frac)\n",
    "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
    "train_y, remaining_y = one_hot[:split_idx], one_hot[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "\n",
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above only 2,0,1 because I only used  3 datasample to do all of these"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoaders and Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(np.array(train_x)), torch.from_numpy(np.array(train_y)[:,0]))\n",
    "valid_data = TensorDataset(torch.from_numpy(np.array(val_x)), torch.from_numpy(np.array(val_y)))\n",
    "test_data = TensorDataset(torch.from_numpy(np.array(test_x)), torch.from_numpy(np.array(test_y)))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "RANDOM_SEED = 1\n",
    "NUM_EPOCHS = 10\n",
    "NUM_CLASSES = 227\n",
    "GRAYSCALE = False\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# shuffling and batching data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10272, 2975)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10272,)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_y)[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU.\n"
     ]
    }
   ],
   "source": [
    "#First checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SentimentCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The embedding layer + CNN model that will be used to perform sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_model, vocab_size, output_size, embedding_dim,\n",
    "                 num_filters=100, kernel_sizes=[3, 4, 5], freeze_embeddings=True, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentCNN, self).__init__()\n",
    "\n",
    "        # set class vars\n",
    "        self.num_filters = num_filters\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # 1. embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # set weights to pre-trained\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(embed_model.vectors)) # all vectors\n",
    "        # (optional) freeze embedding weights\n",
    "        if freeze_embeddings:\n",
    "            self.embedding.requires_grad = False\n",
    "        \n",
    "        # 2. convolutional layers\n",
    "        self.convs_1d = nn.ModuleList([\n",
    "            nn.Conv2d(1, num_filters, (k, embedding_dim), padding=(k-2,0)) \n",
    "            for k in kernel_sizes])\n",
    "        \n",
    "        # 3. final, fully-connected layer for classification\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * num_filters, output_size) \n",
    "        \n",
    "        # 4. dropout and sigmoid layers\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    def conv_and_pool(self, x, conv):\n",
    "        \"\"\"\n",
    "        Convolutional + max pooling layer\n",
    "        \"\"\"\n",
    "        # squeeze last dim to get size: (batch_size, num_filters, conv_seq_length)\n",
    "        # conv_seq_length will be ~ 200\n",
    "        x = F.relu(conv(x)).squeeze(3)\n",
    "        \n",
    "        # 1D pool over conv_seq_length\n",
    "        # squeeze to get size: (batch_size, num_filters)\n",
    "        x_max = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x_max\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines how a batch of inputs, x, passes through the model layers.\n",
    "        Returns a single, sigmoid-activated class score as output.\n",
    "        \"\"\"\n",
    "        # embedded vectors\n",
    "        embeds = self.embedding(x) # (batch_size, seq_length, embedding_dim)\n",
    "        # embeds.unsqueeze(1) creates a channel dimension that conv layers expect\n",
    "        embeds = embeds.unsqueeze(1)\n",
    "        \n",
    "        # get output of each conv-pool layer\n",
    "        conv_results = [self.conv_and_pool(embeds, conv) for conv in self.convs_1d]\n",
    "        \n",
    "        # concatenate results and add dropout\n",
    "        x = torch.cat(conv_results, 1)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # final logit\n",
    "        logit = self.fc(x) \n",
    "        probas = F.softmax(logit, dim=1)\n",
    "       \n",
    "        # sigmoid-activated --> a class score\n",
    "        return self.sig(logit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "\n",
    "vocab_size = len(pretrained_words)\n",
    "output_size = 1 # binary class (1 or 0)\n",
    "embedding_dim = len(embed_lookup[pretrained_words[0]]) # 300-dim vectors\n",
    "num_filters = 100\n",
    "kernel_sizes = [3, 4, 5]\n",
    "\n",
    "net = SentimentCNN(embed_lookup, vocab_size, output_size, embedding_dim,\n",
    "                   num_filters, kernel_sizes)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train(net, train_loader, epochs, print_every=100):\n",
    "\n",
    "    # move model to GPU, if available\n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "\n",
    "    counter = 0 # for printing\n",
    "    \n",
    "    # train for some number of epochs\n",
    "    net.train()\n",
    "    for e in range(epochs):\n",
    "\n",
    "        # batch loop\n",
    "        for inputs, labels in train_loader:\n",
    "            counter += 1\n",
    "\n",
    "            if(train_on_gpu):\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "\n",
    "            # get the output from the model\n",
    "            logit = net(inputs)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(logit.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for inputs, labels in valid_loader:\n",
    "\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                    output = net(inputs)\n",
    "                    val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                net.train()\n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training params\n",
    "\n",
    "epochs = 2 # this is approx where I noticed the validation loss stop decreasing\n",
    "print_every = 100\n",
    "\n",
    "train(net, train_loader, epochs, print_every=print_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
