{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# read the text file and add the column names\n",
    "read_file = pd.read_csv(r\"./booksummries.txt\", sep='\t', header=None)\n",
    "read_file.columns = ['ID', 'm number', 'book name', 'author name', 'date', 'label', 'summary']\n",
    "\n",
    "# clean data\n",
    "read_file['label'] = read_file['label'].str.replace(r'/m/\\S*\\s', '')\n",
    "read_file['label'] = read_file['label'].str.replace(r'{', '')\n",
    "read_file['label'] = read_file['label'].str.replace(r'}', '')\n",
    "read_file['label'] = read_file['label'].str.replace(r'}', '')\n",
    "read_file['label'] = read_file['label'].str.replace(r'\\\\u00e0\\s+clef', '')\n",
    "\n",
    "# select columns\n",
    "new_file = read_file.loc[:, ['book name', 'label', 'summary']]\n",
    "\n",
    "#delete the columns with no labels\n",
    "new_file.dropna(axis = 0, how = 'any', inplace = True)\n",
    "new_file = new_file.iloc[:, [0, 2, 1]]\n",
    "\n",
    "#output data as csv\n",
    "new_file.to_csv(r'./booksummries.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return a.dot(b)/np.sqrt(a.dot(a) * b.dot(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculation(label_list):\n",
    "    for i in range(len(label_list)):\n",
    "        j = i+1\n",
    "        if (j < len(label_list)):\n",
    "            for j in range(len(label_list)):\n",
    "                a = nlp(label_list[i]).vector\n",
    "                b = nlp(label_list[j]).vector\n",
    "                if cosine_similarity(a, b)>0.6:\n",
    "                        if(len(label_list[i])>len(label_list[j])):\n",
    "                            label_list[i] = label_list[j]\n",
    "                        else:\n",
    "                            label_list[j] = label_list[i]\n",
    "    return set(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file[\"label\"] = read_file[\"label\"].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for index in range(len(read_file['label'])):\n",
    "    label = read_file['label'][index].replace('\"', ''). lower()\n",
    "    label_list = re.split(', ', label)  \n",
    "    label_list = calculation(label_list)\n",
    "    read_file.xs(index)['label']= label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def words_process(new_file):\n",
    "    book_summaries = new_file['summary']\n",
    "    summary_list = [summary for summary in book_summaries]\n",
    "    summary_num = len(summary_list)\n",
    "    #summaries = ''.join(summary_list)\n",
    "    print(\"the total number of books: {}\\n\".format(summary_num))\n",
    "    \n",
    "    all_docs = []\n",
    "\n",
    "    for doc in summary_list:\n",
    "        # Tokenize the string into words\n",
    "        tokens = word_tokenize(doc)\n",
    "        # Remove non-alphabetic tokens, such as punctuation\n",
    "        words = [word.lower() for word in tokens if word.isalpha()]\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [word for word in words if not word in stop_words]\n",
    "        all_docs.append(words)\n",
    "        \n",
    "    return all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_process(new_file.iloc[1:3, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mehtod1: Using the pre-trained model to obtain the vectors of words and averaging all the words in each summary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load word2vec model (trained on an enormous Google corpus)\n",
    "google_vecs = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)\n",
    "vec_dim = google_vecs.vector_size\n",
    "\n",
    "def words_embedding(words):\n",
    "    \n",
    "\n",
    "    # Filter the list of vectors to include only those that Word2Vec has a vector for\n",
    "    vector_list = [google_vecs[word] for word in words if word in google_vecs.vocab]\n",
    "\n",
    "    # Create a list of the words corresponding to these vectors\n",
    "    words_filtered = [word for word in words if word in google_vecs.vocab]\n",
    "\n",
    "    #Zip the words together with their vector representations\n",
    "    word_vec_zip = zip(words_filtered, vector_list)\n",
    "\n",
    "    # Cast to a dict so we can turn it into a DataFrame\n",
    "    word_vec_dict = dict(word_vec_zip)\n",
    "    word_vec = pd.DataFrame.from_dict(word_vec_dict, orient='index')\n",
    "    \n",
    "    return word_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_embedding(all_docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def avg_doc_embedding(all_docs):\n",
    "    doc_vec = []\n",
    "    for doc in all_docs:\n",
    "        if len(doc) != 0:\n",
    "            word_vec =  word_embedding(doc)\n",
    "            doc_vec.append(np.mean(np.array(word_vec), axis=1))\n",
    "    \n",
    "    summary_vec = np.array(doc_vec)\n",
    "    return doc_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_doc_embedding(all_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method2: Using Word2Vec to train all the words and obtain the vectors. Then combine the TF-DIF model to obtain the weighted vector.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import sys\n",
    "from gensim.models import Word2Vec\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise.\"\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "workers = multiprocessing.cpu_count()\n",
    "print('number of cpu: {}'.format(workers))\n",
    "\n",
    "word_model = Word2Vec(all_docs,\n",
    "                      min_count=2,\n",
    "                      size=300,\n",
    "                      window=5,\n",
    "                      workers=workers,\n",
    "                      iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "all_text_docs = []\n",
    "for doc in all_docs:\n",
    "    #print(len(doc))\n",
    "    all_text_docs.append(\" \". join(doc))\n",
    "tf_idf_vect = TfidfVectorizer(stop_words='english', max_features=6000)\n",
    "final_tf_idf = tf_idf_vect.fit_transform(all_text_docs)\n",
    "tfidf_feat = tf_idf_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_doc_vectors_ft = [] # the tfidf-ft for each summary  is stored in this list\n",
    "row=0\n",
    "errors=0\n",
    "for doc in all_docs: # for each summary \n",
    "    doc_vec = np.zeros(300) # as word vectors are of zero length\n",
    "    weight_sum =0; # num of words with a valid vector in the summary \n",
    "    for word in doc: # for each word in a summary \n",
    "        try:\n",
    "            word_vec = word_model.wv[word]\n",
    "            # obtain the tf_idf of a word in a summary \n",
    "            tfidf = final_tf_idf[row, tfidf_feat.index(word)]\n",
    "            print(tfidf)\n",
    "            doc_vec += (word_vec * tfidf)\n",
    "            weight_sum += tfidf\n",
    "        except:\n",
    "            errors =+1\n",
    "            pass\n",
    "    doc_vec /= weight_sum\n",
    "   \n",
    "\n",
    "    tfidf_doc_vectors_ft.append(doc_vec)\n",
    "    row += 1\n",
    "print('errors noted: '+str(errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method3: Using Doc2Vec to train all the summaries and obtain the doc vectors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn import utils\n",
    "\n",
    "\n",
    "def doc2vec(all_docs, all_labels):\n",
    "    \n",
    "#     doc_labels_dict = dict(zip(all_docs, all_labels))\n",
    "#     doc_labels = pd.DataFrame.from_dict(doc_labels_dict, orient='index')\n",
    "    \n",
    "    summary_tags = []\n",
    "    for (doc, labels) in zip(all_docs, all_labels):\n",
    "         summary_tags.append(TaggedDocument(doc, labels))\n",
    " \n",
    "    doc_model = Doc2Vec(dm=1, \n",
    "                        vector_size=300, \n",
    "                        negative=5, \n",
    "                        hs=0, \n",
    "                        min_count=2, \n",
    "                        sample=0, \n",
    "                        workers=workers)\n",
    "    doc_model.build_vocab(summary_tags)\n",
    "    for epoch in range(30):\n",
    "        doc_model.train(utils.shuffle(summary_tags), total_examples=books_number, epochs=1)\n",
    "        doc_model.alpha -= 0.002\n",
    "        doc_model.min_alpha = doc_model.alpha\n",
    "        \n",
    "    \n",
    "#     sents = summary_tags.values\n",
    "    targets, doc_vec = zip(*[(doc.tags, doc_model.infer_vector(doc.words, steps=20)) for doc in summary_tags])\n",
    "    return targets, doc_vec\n",
    "\n",
    "targets, doc_vec = doc2vec(all_docs, all_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
