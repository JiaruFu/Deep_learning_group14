{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# read the text file and add the column names\n",
    "read_file = pd.read_csv(r\"./booksummries.txt\", sep='\t', header=None)\n",
    "read_file.columns = ['ID', 'm number', 'book name', 'author name', 'date', 'label', 'summary']\n",
    "\n",
    "# clean data\n",
    "read_file['label'] = read_file['label'].str.replace(r'/m/\\S*\\s', '')\n",
    "read_file['label'] = read_file['label'].str.replace(r'{', '')\n",
    "read_file['label'] = read_file['label'].str.replace(r'}', '')\n",
    "read_file['label'] = read_file['label'].str.replace(r'}', '')\n",
    "read_file['label'] = read_file['label'].str.replace(r'\\\\u00e0\\s+clef', '')\n",
    "\n",
    "# select columns\n",
    "new_file = read_file.loc[:, ['book name', 'label', 'summary']]\n",
    "\n",
    "#delete the columns with no labels\n",
    "new_file.dropna(axis = 0, how = 'any', inplace = True)\n",
    "new_file = new_file.iloc[:, [0, 2, 1]]\n",
    "\n",
    "#output data as csv\n",
    "new_file.to_csv(r'./booksummries.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return a.dot(b)/np.sqrt(a.dot(a) * b.dot(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculation(label_list):\n",
    "    for i in range(len(label_list)):\n",
    "        j = i+1\n",
    "        if (j < len(label_list)):\n",
    "            for j in range(len(label_list)):\n",
    "                a = nlp(label_list[i]).vector\n",
    "                b = nlp(label_list[j]).vector\n",
    "                if cosine_similarity(a, b)>0.6:\n",
    "                        if(len(label_list[i])>len(label_list[j])):\n",
    "                            label_list[i] = label_list[j]\n",
    "                        else:\n",
    "                            label_list[j] = label_list[i]\n",
    "    return set(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file[\"label\"] = read_file[\"label\"].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for index in range(len(read_file['label'])):\n",
    "    label = read_file['label'][index].replace('\"', ''). lower()\n",
    "    label_list = re.split(', ', label)  \n",
    "    label_list = calculation(label_list)\n",
    "    read_file.xs(index)['label']= label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def words_process(new_file):\n",
    "    book_summaries = new_file['summary']\n",
    "    summary_list = [summary for summary in book_summaries]\n",
    "    summary_num = len(summary_list)\n",
    "    #summaries = ''.join(summary_list)\n",
    "    print(\"the total number of books: {}\\n\".format(summary_num))\n",
    "    \n",
    "    all_docs = []\n",
    "\n",
    "    for doc in summary_list:\n",
    "        # Tokenize the string into words\n",
    "        tokens = word_tokenize(doc)\n",
    "        # Remove non-alphabetic tokens, such as punctuation\n",
    "        words = [word.lower() for word in tokens if word.isalpha()]\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [word for word in words if not word in stop_words]\n",
    "        all_docs.append(words)\n",
    "        \n",
    "    return all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_process(new_file.iloc[1:3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load word2vec model (trained on an enormous Google corpus)\n",
    "google_vecs = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)\n",
    "vec_dim = google_vecs.vector_size\n",
    "\n",
    "def words_embedding(words):\n",
    "    \n",
    "\n",
    "    # Filter the list of vectors to include only those that Word2Vec has a vector for\n",
    "    vector_list = [google_vecs[word] for word in words if word in google_vecs.vocab]\n",
    "\n",
    "    # Create a list of the words corresponding to these vectors\n",
    "    words_filtered = [word for word in words if word in google_vecs.vocab]\n",
    "\n",
    "    #Zip the words together with their vector representations\n",
    "    word_vec_zip = zip(words_filtered, vector_list)\n",
    "\n",
    "    # Cast to a dict so we can turn it into a DataFrame\n",
    "    word_vec_dict = dict(word_vec_zip)\n",
    "    word_vec = pd.DataFrame.from_dict(word_vec_dict, orient='index')\n",
    "    \n",
    "    return word_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_embedding(all_docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def avg_doc_embedding(all_docs):\n",
    "    doc_vec = []\n",
    "    for doc in all_docs:\n",
    "        if len(doc) != 0:\n",
    "            word_vec =  word_embedding(doc)\n",
    "            doc_vec.append(np.mean(np.array(word_vec), axis=1))\n",
    "    \n",
    "    summary_vec = np.array(doc_vec)\n",
    "    return doc_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_doc_embedding(all_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
