{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "moive_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "KyHBQ3l0p7ER"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9Da6mexrp-7L",
        "outputId": "6fa8167f-1880-47a1-d157-585b9bd2c425",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NPnQKgpbp7DO",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import re \n",
        "import math\n",
        "import time\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LxuiAKnnp7DR",
        "outputId": "1faef475-e490-4d8e-8390-bce785c262d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmXeqAOEbT-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QCC0-w4ap7DW",
        "colab": {}
      },
      "source": [
        "RANDOM_SEED = 123\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4rnUxezBRmFg",
        "outputId": "ba8bb23a-d62a-428c-a7bb-23b28a0c1e1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "DEVICE"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdZTT0ItkSSw",
        "colab_type": "code",
        "outputId": "0d16dd92-8a27-44a7-eaa0-bd883788443a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# read the text file and add the column names\n",
        "# read_file = pd.read_csv(r\"./booksummaries.txt\", sep='\t', header=None)  Colab Notebooks/\n",
        "new_file = pd.read_csv(r\"/content/drive/My Drive/final project/movies_genres_en.csv\", sep='\\t', encoding='utf-8')\n",
        "new_file = new_file.iloc[:1300, :]\n",
        "new_file.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>plot</th>\n",
              "      <th>Action</th>\n",
              "      <th>Adult</th>\n",
              "      <th>Adventure</th>\n",
              "      <th>Animation</th>\n",
              "      <th>Biography</th>\n",
              "      <th>Comedy</th>\n",
              "      <th>Crime</th>\n",
              "      <th>Documentary</th>\n",
              "      <th>Drama</th>\n",
              "      <th>Family</th>\n",
              "      <th>Fantasy</th>\n",
              "      <th>Game-Show</th>\n",
              "      <th>History</th>\n",
              "      <th>Horror</th>\n",
              "      <th>Music</th>\n",
              "      <th>Musical</th>\n",
              "      <th>Mystery</th>\n",
              "      <th>News</th>\n",
              "      <th>Reality-TV</th>\n",
              "      <th>Romance</th>\n",
              "      <th>Sci-Fi</th>\n",
              "      <th>Short</th>\n",
              "      <th>Sport</th>\n",
              "      <th>Talk-Show</th>\n",
              "      <th>Thriller</th>\n",
              "      <th>War</th>\n",
              "      <th>Western</th>\n",
              "      <th>plot_lang</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"#7DaysLater\" (2013)</td>\n",
              "      <td>#7dayslater is an interactive comedy series f...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"#BlackLove\" (2015) {Crash the Party (#1.9)}</td>\n",
              "      <td>With just one week left in the workshops, the...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"#BlackLove\" (2015) {Making Lemonade Out of Le...</td>\n",
              "      <td>All of the women start making strides towards...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"#BlackLove\" (2015) {Miss Independent (#1.5)}</td>\n",
              "      <td>All five of these women are independent and s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"#BlackLove\" (2015) {Sealing the Deal (#1.10)}</td>\n",
              "      <td>Despite having gone through a life changing p...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... plot_lang\n",
              "0                               \"#7DaysLater\" (2013)  ...        en\n",
              "1       \"#BlackLove\" (2015) {Crash the Party (#1.9)}  ...        en\n",
              "2  \"#BlackLove\" (2015) {Making Lemonade Out of Le...  ...        en\n",
              "3      \"#BlackLove\" (2015) {Miss Independent (#1.5)}  ...        en\n",
              "4     \"#BlackLove\" (2015) {Sealing the Deal (#1.10)}  ...        en\n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UyUJdT3Gp7DY",
        "colab": {}
      },
      "source": [
        "# import os\n",
        "# import pandas as pd\n",
        "\n",
        "# # read the text file and add the column names\n",
        "# # read_file = pd.read_csv(r\"./booksummaries.txt\", sep='\t', header=None)  Colab Notebooks/\n",
        "# read_file = pd.read_csv(r\"/content/drive/My Drive/Colab Notebooks/final project/booksummaries.txt\", sep='\t', header=None)\n",
        "# read_file.columns = ['ID', 'm number', 'book name', 'author name', 'date', 'label', 'summary']\n",
        "\n",
        "# # clean data\n",
        "# read_file['label'] = read_file['label'].str.replace(r'/m/\\S*\\s', '')\n",
        "# read_file['label'] = read_file['label'].str.replace(r'{', '')\n",
        "# read_file['label'] = read_file['label'].str.replace(r'}', '')\n",
        "# read_file['label'] = read_file['label'].str.replace(r'novel', 'fiction', case = False)\n",
        "# read_file['label'] = read_file['label'].str.replace(r'\\\\u00e0\\s+clef', '')\n",
        "\n",
        "# # select columns\n",
        "# new_file = read_file.loc[:, ['book name', 'label', 'summary']]\n",
        "\n",
        "# #delete the columns with no labels\n",
        "# new_file.dropna(axis = 0, how = 'any', inplace = True)\n",
        "# new_file = new_file.iloc[:, [0, 2, 1]]\n",
        "\n",
        "# new_file = new_file.reset_index(drop=True)\n",
        "\n",
        "# #output data as csv\n",
        "# #new_file.to_csv(r'/content/drive/My Drive/Colab Notebooks/final project/booksummries.csv', index=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFgX_YTIKItu",
        "colab_type": "code",
        "outputId": "ebe408b5-e635-4a01-cef8-3de28b3c6cc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        }
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "new_file = shuffle(new_file)\n",
        "new_file.head().reset_index(drop=True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>plot</th>\n",
              "      <th>Action</th>\n",
              "      <th>Adult</th>\n",
              "      <th>Adventure</th>\n",
              "      <th>Animation</th>\n",
              "      <th>Biography</th>\n",
              "      <th>Comedy</th>\n",
              "      <th>Crime</th>\n",
              "      <th>Documentary</th>\n",
              "      <th>Drama</th>\n",
              "      <th>Family</th>\n",
              "      <th>Fantasy</th>\n",
              "      <th>Game-Show</th>\n",
              "      <th>History</th>\n",
              "      <th>Horror</th>\n",
              "      <th>Music</th>\n",
              "      <th>Musical</th>\n",
              "      <th>Mystery</th>\n",
              "      <th>News</th>\n",
              "      <th>Reality-TV</th>\n",
              "      <th>Romance</th>\n",
              "      <th>Sci-Fi</th>\n",
              "      <th>Short</th>\n",
              "      <th>Sport</th>\n",
              "      <th>Talk-Show</th>\n",
              "      <th>Thriller</th>\n",
              "      <th>War</th>\n",
              "      <th>Western</th>\n",
              "      <th>plot_lang</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"60 Minutes\" (1968) {Heart Attack!/The Kids fr...</td>\n",
              "      <td>\"Heart Attack!\" Is a segment of 60 Minutes pr...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"2point4 Children\" (1991) {Misery (#2.8)}</td>\n",
              "      <td>Whilst Rona spends Christmas alone,all her ex...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"60 Minutes\" (1968) {Blackwater 61/Fight for a...</td>\n",
              "      <td>\"Blackwater 61\" rebroadcasts an investigation...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"17 Kids and Counting\" (2008) {Duggars Under W...</td>\n",
              "      <td>The Duggars are under water taking a trip to ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"48 Hours\" (1988) {Pain Killers (#12.26)}</td>\n",
              "      <td>CBS News 48 Hours takes a look at pain. For m...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... plot_lang\n",
              "0  \"60 Minutes\" (1968) {Heart Attack!/The Kids fr...  ...        en\n",
              "1          \"2point4 Children\" (1991) {Misery (#2.8)}  ...        en\n",
              "2  \"60 Minutes\" (1968) {Blackwater 61/Fight for a...  ...        en\n",
              "3  \"17 Kids and Counting\" (2008) {Duggars Under W...  ...        en\n",
              "4          \"48 Hours\" (1988) {Pain Killers (#12.26)}  ...        en\n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WCODjGEFp7Dp"
      },
      "source": [
        "  ## Encoding the Labels##\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mvMAalBlKDA",
        "colab_type": "code",
        "outputId": "a9b582bb-0765-4891-f834-e5efa7f99dad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "one_hot = new_file.iloc[:, 2:-1]\n",
        "one_hot.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Action</th>\n",
              "      <th>Adult</th>\n",
              "      <th>Adventure</th>\n",
              "      <th>Animation</th>\n",
              "      <th>Biography</th>\n",
              "      <th>Comedy</th>\n",
              "      <th>Crime</th>\n",
              "      <th>Documentary</th>\n",
              "      <th>Drama</th>\n",
              "      <th>Family</th>\n",
              "      <th>Fantasy</th>\n",
              "      <th>Game-Show</th>\n",
              "      <th>History</th>\n",
              "      <th>Horror</th>\n",
              "      <th>Music</th>\n",
              "      <th>Musical</th>\n",
              "      <th>Mystery</th>\n",
              "      <th>News</th>\n",
              "      <th>Reality-TV</th>\n",
              "      <th>Romance</th>\n",
              "      <th>Sci-Fi</th>\n",
              "      <th>Short</th>\n",
              "      <th>Sport</th>\n",
              "      <th>Talk-Show</th>\n",
              "      <th>Thriller</th>\n",
              "      <th>War</th>\n",
              "      <th>Western</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1111</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>624</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1020</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>319</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>892</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Action  Adult  Adventure  Animation  ...  Talk-Show  Thriller  War  Western\n",
              "1111       0      0          0          0  ...          0         0    0        0\n",
              "624        0      0          0          0  ...          0         0    0        0\n",
              "1020       0      0          0          0  ...          0         0    0        0\n",
              "319        0      0          0          0  ...          0         0    0        0\n",
              "892        0      0          0          0  ...          0         0    0        0\n",
              "\n",
              "[5 rows x 27 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dD8aSL-mWdnV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4ecdeb4e-3aa6-44d6-b63a-6012703afd1f"
      },
      "source": [
        "one_hot.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1300, 27)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8p08_5W-p7D0"
      },
      "source": [
        "## Split the words##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g4kaZS71p7D1",
        "outputId": "8f1f0232-8fc1-430f-882a-59b6af9dd7c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "#def words_process(new_file):\n",
        "book_summaries = new_file['plot']\n",
        "summary_list = [summary for summary in book_summaries]\n",
        "summary_num = len(summary_list)\n",
        "#summaries = ''.join(summary_list)\n",
        "print(\"the total number of books: {}\\n\".format(summary_num))\n",
        "\n",
        "all_docs = []\n",
        "i_index = 0\n",
        "for doc in summary_list:\n",
        "    # Tokenize the string into words\n",
        "    tokens = word_tokenize(doc)\n",
        "    # Remove non-alphabetic tokens, such as punctuation\n",
        "    words = [word.lower() for word in tokens if word.isalpha()]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if not word in stop_words]\n",
        "    if len(words) >= 5:\n",
        "      all_docs.append(words)\n",
        "    else:\n",
        "      print(i_index)\n",
        "      one_hot = one_hot.drop(i_index, axis=0)\n",
        "      print(one_hot.shape)\n",
        "    i_index += 1\n",
        "    #return all_docs, one_hot_delt\n",
        "\n",
        "one_hot = one_hot.reset_index(drop=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the total number of books: 1300\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cFlg1asUnghh",
        "colab": {}
      },
      "source": [
        "# l1 = []\n",
        "# for i in range(len(all_docs)):\n",
        "#   l1.append(len(all_docs[i]))\n",
        "# #print(l1.index(6))\n",
        "# len_counts = Counter(l1)\n",
        "# df_len = pd.DataFrame(len_counts.items(), columns=['len', '#books'])\n",
        "# df_len = df_len.sort_values(by = 'len', ascending = True)\n",
        "# #print(min(l1))\n",
        "# df_len.head(17)\n",
        "\n",
        "# #one_hot = one_hot.reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7tn7GXCqp7D4"
      },
      "source": [
        "## outliners ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aRZEVHYrp7D5"
      },
      "source": [
        "## Using a Pre-Trained Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwzIZmY1UHta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import multiprocessing\n",
        "# import sys\n",
        "# import gensim\n",
        "# from gensim.models import Word2Vec\n",
        "# assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise.\"\n",
        "# #from UtilWordEmbedding import TfidfEmbeddingVectorizer\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# workers = multiprocessing.cpu_count()\n",
        "# print('number of cpu: {}'.format(workers))\n",
        "\n",
        "# word_model = Word2Vec(all_docs,\n",
        "#                       min_count=1,\n",
        "#                       size=300,\n",
        "#                       window=5,\n",
        "#                       workers=workers,\n",
        "#                       iter=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xCtCbDx_p7D5",
        "outputId": "dfe69e42-0fb6-4c69-bc9f-c7b1196df084",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "\n",
        "# def words_embedding(docs):\n",
        "   \n",
        "all_words = [ word for doc in all_docs for word in doc]\n",
        "all_words_nodup = list(dict.fromkeys(all_words))\n",
        "# Load word2vec model (trained on an enormous Google corpus)\n",
        "# google_vecs = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary = True) /Colab Notebooks\n",
        "google_vecs = KeyedVectors.load_word2vec_format('/content/drive/My Drive/final project/GoogleNews-vectors-negative300.bin', binary = True)\n",
        "embedding_dim_dim = google_vecs.vector_size\n",
        "\n",
        "def words_embedding(doc):\n",
        "    # Filter the list of vectors to include only those that Word2Vec has a vector for\n",
        "    vector_list = [google_vecs[word] for word in doc if word in google_vecs.vocab]\n",
        "    #vector_list = [word_model.wv[word] for word in doc]\n",
        "    #google_vectors = np.asarray(vector_list)\n",
        "    # Create a list of the words corresponding to these vectors\n",
        "    words_filtered = [word for word in doc if word in google_vecs.vocab]\n",
        "    #Zip the words together with their vector representations\n",
        "    word_vec_zip = zip(words_filtered, vector_list)\n",
        "\n",
        "    # Cast to a dict so we can turn it into a DataFrame\n",
        "    word_vec_dict = dict(word_vec_zip)\n",
        "    word_vec = pd.DataFrame.from_dict(word_vec_dict, orient='index')\n",
        "    return word_vec\n",
        "\n",
        "word_vec  = words_embedding(all_words_nodup)\n",
        "word_vec_array = np.array(word_vec)\n",
        "word_vec.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10680, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OPJrtUy8Qg1p",
        "outputId": "2e343c5f-6895-4c77-be90-2e74abf90832",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "\n",
        "len(all_words)\n",
        "#word_vec_array.shape\n",
        "word_vec_array"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.15039062,  0.24316406,  0.06396484, ...,  0.140625  ,\n",
              "        -0.0612793 ,  0.28320312],\n",
              "       [ 0.18066406,  0.13183594,  0.07714844, ..., -0.33007812,\n",
              "        -0.24414062,  0.1328125 ],\n",
              "       [-0.140625  ,  0.12011719, -0.19238281, ...,  0.03637695,\n",
              "         0.03417969, -0.32226562],\n",
              "       ...,\n",
              "       [-0.36523438, -0.15332031,  0.01843262, ..., -0.11132812,\n",
              "         0.19628906, -0.2578125 ],\n",
              "       [-0.22265625,  0.109375  ,  0.19238281, ..., -0.06640625,\n",
              "         0.05639648,  0.20703125],\n",
              "       [-0.00267029,  0.27148438, -0.03369141, ..., -0.03564453,\n",
              "         0.01599121, -0.13183594]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c9xCvLWnEjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_vec_array = np.insert(word_vec_array, 0, np.zeros(word_vec_array.shape[1]), 0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4mxTtNdaS4zY",
        "outputId": "e82bd641-580b-46cb-9de0-9a684ccb011f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "word_vec.tail()\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>260</th>\n",
              "      <th>261</th>\n",
              "      <th>262</th>\n",
              "      <th>263</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>266</th>\n",
              "      <th>267</th>\n",
              "      <th>268</th>\n",
              "      <th>269</th>\n",
              "      <th>270</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "      <th>281</th>\n",
              "      <th>282</th>\n",
              "      <th>283</th>\n",
              "      <th>284</th>\n",
              "      <th>285</th>\n",
              "      <th>286</th>\n",
              "      <th>287</th>\n",
              "      <th>288</th>\n",
              "      <th>289</th>\n",
              "      <th>290</th>\n",
              "      <th>291</th>\n",
              "      <th>292</th>\n",
              "      <th>293</th>\n",
              "      <th>294</th>\n",
              "      <th>295</th>\n",
              "      <th>296</th>\n",
              "      <th>297</th>\n",
              "      <th>298</th>\n",
              "      <th>299</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>foresees</th>\n",
              "      <td>0.152344</td>\n",
              "      <td>0.068359</td>\n",
              "      <td>0.075684</td>\n",
              "      <td>-0.116211</td>\n",
              "      <td>0.002350</td>\n",
              "      <td>-0.019409</td>\n",
              "      <td>0.247070</td>\n",
              "      <td>-0.169922</td>\n",
              "      <td>-0.053711</td>\n",
              "      <td>-0.033447</td>\n",
              "      <td>-0.330078</td>\n",
              "      <td>-0.079102</td>\n",
              "      <td>0.023315</td>\n",
              "      <td>0.024780</td>\n",
              "      <td>-0.096680</td>\n",
              "      <td>0.263672</td>\n",
              "      <td>-0.090332</td>\n",
              "      <td>0.410156</td>\n",
              "      <td>0.025513</td>\n",
              "      <td>-0.008850</td>\n",
              "      <td>-0.371094</td>\n",
              "      <td>0.376953</td>\n",
              "      <td>-0.029297</td>\n",
              "      <td>-0.029053</td>\n",
              "      <td>0.075195</td>\n",
              "      <td>-0.190430</td>\n",
              "      <td>-0.056396</td>\n",
              "      <td>0.015991</td>\n",
              "      <td>-0.231445</td>\n",
              "      <td>0.244141</td>\n",
              "      <td>-0.075195</td>\n",
              "      <td>-0.203125</td>\n",
              "      <td>-0.103516</td>\n",
              "      <td>0.417969</td>\n",
              "      <td>0.113770</td>\n",
              "      <td>-0.179688</td>\n",
              "      <td>-0.129883</td>\n",
              "      <td>0.172852</td>\n",
              "      <td>0.085938</td>\n",
              "      <td>-0.028442</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.096680</td>\n",
              "      <td>0.121094</td>\n",
              "      <td>0.205078</td>\n",
              "      <td>0.009460</td>\n",
              "      <td>0.210938</td>\n",
              "      <td>0.239258</td>\n",
              "      <td>-0.176758</td>\n",
              "      <td>-0.174805</td>\n",
              "      <td>-0.215820</td>\n",
              "      <td>0.140625</td>\n",
              "      <td>0.099121</td>\n",
              "      <td>-0.234375</td>\n",
              "      <td>-0.072754</td>\n",
              "      <td>-0.296875</td>\n",
              "      <td>-0.116211</td>\n",
              "      <td>0.083008</td>\n",
              "      <td>-0.101562</td>\n",
              "      <td>-0.108887</td>\n",
              "      <td>0.300781</td>\n",
              "      <td>-0.045654</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.166992</td>\n",
              "      <td>-0.353516</td>\n",
              "      <td>-0.113770</td>\n",
              "      <td>0.308594</td>\n",
              "      <td>0.160156</td>\n",
              "      <td>-0.003693</td>\n",
              "      <td>0.042969</td>\n",
              "      <td>0.192383</td>\n",
              "      <td>-0.036377</td>\n",
              "      <td>-0.447266</td>\n",
              "      <td>-0.181641</td>\n",
              "      <td>-0.104980</td>\n",
              "      <td>-0.189453</td>\n",
              "      <td>-0.116699</td>\n",
              "      <td>0.081055</td>\n",
              "      <td>-0.099609</td>\n",
              "      <td>0.093262</td>\n",
              "      <td>0.324219</td>\n",
              "      <td>-0.023560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>instigate</th>\n",
              "      <td>0.022217</td>\n",
              "      <td>0.098145</td>\n",
              "      <td>0.152344</td>\n",
              "      <td>-0.224609</td>\n",
              "      <td>-0.269531</td>\n",
              "      <td>0.149414</td>\n",
              "      <td>0.013062</td>\n",
              "      <td>0.055420</td>\n",
              "      <td>0.045410</td>\n",
              "      <td>-0.088379</td>\n",
              "      <td>-0.206055</td>\n",
              "      <td>-0.185547</td>\n",
              "      <td>-0.018066</td>\n",
              "      <td>0.376953</td>\n",
              "      <td>0.051514</td>\n",
              "      <td>0.054199</td>\n",
              "      <td>-0.162109</td>\n",
              "      <td>0.012268</td>\n",
              "      <td>0.147461</td>\n",
              "      <td>0.000185</td>\n",
              "      <td>0.178711</td>\n",
              "      <td>0.191406</td>\n",
              "      <td>0.247070</td>\n",
              "      <td>0.022095</td>\n",
              "      <td>-0.020752</td>\n",
              "      <td>0.124512</td>\n",
              "      <td>-0.161133</td>\n",
              "      <td>0.106934</td>\n",
              "      <td>0.037354</td>\n",
              "      <td>0.150391</td>\n",
              "      <td>-0.117188</td>\n",
              "      <td>0.021606</td>\n",
              "      <td>0.025024</td>\n",
              "      <td>-0.082031</td>\n",
              "      <td>-0.275391</td>\n",
              "      <td>0.013428</td>\n",
              "      <td>-0.029785</td>\n",
              "      <td>0.154297</td>\n",
              "      <td>0.208984</td>\n",
              "      <td>0.243164</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.041504</td>\n",
              "      <td>0.045654</td>\n",
              "      <td>-0.047852</td>\n",
              "      <td>0.153320</td>\n",
              "      <td>-0.051270</td>\n",
              "      <td>-0.143555</td>\n",
              "      <td>-0.218750</td>\n",
              "      <td>0.052246</td>\n",
              "      <td>0.095215</td>\n",
              "      <td>-0.170898</td>\n",
              "      <td>-0.053955</td>\n",
              "      <td>0.163086</td>\n",
              "      <td>0.104492</td>\n",
              "      <td>0.296875</td>\n",
              "      <td>0.015320</td>\n",
              "      <td>0.035156</td>\n",
              "      <td>-0.041260</td>\n",
              "      <td>0.026001</td>\n",
              "      <td>0.216797</td>\n",
              "      <td>-0.068359</td>\n",
              "      <td>0.088379</td>\n",
              "      <td>0.026855</td>\n",
              "      <td>0.098145</td>\n",
              "      <td>-0.094238</td>\n",
              "      <td>0.067383</td>\n",
              "      <td>-0.014404</td>\n",
              "      <td>-0.211914</td>\n",
              "      <td>-0.061768</td>\n",
              "      <td>0.091797</td>\n",
              "      <td>-0.212891</td>\n",
              "      <td>-0.090820</td>\n",
              "      <td>0.197266</td>\n",
              "      <td>0.083984</td>\n",
              "      <td>0.186523</td>\n",
              "      <td>-0.174805</td>\n",
              "      <td>-0.361328</td>\n",
              "      <td>0.052246</td>\n",
              "      <td>0.005341</td>\n",
              "      <td>0.138672</td>\n",
              "      <td>0.141602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>champagne</th>\n",
              "      <td>-0.365234</td>\n",
              "      <td>-0.153320</td>\n",
              "      <td>0.018433</td>\n",
              "      <td>0.233398</td>\n",
              "      <td>-0.013062</td>\n",
              "      <td>-0.150391</td>\n",
              "      <td>0.033203</td>\n",
              "      <td>-0.287109</td>\n",
              "      <td>0.215820</td>\n",
              "      <td>0.255859</td>\n",
              "      <td>-0.207031</td>\n",
              "      <td>-0.378906</td>\n",
              "      <td>0.257812</td>\n",
              "      <td>-0.081055</td>\n",
              "      <td>-0.143555</td>\n",
              "      <td>0.376953</td>\n",
              "      <td>0.165039</td>\n",
              "      <td>0.102539</td>\n",
              "      <td>0.296875</td>\n",
              "      <td>-0.253906</td>\n",
              "      <td>0.141602</td>\n",
              "      <td>0.361328</td>\n",
              "      <td>0.138672</td>\n",
              "      <td>-0.462891</td>\n",
              "      <td>0.134766</td>\n",
              "      <td>0.449219</td>\n",
              "      <td>-0.376953</td>\n",
              "      <td>0.302734</td>\n",
              "      <td>0.109863</td>\n",
              "      <td>0.220703</td>\n",
              "      <td>-0.279297</td>\n",
              "      <td>-0.199219</td>\n",
              "      <td>0.184570</td>\n",
              "      <td>-0.007263</td>\n",
              "      <td>-0.012451</td>\n",
              "      <td>-0.021606</td>\n",
              "      <td>0.088867</td>\n",
              "      <td>-0.292969</td>\n",
              "      <td>-0.044434</td>\n",
              "      <td>-0.246094</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.357422</td>\n",
              "      <td>0.283203</td>\n",
              "      <td>-0.237305</td>\n",
              "      <td>0.201172</td>\n",
              "      <td>-0.010254</td>\n",
              "      <td>-0.316406</td>\n",
              "      <td>-0.050781</td>\n",
              "      <td>0.009583</td>\n",
              "      <td>-0.007324</td>\n",
              "      <td>-0.013489</td>\n",
              "      <td>-0.003387</td>\n",
              "      <td>0.180664</td>\n",
              "      <td>0.122559</td>\n",
              "      <td>0.092285</td>\n",
              "      <td>0.302734</td>\n",
              "      <td>0.121582</td>\n",
              "      <td>-0.017090</td>\n",
              "      <td>0.126953</td>\n",
              "      <td>0.013855</td>\n",
              "      <td>-0.218750</td>\n",
              "      <td>-0.043457</td>\n",
              "      <td>-0.041992</td>\n",
              "      <td>0.184570</td>\n",
              "      <td>-0.148438</td>\n",
              "      <td>0.292969</td>\n",
              "      <td>0.298828</td>\n",
              "      <td>0.079590</td>\n",
              "      <td>0.394531</td>\n",
              "      <td>0.121582</td>\n",
              "      <td>-0.032715</td>\n",
              "      <td>0.261719</td>\n",
              "      <td>-0.137695</td>\n",
              "      <td>0.043945</td>\n",
              "      <td>-0.032959</td>\n",
              "      <td>0.020508</td>\n",
              "      <td>-0.357422</td>\n",
              "      <td>0.296875</td>\n",
              "      <td>-0.111328</td>\n",
              "      <td>0.196289</td>\n",
              "      <td>-0.257812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tupperware</th>\n",
              "      <td>-0.222656</td>\n",
              "      <td>0.109375</td>\n",
              "      <td>0.192383</td>\n",
              "      <td>0.589844</td>\n",
              "      <td>-0.022339</td>\n",
              "      <td>0.242188</td>\n",
              "      <td>0.143555</td>\n",
              "      <td>-0.090332</td>\n",
              "      <td>-0.267578</td>\n",
              "      <td>0.127930</td>\n",
              "      <td>0.021973</td>\n",
              "      <td>-0.238281</td>\n",
              "      <td>0.058838</td>\n",
              "      <td>0.059814</td>\n",
              "      <td>0.036865</td>\n",
              "      <td>0.076172</td>\n",
              "      <td>-0.040039</td>\n",
              "      <td>0.072266</td>\n",
              "      <td>-0.051025</td>\n",
              "      <td>-0.093750</td>\n",
              "      <td>0.183594</td>\n",
              "      <td>-0.042236</td>\n",
              "      <td>0.013184</td>\n",
              "      <td>0.071289</td>\n",
              "      <td>0.063965</td>\n",
              "      <td>0.144531</td>\n",
              "      <td>-0.165039</td>\n",
              "      <td>0.243164</td>\n",
              "      <td>-0.109863</td>\n",
              "      <td>-0.277344</td>\n",
              "      <td>-0.117676</td>\n",
              "      <td>0.102051</td>\n",
              "      <td>-0.050537</td>\n",
              "      <td>0.072266</td>\n",
              "      <td>0.151367</td>\n",
              "      <td>-0.009888</td>\n",
              "      <td>0.215820</td>\n",
              "      <td>-0.004883</td>\n",
              "      <td>-0.104004</td>\n",
              "      <td>-0.099609</td>\n",
              "      <td>...</td>\n",
              "      <td>0.081543</td>\n",
              "      <td>0.078125</td>\n",
              "      <td>-0.253906</td>\n",
              "      <td>-0.281250</td>\n",
              "      <td>-0.070801</td>\n",
              "      <td>0.055908</td>\n",
              "      <td>-0.289062</td>\n",
              "      <td>-0.009338</td>\n",
              "      <td>-0.199219</td>\n",
              "      <td>0.028442</td>\n",
              "      <td>0.018921</td>\n",
              "      <td>0.144531</td>\n",
              "      <td>0.341797</td>\n",
              "      <td>0.216797</td>\n",
              "      <td>0.168945</td>\n",
              "      <td>-0.005585</td>\n",
              "      <td>0.061768</td>\n",
              "      <td>-0.083008</td>\n",
              "      <td>-0.167969</td>\n",
              "      <td>0.032471</td>\n",
              "      <td>-0.092773</td>\n",
              "      <td>0.076660</td>\n",
              "      <td>0.069824</td>\n",
              "      <td>0.168945</td>\n",
              "      <td>0.152344</td>\n",
              "      <td>-0.034180</td>\n",
              "      <td>0.064453</td>\n",
              "      <td>0.032227</td>\n",
              "      <td>-0.214844</td>\n",
              "      <td>0.064453</td>\n",
              "      <td>-0.088379</td>\n",
              "      <td>0.028809</td>\n",
              "      <td>-0.100098</td>\n",
              "      <td>0.113770</td>\n",
              "      <td>0.028687</td>\n",
              "      <td>-0.197266</td>\n",
              "      <td>-0.025513</td>\n",
              "      <td>-0.066406</td>\n",
              "      <td>0.056396</td>\n",
              "      <td>0.207031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sacrifices</th>\n",
              "      <td>-0.002670</td>\n",
              "      <td>0.271484</td>\n",
              "      <td>-0.033691</td>\n",
              "      <td>0.464844</td>\n",
              "      <td>-0.006622</td>\n",
              "      <td>-0.199219</td>\n",
              "      <td>0.302734</td>\n",
              "      <td>-0.158203</td>\n",
              "      <td>0.163086</td>\n",
              "      <td>0.294922</td>\n",
              "      <td>-0.090820</td>\n",
              "      <td>-0.040771</td>\n",
              "      <td>-0.144531</td>\n",
              "      <td>0.402344</td>\n",
              "      <td>-0.002090</td>\n",
              "      <td>-0.010437</td>\n",
              "      <td>0.005676</td>\n",
              "      <td>0.084473</td>\n",
              "      <td>-0.047119</td>\n",
              "      <td>-0.188477</td>\n",
              "      <td>-0.026611</td>\n",
              "      <td>0.104980</td>\n",
              "      <td>0.048340</td>\n",
              "      <td>-0.223633</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.215820</td>\n",
              "      <td>-0.175781</td>\n",
              "      <td>-0.386719</td>\n",
              "      <td>-0.180664</td>\n",
              "      <td>-0.375000</td>\n",
              "      <td>-0.113281</td>\n",
              "      <td>-0.021484</td>\n",
              "      <td>0.443359</td>\n",
              "      <td>0.136719</td>\n",
              "      <td>0.196289</td>\n",
              "      <td>-0.361328</td>\n",
              "      <td>0.425781</td>\n",
              "      <td>0.298828</td>\n",
              "      <td>-0.094727</td>\n",
              "      <td>0.052979</td>\n",
              "      <td>...</td>\n",
              "      <td>0.115234</td>\n",
              "      <td>0.037598</td>\n",
              "      <td>-0.074219</td>\n",
              "      <td>0.099609</td>\n",
              "      <td>0.244141</td>\n",
              "      <td>0.165039</td>\n",
              "      <td>0.189453</td>\n",
              "      <td>0.074219</td>\n",
              "      <td>-0.281250</td>\n",
              "      <td>0.078125</td>\n",
              "      <td>-0.081543</td>\n",
              "      <td>0.277344</td>\n",
              "      <td>0.472656</td>\n",
              "      <td>0.101074</td>\n",
              "      <td>0.211914</td>\n",
              "      <td>-0.066895</td>\n",
              "      <td>-0.086426</td>\n",
              "      <td>0.229492</td>\n",
              "      <td>-0.486328</td>\n",
              "      <td>0.049072</td>\n",
              "      <td>0.259766</td>\n",
              "      <td>-0.047363</td>\n",
              "      <td>0.032471</td>\n",
              "      <td>-0.060303</td>\n",
              "      <td>-0.292969</td>\n",
              "      <td>0.006531</td>\n",
              "      <td>-0.123535</td>\n",
              "      <td>0.451172</td>\n",
              "      <td>-0.180664</td>\n",
              "      <td>0.349609</td>\n",
              "      <td>-0.285156</td>\n",
              "      <td>-0.353516</td>\n",
              "      <td>-0.337891</td>\n",
              "      <td>0.018677</td>\n",
              "      <td>-0.328125</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>-0.227539</td>\n",
              "      <td>-0.035645</td>\n",
              "      <td>0.015991</td>\n",
              "      <td>-0.131836</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 300 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0         1         2    ...       297       298       299\n",
              "foresees    0.152344  0.068359  0.075684  ...  0.093262  0.324219 -0.023560\n",
              "instigate   0.022217  0.098145  0.152344  ...  0.005341  0.138672  0.141602\n",
              "champagne  -0.365234 -0.153320  0.018433  ... -0.111328  0.196289 -0.257812\n",
              "tupperware -0.222656  0.109375  0.192383  ... -0.066406  0.056396  0.207031\n",
              "sacrifices -0.002670  0.271484 -0.033691  ... -0.035645  0.015991 -0.131836\n",
              "\n",
              "[5 rows x 300 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1nCtESDnEkM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# word_vec_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TyVlCnwUQGFh",
        "colab": {}
      },
      "source": [
        "# word_vec.to_csv(r'./wor_vec.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zAE7B160QJHI",
        "colab": {}
      },
      "source": [
        "#word_vec = pd.read_csv(r\"/content/drive/My Drive/Colab Notebooks/final project/wor_vec.csv\", header=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1zaxCSpzp7D8"
      },
      "source": [
        "# Generate the integer vectors of all summaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VjTTRo8pp7D8",
        "colab": {}
      },
      "source": [
        "word_index = pd.DataFrame(index = word_vec.index)\n",
        "wordlen_list = range(1, word_vec.shape[0]+1)\n",
        "word_index['index'] = wordlen_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gfkhtxi7nEkm",
        "colab_type": "code",
        "outputId": "0927e03e-7ff3-475f-d9c5-a05a4518f847",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "wordlen_list[-1]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10680"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Aoi4c-sTp7D-",
        "colab": {}
      },
      "source": [
        "\n",
        "def words_index(all_docs):\n",
        "    all_words_index = []\n",
        "    for doc in all_docs:\n",
        "        inds = []\n",
        "        for word in doc:\n",
        "            try:\n",
        "                inds.append(word_index.at[word, 'index'])\n",
        "            except: \n",
        "                continue\n",
        "            \n",
        "        all_words_index.append(inds)\n",
        "    return all_words_index\n",
        "\n",
        "all_words_index = words_index(all_docs)\n",
        "# all_words_index[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_HyrkAHnEkx",
        "colab_type": "code",
        "outputId": "8777287b-efd7-455d-b045-3979f82b7aea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(all_words_index)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yanGgpNnEk4",
        "colab_type": "code",
        "outputId": "d026e9fb-5062-47cf-c72c-b0ad574ec0fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max(all_words_index[-1])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10680"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rlA2ucnkp7EA",
        "outputId": "27bfa508-deb0-46b9-fc15-fbc8e8ee4d9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "l = []\n",
        "for i in range(len(all_words_index)):\n",
        "    l.append(len(all_words_index[i]))\n",
        "l.index(max(l))\n",
        "max(l)\n",
        "#len(all_words_index)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "174"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "85Jk49Tfp7EC"
      },
      "source": [
        "# Visualize the cluster of books"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sLZd4ZUqp7EC",
        "colab": {}
      },
      "source": [
        "# import numpy as np\n",
        "# def avg_doc_embedding(all_docs):\n",
        "#     doc_vec = []\n",
        "#     for doc in all_docs:\n",
        "#         if len(doc) != 0:\n",
        "#             vector =  words_embedding(doc)\n",
        "#             doc_vec.append(np.mean(np.array(vector), axis=0))\n",
        "    \n",
        "#     summary_vec = np.array(doc_vec)\n",
        "#     return summary_vec\n",
        "\n",
        "# summary_vec = avg_doc_embedding(all_docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s9f1TtHUp7EE",
        "colab": {}
      },
      "source": [
        "# summary_vec.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fnpmmu6_UZhX",
        "colab_type": "code",
        "outputId": "3621a14e-04cd-414a-fcad-0d63bcd224f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "pip install gensim"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.3)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.12.47)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.23.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.47 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.15.47)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2020.4.5.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->smart-open>=1.2.1->gensim) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->smart-open>=1.2.1->gensim) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieyUjhCtRWXc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#def tf_dif(doc):\n",
        "all_text_docs = []\n",
        "for doc in all_docs:\n",
        "    # print(len(doc))\n",
        "    all_text_docs.append(\" \". join(doc))\n",
        "tf_idf_vect = TfidfVectorizer(stop_words='english', max_features=6000)\n",
        "final_tf_idf = tf_idf_vect.fit_transform(all_text_docs)\n",
        "tfidf_feat = tf_idf_vect.get_feature_names()\n",
        "    #return final_tf_idf\n",
        "# final_tf_idf\n",
        "# tfidf_feat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab2PhoHARXsf",
        "colab_type": "code",
        "outputId": "61b98dd6-d845-40d8-fc65-610b30b281d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tfidf_doc_vectors_ft = [] # the tfidf-ft for each summary  is stored in this list\n",
        "row=0\n",
        "errors=0\n",
        "for doc in all_docs: # for each summary \n",
        "    doc_vec = np.zeros(300) # as word vectors are of zero length\n",
        "    weight_sum =0; # num of words with a valid vector in the summary \n",
        "    for word in doc: # for each word in a summary \n",
        "        try:\n",
        "            word_vec = google_vecs[word]\n",
        "            #word_vec = word_model.wv[word]\n",
        "            # obtain the tf_idf of a word in a summary \n",
        "            tfidf = final_tf_idf[row, tfidf_feat.index(word)]\n",
        "            #print(tfidf)\n",
        "            doc_vec += (word_vec * tfidf)\n",
        "            weight_sum += tfidf\n",
        "        except:\n",
        "            errors =+1\n",
        "            pass\n",
        "    doc_vec /= weight_sum\n",
        "   \n",
        "\n",
        "    tfidf_doc_vectors_ft.append(doc_vec)\n",
        "    row += 1\n",
        "\n",
        "tfidf_doc_vectors_ft_array = np.array(tfidf_doc_vectors_ft)\n",
        "print('errors noted: '+str(errors))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "errors noted: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvtNRhVRTv6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tfidf_doc_vectors_ft_array.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PloAPO6PeQtj",
        "colab_type": "code",
        "outputId": "58d55edc-c625-4398-e654-8bdca27e9896",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "X = tfidf_doc_vectors_ft_array\n",
        "y = one_hot.values\n",
        "print(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "neigh = KNeighborsClassifier(n_neighbors=3)\n",
        "neigh.fit(X_train, y_train)\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
              "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
              "                     weights='uniform')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yESZ40mVAfrP",
        "colab_type": "code",
        "outputId": "88968835-22b8-4b08-a99c-aef5ab3586d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "print(X_test.shape)\n",
        "y_pred = neigh.predict(X_test)\n",
        "\n",
        "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
        "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(325, 300)\n",
            "Testing accuracy 0.5353846153846153\n",
            "Testing F1 score: 0.6980464729371313\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-8YShqDVtORv",
        "colab": {}
      },
      "source": [
        "# pip install adjustText"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6QoaEMczp7EF",
        "colab": {}
      },
      "source": [
        "# from sklearn.manifold import TSNE\n",
        "# from adjustText import adjust_text\n",
        "# import seaborn as sns\n",
        "# import matplotlib.pyplot as plt\n",
        "# book_titles = new_file['title']\n",
        "# titles_list = [title for title in book_titles]\n",
        "# titles_list = titles_list[:400]\n",
        "# def t_SNE_plot(df_data):\n",
        "#     # Initialize t-SNE\n",
        "#     tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 100)\n",
        "\n",
        "#     # Use only 400 rows to shorten processing time\n",
        "#     tsne_df = tsne.fit_transform(df_data)#tsne.fit_transform(df_data[:400])\n",
        "\n",
        "#     sns.set()\n",
        "#     # Initialize figure\n",
        "#     fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
        "#     sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
        "\n",
        "#     # Import adjustText, initialize list of texts\n",
        "\n",
        "#     texts = []\n",
        "#     data_to_plot = list(np.arange(0, 400, 20))#list(np.arange(0, 400, 40))\n",
        "\n",
        "#     # Append words to list\n",
        "#     for data in data_to_plot:\n",
        "#         texts.append(plt.text(tsne_df[data, 0], tsne_df[data, 1], titles_list[data], fontsize = 14))\n",
        "\n",
        "#     # Plot text using adjust_text (because overlapping text is hard to read)\n",
        "#     adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
        "#                 expand_points = (2,1), expand_text = (1,2),\n",
        "#                 arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
        "\n",
        "#     plt.show()\n",
        "\n",
        "#     return tsne_df\n",
        "\n",
        "# tsne_df = t_SNE_plot(tfidf_doc_vectors_ft_array[:400, :])    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkxycIRUWNzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # An user defined function to create scatter plot of vectors\n",
        "# def scatter(x, colors):\n",
        "#     # We choose a color palette with seaborn.\n",
        "#     palette = np.array(sns.color_palette(\"hls\", 27))\n",
        "\n",
        "#     # We create a scatter plot.\n",
        "#     f = plt.figure(figsize=(32, 32))\n",
        "#     ax = plt.subplot(aspect='equal')\n",
        "#     sc = ax.scatter(x[:,0], x[:,1], lw=0, s=120,\n",
        "#                     c=palette[colors.astype(np.int)])\n",
        "#     #plt.xlim(-25, 25)\n",
        "#     #plt.ylim(-25, 25)\n",
        "#     ax.axis('off')\n",
        "#     ax.axis('tight')\n",
        "\n",
        "#     # We add the labels for each cluster.\n",
        "#     txts = []\n",
        "#     for i in range(27):\n",
        "#         # Position of each label.\n",
        "#         xtext, ytext = np.median(x[colors == i, :], axis=0)\n",
        "#         txt = ax.text(xtext, ytext, str(i), fontsize=50)\n",
        "#         txt.set_path_effects([\n",
        "#             PathEffects.Stroke(linewidth=5, foreground=\"w\"),\n",
        "#             PathEffects.Normal()])\n",
        "#         txts.append(txt)\n",
        "\n",
        "#     return f, ax, sc, txts\n",
        "\n",
        "# Y = one_hot.iloc[:400, :].to_numpy()\n",
        "# print(list(range(0,18)))\n",
        "# sns.palplot(np.array(sns.color_palette(\"hls\", 27)))\n",
        "# scatter(tsne_df, Y)   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2joV1EE6p7EH"
      },
      "source": [
        "## Padding sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "enefEKmQp7EH",
        "outputId": "bc5f685a-26de-4728-95db-e494cba48788",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max_Length = 0  \n",
        "docs_length = []\n",
        "for x in all_words_index:\n",
        "    docs_length.append(len(x))\n",
        "    if len(x) > max_Length:\n",
        "        max_Length = len(x)\n",
        "\n",
        "docs_length = np.array(docs_length)\n",
        "max_Length"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "174"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sa5MKaO-cKk7",
        "outputId": "ccf30f16-d2e0-4092-aea4-a185cd7c24f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.where(docs_length<1)\n",
        "#docs_length[3382]"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([], dtype=int64),)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zVhVxD4Yp7EJ",
        "colab": {}
      },
      "source": [
        "def pad_sequences(all_words_index, seq_length):\n",
        "    \n",
        "    # getting the correct rows x cols shape\n",
        "    sequences = np.zeros((len(all_words_index), seq_length), dtype=int)\n",
        " \n",
        "    # for each review, I grab that review and \n",
        "    for i, row in enumerate(all_words_index):\n",
        "        if len(row)>0:\n",
        "            sequences[i, 0:len(row)] =  np.array(row)[:seq_length]\n",
        "    \n",
        "    return sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y6dnQuwrp7EL",
        "outputId": "4e2ae7ee-eb49-4ef0-a912-70a58ed4a45f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "sequences = pad_sequences(all_words_index, max_Length)\n",
        "\n",
        "assert len(sequences)==len(all_words_index), \"Sequences should have as many rows as reviews.\"\n",
        "assert len(sequences[0])==max_Length, \"Each sequence row should contain seq_length values.\"\n",
        "\n",
        "print(sequences[-1])\n",
        "np.where(sequences[-1] ==51242 )"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  282  3342  3343  6049   251  1358   496   880  3188  1179  2094   281\n",
            "   239  3258  6049  1087   275  3945   719  2766   282   292  1358  6049\n",
            "  3307   781  7316  2301    58   254   281    43 10680   101   108   107\n",
            "  3085   263  1028  2774   271   263  1205  5219  2774     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([], dtype=int64),)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tM4vXOwrp7EN"
      },
      "source": [
        "# Training, Validation, and Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g2q-yFvfp7EN",
        "outputId": "244c98e3-d275-45e9-9e9c-bf587aa458e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "split_frac = 0.7\n",
        "\n",
        "## split data into training, validation, and test data (features and labels, x and y)\n",
        "\n",
        "split_idx = int(len(sequences)*split_frac)\n",
        "train_x, remaining_x = sequences[:split_idx], sequences[split_idx:]\n",
        "train_y, remaining_y = one_hot[:split_idx], one_hot[split_idx:]\n",
        "train_len, remaining_len = docs_length[:split_idx], docs_length[split_idx:]\n",
        "\n",
        "# train_x, remaining_x = sequences[:10000], sequences[-4000:]\n",
        "# train_y, remaining_y = one_hot[:10000], one_hot[-4000:]\n",
        "# train_len, remaining_len = docs_length[:10000], docs_length[-4000:]\n",
        "\n",
        "test_idx = int(len(remaining_x)*0.5)\n",
        "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
        "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
        "val_len, test_len = remaining_len[:test_idx], remaining_len[test_idx:]\n",
        "\n",
        "## print out the shapes of your resultant feature data\n",
        "print(\"\\t\\t\\tSequences Shapes:\")\n",
        "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
        "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
        "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\t\tSequences Shapes:\n",
            "Train set: \t\t(909, 174) \n",
            "Validation set: \t(195, 174) \n",
            "Test set: \t\t(196, 174)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UDvw1Oi6hBOv",
        "outputId": "1f14a56f-e8a3-4c51-ac40-bc8290c76b9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_len.min"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function ndarray.min>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iwJXkJ8zzrvt",
        "colab": {}
      },
      "source": [
        "# split_frac = 0.8\n",
        "\n",
        "# ## split data into training, validation, and test data (features and labels, x and y)\n",
        "\n",
        "# split_idx = int(len(all_words_index)*split_frac)\n",
        "# train_x, remaining_x = all_words_index[:split_idx], all_words_index[split_idx:]\n",
        "# train_y, remaining_y = one_hot[:split_idx], one_hot[split_idx:]\n",
        "# #train_len, remaining_len = docs_length[:split_idx], docs_length[split_idx:]\n",
        "\n",
        "# test_idx = int(len(remaining_x)*0.5)\n",
        "# val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
        "# val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
        "# #val_len, test_len = remaining_len[:test_idx], remaining_len[test_idx:]\n",
        "\n",
        "# ## print out the shapes of your resultant feature data\n",
        "# print(\"\\t\\t\\tSequences Shapes:\")\n",
        "# print(\"Train set: \\t\\t{}\".format(len(train_x)), \n",
        "#       \"\\nValidation set: \\t{}\".format(len(val_x)),\n",
        "#       \"\\nTest set: \\t\\t{}\".format(len(test_)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KyHBQ3l0p7ER"
      },
      "source": [
        "### Above only 2,0,1 because I only used  3 datasample to do all of these"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tbUPadNHp7ES"
      },
      "source": [
        "# DataLoaders and Batching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lcP-6vOJp7ES",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "# from torch.nn.utils.rnn import pad_sequence\n",
        "# # create Tensor datasets\n",
        "\n",
        "\n",
        "train_data = TensorDataset(torch.from_numpy(np.array(train_x)), torch.from_numpy(np.array(train_y).astype(np.int16)), torch.from_numpy(train_len)) \n",
        "valid_data = TensorDataset(torch.from_numpy(np.array(val_x)), torch.from_numpy(np.array(val_y).astype(np.int16)), torch.from_numpy(val_len))\n",
        "test_data = TensorDataset(torch.from_numpy(np.array(test_x)), torch.from_numpy(np.array(test_y).astype(np.int16)), torch.from_numpy(test_len)) \n",
        "\n",
        "# # train_data = Dataset(train_x, train_y) #, torch.from_numpy(train_len)\n",
        "# # valid_data = Dataset(val_x, val_y)\n",
        "# # test_data = Dataset(test_x, test_y) \n",
        "\n",
        "# # dataloaders\n",
        "batch_size = 128\n",
        "# #RANDOM_SEED = 1\n",
        "\n",
        "# def pad_collate(batch):\n",
        "#   (xx, yy) = zip(*batch)\n",
        "#   x_lens = [len(x) for x in xx]\n",
        "#   y_lens = [len(y) for y in yy]\n",
        "\n",
        "#   xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)\n",
        "#   yy_pad = pad_sequence(yy, batch_first=True, padding_value=0) , collate_fn=pad_collate\n",
        "\n",
        "#   return xx_pad, yy_pad, x_lens, y_lens\n",
        "\n",
        "# shuffling and batching data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, num_workers=4)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size, num_workers=4)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, num_workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtEfcsr_hQHw",
        "colab_type": "code",
        "outputId": "fe7cca8c-d31a-4fca-91fc-cd9451fb717d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train_loader)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tz_3dnhcp7EU"
      },
      "source": [
        "# LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KExMFqzUp7EU",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# import numpy as np\n",
        "# from torch import nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# # def kmax_pooling(x, dim, k):\n",
        "# #     index = x.topk(k, dim = dim)[1].sort(dim = dim)[0]\n",
        "# #     return x.gather(dim, index)\n",
        "\n",
        "# class LSTMText(torch.nn.Module): \n",
        "#     def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, \n",
        "#                 linear_hidden_size, num_classes, freeze_embeddings=True):\n",
        "#         super(LSTMText, self).__init__()\n",
        "        \n",
        "#         self.num_classes = num_classes\n",
        "#         # 1. embedding layer\n",
        "#         self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "#         # set weights to pre-trained\n",
        "#         self.embedding.weight = nn.Parameter(torch.from_numpy(word_vec_array)) # all vectors\n",
        "#         # (optional) freeze embedding weights\n",
        "#         if freeze_embeddings:\n",
        "#             self.embedding.requires_grad = False\n",
        "\n",
        "#         self.lstm =nn.LSTM( input_size = embedding_dim,\n",
        "#                             hidden_size = hidden_size,\n",
        "#                             num_layers = num_layers,\n",
        "#                             bias = True,\n",
        "#                             batch_first = False,\n",
        "#                             # dropout = 0.5,\n",
        "#                             bidirectional = True\n",
        "#                             )\n",
        "\n",
        "#         # self.dropout = nn.Dropout()\n",
        "#         self.fc = nn.Sequential(\n",
        "#             nn.Linear(hidden_size*2, linear_hidden_size),\n",
        "#             nn.BatchNorm1d(linear_hidden_size),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Linear(linear_hidden_size, num_classes)\n",
        "#         )\n",
        "        \n",
        "#          #activation function\n",
        "#         self.act = nn.Sigmoid()\n",
        " \n",
        "#     def forward(self, text):\n",
        "      \n",
        "#         #text = [batch size,sent_length]\n",
        "#         embedded = self.embedding(text)\n",
        "#         #embedded = [batch size, sent_len, emb dim]\n",
        "#         print(embedded.shape)\n",
        "#         #packed sequence\n",
        "#         #packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=True)\n",
        "        \n",
        "#         #packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
        "#         packed_output = self.lstm(embedded) #.view(len(text), 1, -1)\n",
        "#         #hidden = [batch size, num layers * num directions,hid dim]\n",
        "#         #cell = [batch size, num layers * num directions,hid dim]\n",
        "        \n",
        "#         #concat the final forward and backward hidden state\n",
        "#         #hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
        "                \n",
        "#         #hidden = [batch size, hid dim * num directions]\n",
        "#         #logits=self.fc(hidden)\n",
        "#         logits=self.fc(packed_output) #.view(len(text),-1)\n",
        "\n",
        "#         #Final activation function\n",
        "#         probas=self.act(logits, dim = 1)\n",
        "\n",
        "#         return logits, probas\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbXD3JlmA4og",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "# attention layer code inspired from: https://discuss.pytorch.org/t/self-attention-on-words-and-masking/5671/4\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size, batch_first=False):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.batch_first = batch_first\n",
        "\n",
        "        self.att_weights = nn.Parameter(torch.Tensor(1, hidden_size), requires_grad=True)\n",
        "\n",
        "        stdv = 1.0 / np.sqrt(self.hidden_size)\n",
        "        for weight in self.att_weights:\n",
        "            nn.init.uniform_(weight, -stdv, stdv)\n",
        "\n",
        "    def get_mask(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, inputs, lengths):\n",
        "        if self.batch_first:\n",
        "            batch_size, max_len = inputs.size()[:2]\n",
        "        else:\n",
        "            max_len, batch_size = inputs.size()[:2]\n",
        "        # print(inputs.size())\n",
        "        # print(self.att_weights.permute(1, 0).unsqueeze(0).repeat(batch_size, 1, 1).size())    \n",
        "        # apply attention layer\n",
        "        weights = torch.bmm(inputs,\n",
        "                            self.att_weights  # (1, hidden_size)\n",
        "                            .permute(1, 0)  # (hidden_size, 1)\n",
        "                            .unsqueeze(0)  # (1, hidden_size, 1)\n",
        "                            .repeat(batch_size, 1, 1) # (batch_size, hidden_size, 1)\n",
        "                            )\n",
        "    \n",
        "        attentions = torch.softmax(F.tanh(weights.squeeze()), dim=-1)\n",
        "\n",
        "        # create mask based on the sentence lengths\n",
        "        mask = torch.ones(attentions.size(), requires_grad=True).cuda()\n",
        "        for i, l in enumerate(lengths):  # skip the first sentence\n",
        "            if l < max_len:\n",
        "                mask[i, l:] = 0\n",
        "\n",
        "        # apply mask and renormalize attention scores (weights)\n",
        "        masked = attentions * mask\n",
        "        _sums = masked.sum(-1).unsqueeze(-1)  # sums per row\n",
        "        \n",
        "        attentions = masked.div(_sums)\n",
        "\n",
        "        # apply attention weights\n",
        "        weighted = torch.mul(inputs, attentions.unsqueeze(-1).expand_as(inputs))\n",
        "\n",
        "        # get the final fixed vector representations of the sentences\n",
        "        representations = weighted.sum(1).squeeze()\n",
        "\n",
        "        return representations, attentions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kvsMqQ2lv1aO",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "# def kmax_pooling(x, dim, k):\n",
        "#     index = x.topk(k, dim = dim)[1].sort(dim = dim)[0]\n",
        "#     return x.gather(dim, index)\n",
        "\n",
        "class LSTMText1(torch.nn.Module): \n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, \n",
        "                linear_hidden_size, num_classes, freeze_embeddings=True):\n",
        "        super(LSTMText1, self).__init__()\n",
        "        \n",
        "        \n",
        "#         self.num_classes = num_classes\n",
        "        # 1. embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        # set weights to pre-trained\n",
        "        self.embedding.weight = nn.Parameter(torch.from_numpy(word_vec_array)) # all vectors\n",
        "        # (optional) freeze embedding weights\n",
        "        if freeze_embeddings:\n",
        "            self.embedding.requires_grad = False\n",
        "\n",
        "        self.lstm =nn.LSTM( input_size = embedding_dim,\n",
        "                            hidden_size = hidden_size,\n",
        "                            num_layers = num_layers,\n",
        "                            bias = True,\n",
        "                            batch_first = True,\n",
        "                            dropout = 0.5,\n",
        "                            bidirectional = True\n",
        "                            )\n",
        "        self.atten = Attention(hidden_size*2, batch_first=True) # 2 is bidrectional\n",
        "        # self.lstm2 = nn.LSTM(input_size=hidden_size*2,\n",
        "        #                     hidden_size=hidden_size,\n",
        "        #                     num_layers=1, \n",
        "        #                     bidirectional=True)\n",
        "        # self.dropout = nn.Dropout()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(hidden_size*2, linear_hidden_size), #linear_hidden_size\n",
        "            nn.BatchNorm1d(linear_hidden_size),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(linear_hidden_size, num_classes)\n",
        "        )\n",
        "        \n",
        "         #activation function\n",
        "        self.act = nn.Sigmoid()\n",
        " \n",
        "    def forward(self, text, text_lengths):\n",
        "      \n",
        "        #text = [batch size,sent_length]\n",
        "        embedded = self.embedding(text)\n",
        "        #embedded = [batch size, sent_len, emb dim]\n",
        "        #print(embedded.shape)\n",
        "        #packed sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=True, enforce_sorted=False)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
        "        # print(packed_output.shape)\n",
        "        text, text_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
        "        #packed_output = self.lstm(embedded) #.view(len(text), 1, -1)\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\n",
        "        text, _ = self.atten(text, text_lengths) \n",
        "        # print('text: ', text.size())\n",
        "        # out2, (h_n, c_n) = self.lstm2(packed_output)\n",
        "        # y, lengths = nn.utils.rnn.pad_packed_sequence(out2, batch_first=True)\n",
        "        # y, _ = self.atten(y, lengths)\n",
        "        # print('y: ', y.size())\n",
        "\n",
        "        #conv_out = kmax_pooling((content_out),2,self.opt.kmax_pooling)\n",
        "        #concat the final forward and backward hidden state\n",
        "        #hidden = torch.cat([x, y], dim=1)\n",
        "        # hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
        "                \n",
        "        # hidden = [batch size, hid dim * num directions]\n",
        "        logits=self.fc(text)\n",
        "        # reshaped = hidden.view( hidden.size(0), -1)\n",
        "        # logits=self.fc(hidden) #.view(len(text),-1)\n",
        "\n",
        "        #Final activation function\n",
        "        probas=self.act(logits)\n",
        "\n",
        "        return logits, probas\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnsWXqX6jUS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "# def kmax_pooling(x, dim, k):\n",
        "#     index = x.topk(k, dim = dim)[1].sort(dim = dim)[0]\n",
        "#     return x.gather(dim, index)\n",
        "\n",
        "class TextCNN(torch.nn.Module): \n",
        "    def __init__(self, vocab_size, embedding_dim, num_filters, kernel_sizes,\n",
        "                 num_classes, freeze_embeddings=True):\n",
        "        super(TextCNN, self).__init__()\n",
        "        \n",
        "        \n",
        "#         self.num_classes = num_classes\n",
        "        # 1. embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        # set weights to pre-trained\n",
        "        self.embedding.weight = nn.Parameter(torch.from_numpy(word_vec_array)) # all vectors\n",
        "        # (optional) freeze embedding weights\n",
        "        if freeze_embeddings:\n",
        "            self.embedding.requires_grad = False\n",
        "\n",
        "        # 2. convolutional layers\n",
        "        self.convs = nn.ModuleList([\n",
        "                nn.Sequential(nn.Conv1d(in_channels = embedding_dim, \n",
        "                                        out_channels = covhidden_dim, \n",
        "                                        kernel_size = h),\n",
        "                              nn.BatchNorm1d(covhidden_dim), \n",
        "                              nn.ReLU(),\n",
        "                              nn.MaxPool1d(kernel_size = max_Length-h+1))\n",
        "                     for h in kernel_sizes\n",
        "                    ])\n",
        "        # self.convs_1d = nn.ModuleList([ nn.Sequential(                        \n",
        "        #     nn.Conv1d(1, num_filters, (k, embedding_dim), padding=(k-2,0)) \n",
        "        #     for k in kernel_sizes]\n",
        "        #     nn.BatchNorm1d(covhidden_dim),))\n",
        "        \n",
        "        # 3. final, fully-connected layer for classification\n",
        "        self.fc = nn.Sequential(\n",
        "                nn.Dropout(),\n",
        "                nn.Linear(len(kernel_sizes) * num_filters, linear_hidden_size),\n",
        "                nn.BatchNorm1d(linear_hidden_size),\n",
        "                nn.Dropout(),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(linear_hidden_size,num_classes)\n",
        "        )\n",
        "\n",
        "       \n",
        "        self.act = nn.Sigmoid()\n",
        "\n",
        "    def conv_and_pool(self, x, conv):\n",
        "      \"\"\"\n",
        "      Convolutional + max pooling layer\n",
        "      \"\"\"\n",
        "      # squeeze last dim to get size: (batch_size, num_filters, conv_seq_length)\n",
        "      # conv_seq_length will be ~ 200\n",
        "      x = F.relu(conv(x)).squeeze(3)\n",
        "      \n",
        "      # 1D pool over conv_seq_length\n",
        "      # squeeze to get size: (batch_size, num_filters)\n",
        "      x_max = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
        "      return x_max\n",
        " \n",
        "    def forward(self, text):\n",
        "      \n",
        "      #text = [batch size,sent_length]\n",
        "      embedded = self.embedding(text)\n",
        "      #  creates a channel dimension that conv layers expect\n",
        "      embedded = embedded.permute(0, 2, 1)\n",
        "      #embedded = [batch size, sent_len, emb dim]\n",
        "      # get output of each conv-pool layer\n",
        "      conv_results = [conv(embedded) for conv in self.convs]\n",
        "      # conv_results = [self.conv_and_pool(embedded, conv) for conv in self.convs_1d]\n",
        "      \n",
        "      # concatenate results and add dropout\n",
        "      text = torch.cat(conv_results, dim = 1)\n",
        "      text = text.view(-1, text.size(1)) \n",
        "      # final logit\n",
        "      logits = self.fc(text) \n",
        "\n",
        "      #Final activation function\n",
        "      probas=self.act(logits)\n",
        "\n",
        "      return logits, probas\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sntAFhWyXEr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def kmax_pooling(x, dim, k):\n",
        "    index = x.topk(k, dim = dim)[1].sort(dim = dim)[0]\n",
        "    return x.gather(dim, index)\n",
        "\n",
        "class RCNN(torch.nn.Module): \n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, covhidden_dim, \n",
        "                linear_hidden_size, kernel_size, kmax_pooling,num_classes, freeze_embeddings=True ):\n",
        "        super(RCNN, self).__init__()\n",
        "        \n",
        "        # self.kernel_size = kernel_size #[2,3,4]\n",
        "        self.kmax_pooling = kmax_pooling\n",
        "        # 1. embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        # set weights to pre-trained\n",
        "        self.embedding.weight = nn.Parameter(torch.from_numpy(word_vec_array)) # all vectors\n",
        "        # (optional) freeze embedding weights\n",
        "        if freeze_embeddings:\n",
        "            self.embedding.requires_grad = False\n",
        "\n",
        "        self.lstm =nn.LSTM( input_size = embedding_dim,\n",
        "                            hidden_size = hidden_size,\n",
        "                            num_layers = num_layers,\n",
        "                            bias = True,\n",
        "                            batch_first = True,\n",
        "                            dropout = 0.5,\n",
        "                            bidirectional = True\n",
        "                            )\n",
        "        \n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv1d(in_channels = hidden_size*2 ,\n",
        "                      out_channels = covhidden_dim,\n",
        "                      kernel_size =  kernel_size),\n",
        "            nn.BatchNorm1d(covhidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv1d(in_channels = covhidden_dim,\n",
        "                      out_channels = covhidden_dim,\n",
        "                      kernel_size =  kernel_size),\n",
        "            nn.BatchNorm1d(covhidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            \n",
        "            nn.MaxPool1d(kernel_size)\n",
        "        )\n",
        "        # self.dropout = nn.Dropout()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(self.kmax_pooling*covhidden_dim, linear_hidden_size),\n",
        "            nn.BatchNorm1d(linear_hidden_size),\n",
        "            nn.Dropout(),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(linear_hidden_size,num_classes)\n",
        "        )\n",
        "        # self.fc = nn.Linear(3 * (opt.title_dim+opt.content_dim), opt.num_classes)\n",
        "       \n",
        " \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        embedded = self.embedding(text)\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=True, enforce_sorted=False)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
        "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
        "        \n",
        "\n",
        "        # content_out = self.content_lstm(content.permute(1,0,2))[0].permute(1,2,0)\n",
        "        # content_em = (content).permute(0,2,1)\n",
        "        # content_out = t.cat((hidden,content_em),dim=1)\n",
        "\n",
        "\n",
        "        conv_out = self.conv(hidden) #kmax_pooling(self.conv(hidden),2,self.kmax_pooling)\n",
        "\n",
        "        \n",
        "        reshaped = conv_out.view(conv_out.size(0), -1)\n",
        "        logits = self.fc(reshaped)\n",
        "        probas=self.act(logits)\n",
        "        return logits, probas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "A7k-ELD2p7EW"
      },
      "source": [
        "# Instantiate the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JNLvp2Zyp7EW",
        "outputId": "292bba95-6ff3-4b9a-fd36-07721307f1ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# vocab_size, embedding_dim, hidden_size, num_layers, \n",
        "#                  bidirectional, dropout, linear_hidden_size, num_classes\n",
        "    \n",
        "# Instantiate the model with hyperparameters\n",
        "import random\n",
        "vocab_size = word_vec.shape[0]+1#len(google_vecs.vocab)\n",
        "#num_classes = 1 # binary class (1 or 0)\n",
        "embedding_dim = google_vecs.vector_size # 300-dim vectors\n",
        "hidden_size = 256 #LSTM hidden size\n",
        "num_layers=2 #LSTM layers\n",
        "linear_hidden_size = 200 # units number of full connected \n",
        "num_classes = one_hot.shape[1]\n",
        "num_filters = 100\n",
        "kernel_sizes = [3, 4, 5]\n",
        "kmax_pooling = 2\n",
        "covhidden_dim = 128\n",
        "\n",
        "# random.seed(RANDOM_SEED)\n",
        "# torch.manual_seed(RANDOM_SEED)  ###revise\n",
        "\n",
        "\n",
        "# torch.nn.init.xavier_uniform(m.weight.data)\n",
        "\n",
        "model = LSTMText1(vocab_size, embedding_dim, hidden_size, num_layers, \n",
        "                 linear_hidden_size, num_classes)\n",
        "\n",
        "# model = TextCNN(vocab_size, embedding_dim, num_filters, kernel_sizes,\n",
        "#                  num_classes)\n",
        "# model = model.float()\n",
        "\n",
        "print(model)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "#No. of trianable parameters\n",
        "def count_parameters(model):\n",
        "    return sum(m.numel() for m in model.parameters() if m.requires_grad)\n",
        "    \n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTMText1(\n",
            "  (embedding): Embedding(301, 300)\n",
            "  (lstm): LSTM(300, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
            "  (atten): Attention()\n",
            "  (fc): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=512, out_features=200, bias=True)\n",
            "    (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Dropout(p=0.5, inplace=False)\n",
            "    (5): Linear(in_features=200, out_features=27, bias=True)\n",
            "  )\n",
            "  (act): Sigmoid()\n",
            ")\n",
            "The model has 6,032,983 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DDXXgXiNp7EY",
        "colab": {}
      },
      "source": [
        "NUM_EPOCHS = 50\n",
        "intial_lr = 0.3\n",
        "criterion = nn.BCELoss()       \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=intial_lr, weight_decay=4e-4, momentum=0.9) #Adam \n",
        "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
        "#                                                    gamma=0.1,\n",
        "#                                                    last_epoch=-1)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10, gamma=0.5, last_epoch=-1)\n",
        "#scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlDvYpwLPVRH",
        "colab_type": "code",
        "outputId": "11045850-01dd-4d08-9988-ccfa1f803a8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "  for epoch in range(30):\n",
        "    if not epoch % 10 and epoch != 0:\n",
        "      print(epoch)\n",
        "probas = np.array([[0.25, 0.4, 0.6, 0.9, 0.12], [0.25, 0.4, 0.6, 0.9, 0.12], [0.25, 0.4, 0.6, 0.9, 0.12], [0.25, 0.4, 0.6, 0.9, 0.12]]  ) \n",
        "#(probas > 0.4).cpu().numpy().astype(np.uint8)   \n",
        "target_index = np.argsort(probas[1, :])#[-3:]  \n",
        "np.argsort(probas[1, :])[-3:]"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "h-rrGm1Sp7Ea"
      },
      "source": [
        "# Define loss function and evaluation parameters and Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpbGeCxenEnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_accuracy_and_loss(model, data_loader, device):\n",
        "    correct_pred, num_examples = 0, 0\n",
        "    loss = 0.\n",
        "    accuracy_list = []\n",
        "    f_measure_list = []\n",
        "    list_prob = []\n",
        "    for i, (features, targets, features_len) in enumerate(data_loader):\n",
        "            \n",
        "        features = features.to(device)\n",
        "        targets = targets.to(device)\n",
        "        features_len = features_len.to(device)\n",
        "        # print('features_len', features_len.shape)\n",
        "        # print('targets', targets.shape)\n",
        "        # print('features', features.shape)\n",
        "        logits, probas = model(features, features_len)\n",
        "        #list_prob.append(probas.cpu().numpy().tolist())\n",
        "        loss += criterion(probas, targets.float()).item()\n",
        "        # targets_arr = targets.cpu().numpy()\n",
        "        # # print(targets_arr)\n",
        "        # targets_num = np.count_nonzero(targets_arr, axis=1)\n",
        "        # # print(targets_num)\n",
        "        # probas_arr = probas.cpu().numpy() \n",
        "        # predicted_labels = np.zeros(probas_arr.shape)\n",
        "        # for i in range(probas_arr.shape[0]):\n",
        "        #   target_index = np.argsort(probas_arr[i, :])[-targets_num[i]:]\n",
        "        #   for j in target_index:\n",
        "        #     predicted_labels[i, j] = 1\n",
        "        predicted_labels = (probas > 0.6).cpu().numpy().astype(np.uint8)#(probas > 0.4).cpu().numpy()\n",
        "       \n",
        "        print('target:', targets[:20])\n",
        "        print('probas:', probas[:20])\n",
        "        #print(predicted_labels)\n",
        "        # print(precision_recall_fscore_support(targets.cpu().numpy(), predicted_labels,average='weighted')[:3])\n",
        "        num_examples += targets.size(0)\n",
        "        accuracy_list.append(accuracy_score(targets.cpu().numpy(), predicted_labels))\n",
        "        f_measure_list.append(list(precision_recall_fscore_support(targets.cpu().numpy(), predicted_labels,average='weighted')[:3]))\n",
        "    average_loss = loss/ num_examples\n",
        "    # accuracy = accuracy_score(targets.cpu().numpy(), predicted_labels)\n",
        "    # #ACU = roc_auc_score(targets.cpu().numpy(), predicted_labels)\n",
        "    # f_measure = precision_recall_fscore_support(targets.cpu().numpy(), predicted_labels,average='weighted')\n",
        "    accuracy = np.mean(np.array(accuracy_list))\n",
        "    f_measure = np.mean(np.array(f_measure_list), axis = 0)\n",
        "    return accuracy, average_loss, f_measure"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kAqtQ44mp7Ea",
        "outputId": "c36d1688-d3a0-4994-b2f7-77f691545555",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "start_time = time.time()\n",
        "train_acc_lst, valid_acc_lst = [], []\n",
        "train_loss_lst, valid_loss_lst = [], []\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch_idx, (features, targets, features_len) in enumerate(train_loader): #, \n",
        "\n",
        "        ### PREPARE MINIBATCH\n",
        "        features = features.to(DEVICE)\n",
        "        targets = targets.to(DEVICE)\n",
        "        features_len = features_len.to(DEVICE)\n",
        "#         features, features_lengths = batch.features\n",
        "#         targets = batch.targets\n",
        "        #print(features_len)\n",
        "        #retrieve text and no. of words\n",
        "        #print(features_len)\n",
        "            \n",
        "        ### FORWARD AND BACK PROP\n",
        "        features = features.view(features.size(0), -1)\n",
        "        logits, probas = model(features, features_len)\n",
        "        cost = criterion(probas, targets.float())\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        cost.backward()\n",
        "        \n",
        "        ### UPDATE MODEL PARAMETERS\n",
        "        optimizer.step()\n",
        "        # model.apply(weights_init)\n",
        "        ### LOGGING\n",
        "        if not batch_idx % 20:\n",
        "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
        "                   f'Batch {batch_idx:03d}/{len(train_loader):03d} |' \n",
        "                   f' Cost: {cost:.4f}')\n",
        "        if not epoch % 15 and epoch != 0:\n",
        "            scheduler.step()\n",
        "        \n",
        "    # no need to build the computation graph for backprop when computing accuracy\n",
        "    model.eval()\n",
        "    with torch.set_grad_enabled(False):\n",
        "        train_acc, train_loss, train_f_measure = compute_accuracy_and_loss(model, train_loader, device=DEVICE)\n",
        "        valid_acc, valid_loss, valid_f_measure = compute_accuracy_and_loss(model, valid_loader, device=DEVICE)\n",
        "        train_acc_lst.append(train_acc)\n",
        "        valid_acc_lst.append(valid_acc)\n",
        "        train_loss_lst.append(train_loss)\n",
        "        valid_loss_lst.append(valid_loss)\n",
        "        print(f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} Train Acc.: {train_acc:.2f}'\n",
        "              f' | Validation Acc.: {valid_acc:.2f}')\n",
        "\n",
        "    elapsed = (time.time() - start_time)/60\n",
        "    print(f'Time elapsed: {elapsed:.2f} min')\n",
        "    print('the precision_recall_fscore_support of F-score_train: ', train_f_measure)\n",
        "    print('the precision_recall_fscore_support of F-score_val: ', valid_f_measure)\n",
        "\n",
        "\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-f28d8ef41579>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m### FORWARD AND BACK PROP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-fb879fad5224>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text, text_lengths)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mpacked_embedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mpacked_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_embedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;31m# print(packed_output.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001b[0;32m--> 573\u001b[0;31m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[0m\u001b[1;32m    574\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_BAD_PARAM"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VqIyq4Mkp7Ed",
        "colab": {}
      },
      "source": [
        "plt.plot(range(1, NUM_EPOCHS+1), train_loss_lst, label='Training loss')\n",
        "plt.plot(range(1, NUM_EPOCHS+1), valid_loss_lst, label='Validation loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Binary cross entropy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbOiGm5InEn3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(range(1, NUM_EPOCHS+1), train_acc_lst, label='Training accuracy')\n",
        "plt.plot(range(1, NUM_EPOCHS+1), valid_acc_lst, label='Validation accuracy')\n",
        "plt.legend(loc='upper left')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()\n",
        "print(f'Training accuracy: {train_acc_lst[-1]:.2f}%')\n",
        "print(f'Validation accuracy: {valid_acc_lst[-1]:.2f}%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOioUK28nEn8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model.eval()\n",
        "with torch.set_grad_enabled(False): # save memory during inference\n",
        "    test_acc, test_loss, test_f_measure = compute_accuracy_and_loss(model, test_loader, DEVICE)\n",
        "    print(f'Test accuracy: {test_acc:.2f}%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SParg6smnEoK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, (features, targets, features_len) in enumerate(test_loader):\n",
        "            \n",
        "#         features = features.to(device)\n",
        "#         targets = targets.to(device)\n",
        "#         features_len = features_len.to(device)\n",
        "        print('features_len', features_len.shape)\n",
        "        print('targets', targets.shape)\n",
        "        print('features', features.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV67T1IhnEoO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}