{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9Da6mexrp-7L",
    "outputId": "74fa0a2a-fb4e-49f0-8116-c70a18849b45"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NPnQKgpbp7DO"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import re \n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LxuiAKnnp7DR"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WEHoFSmDp7DU"
   },
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# # Need to load the large model to get the vectors\n",
    "# import en_core_web_sm\n",
    "# nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QCC0-w4ap7DW"
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 123\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4rnUxezBRmFg",
    "outputId": "5933875a-4ba5-4b04-eec1-e03852d696c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UyUJdT3Gp7DY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# read the text file and add the column names\n",
    "read_file = pd.read_csv(r\"./booksummaries.txt\", sep='\t', header=None)\n",
    "#read_file = pd.read_csv(r\"/content/drive/My Drive/Colab Notebooks/final project/booksummaries.txt\", sep='\t', header=None)\n",
    "read_file.columns = ['ID', 'm number', 'book name', 'author name', 'date', 'label', 'summary']\n",
    "\n",
    "# clean data\n",
    "read_file['label'] = read_file['label'].str.replace(r'/m/\\S*\\s', '')\n",
    "read_file['label'] = read_file['label'].str.replace(r'{', '')\n",
    "read_file['label'] = read_file['label'].str.replace(r'}', '')\n",
    "read_file['label'] = read_file['label'].str.replace(r'}', '')\n",
    "read_file['label'] = read_file['label'].str.replace(r'\\\\u00e0\\s+clef', '')\n",
    "\n",
    "# select columns\n",
    "new_file = read_file.loc[:, ['book name', 'label', 'summary']]\n",
    "\n",
    "#delete the columns with no labels\n",
    "new_file.dropna(axis = 0, how = 'any', inplace = True)\n",
    "new_file = new_file.iloc[:, [0, 2, 1]]\n",
    "\n",
    "new_file = new_file.reset_index(drop=True)\n",
    "\n",
    "#output data as csv\n",
    "#new_file.to_csv(r'/content/drive/My Drive/Colab Notebooks/final project/booksummries.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "TABDJ59Rp7Da",
    "outputId": "0224e092-ce19-4ad5-e8cf-c54b4e874e15"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book name</th>\n",
       "      <th>summary</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Animal Farm</td>\n",
       "      <td>Old Major, the old boar on the Manor Farm, ca...</td>\n",
       "      <td>\"\"Roman \", \"\"Satire\", \"\"Children's literature\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Clockwork Orange</td>\n",
       "      <td>Alex, a teenager living in near-future Englan...</td>\n",
       "      <td>\"\"Science Fiction\", \"\"Novella\", \"\"Speculative ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Plague</td>\n",
       "      <td>The text of The Plague is divided into five p...</td>\n",
       "      <td>\"\"Existentialism\", \"\"Fiction\", \"\"Absurdist fic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Fire Upon the Deep</td>\n",
       "      <td>The novel posits that space around the Milky ...</td>\n",
       "      <td>\"\"Hard science fiction\", \"\"Science Fiction\", \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All Quiet on the Western Front</td>\n",
       "      <td>The book tells the story of Paul B채umer, a Ge...</td>\n",
       "      <td>\"\"War novel\", \"\"Roman \"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        book name  \\\n",
       "0                     Animal Farm   \n",
       "1              A Clockwork Orange   \n",
       "2                      The Plague   \n",
       "3            A Fire Upon the Deep   \n",
       "4  All Quiet on the Western Front   \n",
       "\n",
       "                                             summary  \\\n",
       "0   Old Major, the old boar on the Manor Farm, ca...   \n",
       "1   Alex, a teenager living in near-future Englan...   \n",
       "2   The text of The Plague is divided into five p...   \n",
       "3   The novel posits that space around the Milky ...   \n",
       "4   The book tells the story of Paul B채umer, a Ge...   \n",
       "\n",
       "                                               label  \n",
       "0  \"\"Roman \", \"\"Satire\", \"\"Children's literature\"...  \n",
       "1  \"\"Science Fiction\", \"\"Novella\", \"\"Speculative ...  \n",
       "2  \"\"Existentialism\", \"\"Fiction\", \"\"Absurdist fic...  \n",
       "3  \"\"Hard science fiction\", \"\"Science Fiction\", \"...  \n",
       "4                            \"\"War novel\", \"\"Roman \"  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rTtCwMPmp7Dd"
   },
   "outputs": [],
   "source": [
    "def text_process(label_list):\n",
    "    has_fiction = False\n",
    "    has_spec_fiction = False\n",
    "    has_novel = False\n",
    "    has_spec_novel = False\n",
    "    for i in range(len(label_list)):\n",
    "        if 'novel' in label_list[i].lower():\n",
    "            if 'novel' == label_list[i].lower():\n",
    "                has_novel = True\n",
    "            else:\n",
    "                has_spec_novel = True\n",
    "        if 'fiction' in label_list[i].lower():\n",
    "            if 'fiction' == label_list[i].lower():\n",
    "                has_fiction = True\n",
    "            else:\n",
    "                has_spec_fiction = True\n",
    "        \n",
    "    if has_spec_fiction and has_spec_novel:\n",
    "        if has_fiction:\n",
    "            label_list.remove('fiction')\n",
    "        if has_novel:\n",
    "            label_list.remove('novel')\n",
    "    elif has_spec_fiction:\n",
    "        if has_fiction:\n",
    "            label_list.remove('fiction')\n",
    "        if has_novel:\n",
    "            label_list.remove('novel')\n",
    "    elif has_spec_novel:\n",
    "        if has_fiction:\n",
    "            label_list.remove('fiction')\n",
    "        if has_novel:\n",
    "            label_list.remove('novel')\n",
    "    elif has_fiction and has_novel:\n",
    "        label_list.remove('fiction')\n",
    "    return label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C0wSYIeep7Df"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "for index in range(len(new_file['label'])):\n",
    "    label = new_file['label'][index].replace('\"', ''). lower()\n",
    "    label_list = re.split(', ', label)  \n",
    "    label_list = text_process(label_list)\n",
    "    new_file.xs(index)['label']= label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cYRkEBhDp7Dh"
   },
   "outputs": [],
   "source": [
    "#output data as csv\n",
    "new_file.to_csv(r'./booksummries.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "-gDhRTCap7Dj",
    "outputId": "f4af41ae-23a9-4000-ad16-8ffb7f5cce4e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book name</th>\n",
       "      <th>summary</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Animal Farm</td>\n",
       "      <td>Old Major, the old boar on the Manor Farm, ca...</td>\n",
       "      <td>[roman , satire, children's literature, specul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Clockwork Orange</td>\n",
       "      <td>Alex, a teenager living in near-future Englan...</td>\n",
       "      <td>[science fiction, novella, speculative fiction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Plague</td>\n",
       "      <td>The text of The Plague is divided into five p...</td>\n",
       "      <td>[existentialism, absurdist fiction]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Fire Upon the Deep</td>\n",
       "      <td>The novel posits that space around the Milky ...</td>\n",
       "      <td>[hard science fiction, science fiction, specul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All Quiet on the Western Front</td>\n",
       "      <td>The book tells the story of Paul B채umer, a Ge...</td>\n",
       "      <td>[war novel, roman ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        book name  \\\n",
       "0                     Animal Farm   \n",
       "1              A Clockwork Orange   \n",
       "2                      The Plague   \n",
       "3            A Fire Upon the Deep   \n",
       "4  All Quiet on the Western Front   \n",
       "\n",
       "                                             summary  \\\n",
       "0   Old Major, the old boar on the Manor Farm, ca...   \n",
       "1   Alex, a teenager living in near-future Englan...   \n",
       "2   The text of The Plague is divided into five p...   \n",
       "3   The novel posits that space around the Milky ...   \n",
       "4   The book tells the story of Paul B채umer, a Ge...   \n",
       "\n",
       "                                               label  \n",
       "0  [roman , satire, children's literature, specul...  \n",
       "1  [science fiction, novella, speculative fiction...  \n",
       "2                [existentialism, absurdist fiction]  \n",
       "3  [hard science fiction, science fiction, specul...  \n",
       "4                                [war novel, roman ]  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 576
    },
    "colab_type": "code",
    "id": "k5rWKWOFp7Dl",
    "outputId": "451d7b14-28bc-4174-f51f-2c3b8605d5de"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>#books</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>speculative fiction</td>\n",
       "      <td>4314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>science fiction</td>\n",
       "      <td>2870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>fantasy</td>\n",
       "      <td>2413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>children's literature</td>\n",
       "      <td>2122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>novel</td>\n",
       "      <td>1581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>mystery</td>\n",
       "      <td>1396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>fiction</td>\n",
       "      <td>1007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>young adult literature</td>\n",
       "      <td>825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>suspense</td>\n",
       "      <td>765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>crime fiction</td>\n",
       "      <td>753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>historical novel</td>\n",
       "      <td>654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>thriller</td>\n",
       "      <td>568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>horror</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>romance novel</td>\n",
       "      <td>435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>historical fiction</td>\n",
       "      <td>388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>detective fiction</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>adventure novel</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    labels  #books\n",
       "3      speculative fiction    4314\n",
       "4          science fiction    2870\n",
       "10                 fantasy    2413\n",
       "2    children's literature    2122\n",
       "33                   novel    1581\n",
       "19                 mystery    1396\n",
       "16                 fiction    1007\n",
       "54  young adult literature     825\n",
       "28                suspense     765\n",
       "47           crime fiction     753\n",
       "32        historical novel     654\n",
       "53                thriller     568\n",
       "17                  horror     511\n",
       "41           romance novel     435\n",
       "29      historical fiction     388\n",
       "27       detective fiction     341\n",
       "30         adventure novel     330"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = list(new_file['label'].values)\n",
    "all_labels = [ word for labels in categories for word in labels]\n",
    "counts = Counter(all_labels)\n",
    "# for i in categories:\n",
    "#     counts.append((i, new_file['label'][i].sum()))\n",
    "df_stats = pd.DataFrame(counts.items(), columns=['labels', '#books'])\n",
    "df_stats = df_stats.sort_values(by = '#books', ascending = False)\n",
    "df_stats.head(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 703
    },
    "colab_type": "code",
    "id": "PzGbvwSBp7Dn",
    "outputId": "229323b7-5f00-4ee4-a93f-4de80052742c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a297ade50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_stats.iloc[:50, :].plot(x='labels', y='#books', kind='bar', legend=False, grid=True, figsize=(15, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WCODjGEFp7Dp"
   },
   "source": [
    "  ## Encoding the Labels##\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-DQcm51Up7Dq"
   },
   "outputs": [],
   "source": [
    "label_list = []\n",
    "for index in range(len(new_file['label'])):\n",
    "    object_label = new_file['label'][index]\n",
    "    for l in object_label:\n",
    "        if l not in label_list:\n",
    "            label_list.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dD79YUT-p7Dr",
    "outputId": "90fa1ea7-07fc-4b77-e5e4-5759141fa4bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NCpBJIs7p7Dt"
   },
   "outputs": [],
   "source": [
    "one_hot = pd.DataFrame(np.zeros((12841, 227)), columns=label_list).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8E5hJZ6Lp7Dv"
   },
   "outputs": [],
   "source": [
    "for index in range(len(new_file['label'])):\n",
    "    object_label = new_file['label'][index]\n",
    "    for l in object_label:\n",
    "        one_hot[l][index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "colab_type": "code",
    "id": "yYEZmlRjp7Dx",
    "outputId": "76b2ca3e-44aa-4764-a96f-246902b27ba6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roman</th>\n",
       "      <th>satire</th>\n",
       "      <th>children's literature</th>\n",
       "      <th>speculative fiction</th>\n",
       "      <th>science fiction</th>\n",
       "      <th>novella</th>\n",
       "      <th>utopian and dystopian fiction</th>\n",
       "      <th>existentialism</th>\n",
       "      <th>absurdist fiction</th>\n",
       "      <th>hard science fiction</th>\n",
       "      <th>...</th>\n",
       "      <th>encyclopedia</th>\n",
       "      <th>mashup</th>\n",
       "      <th>biopunk</th>\n",
       "      <th>popular culture</th>\n",
       "      <th>neuroscience</th>\n",
       "      <th>new york times best seller list</th>\n",
       "      <th>epic science fiction and fantasy</th>\n",
       "      <th>alien invasion</th>\n",
       "      <th>prose</th>\n",
       "      <th>pastiche</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12836</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12837</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12838</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12839</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12840</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12841 rows  227 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       roman   satire  children's literature  speculative fiction  \\\n",
       "0           1       1                      1                    1   \n",
       "1           0       1                      0                    1   \n",
       "2           0       0                      0                    0   \n",
       "3           0       0                      0                    1   \n",
       "4           1       0                      0                    0   \n",
       "...       ...     ...                    ...                  ...   \n",
       "12836       0       0                      0                    0   \n",
       "12837       0       0                      0                    0   \n",
       "12838       0       0                      0                    0   \n",
       "12839       0       0                      0                    0   \n",
       "12840       0       0                      0                    1   \n",
       "\n",
       "       science fiction  novella  utopian and dystopian fiction  \\\n",
       "0                    0        0                              0   \n",
       "1                    1        1                              1   \n",
       "2                    0        0                              0   \n",
       "3                    1        0                              0   \n",
       "4                    0        0                              0   \n",
       "...                ...      ...                            ...   \n",
       "12836                1        0                              0   \n",
       "12837                0        0                              0   \n",
       "12838                0        0                              0   \n",
       "12839                0        0                              0   \n",
       "12840                0        0                              0   \n",
       "\n",
       "       existentialism  absurdist fiction  hard science fiction  ...  \\\n",
       "0                   0                  0                     0  ...   \n",
       "1                   0                  0                     0  ...   \n",
       "2                   1                  1                     0  ...   \n",
       "3                   0                  0                     1  ...   \n",
       "4                   0                  0                     0  ...   \n",
       "...               ...                ...                   ...  ...   \n",
       "12836               0                  0                     0  ...   \n",
       "12837               0                  0                     0  ...   \n",
       "12838               0                  0                     0  ...   \n",
       "12839               0                  0                     0  ...   \n",
       "12840               0                  0                     0  ...   \n",
       "\n",
       "       encyclopedia  mashup  biopunk  popular culture  neuroscience  \\\n",
       "0                 0       0        0                0             0   \n",
       "1                 0       0        0                0             0   \n",
       "2                 0       0        0                0             0   \n",
       "3                 0       0        0                0             0   \n",
       "4                 0       0        0                0             0   \n",
       "...             ...     ...      ...              ...           ...   \n",
       "12836             0       0        0                0             0   \n",
       "12837             0       0        0                0             0   \n",
       "12838             0       0        0                0             0   \n",
       "12839             0       0        0                0             0   \n",
       "12840             0       0        0                0             0   \n",
       "\n",
       "       new york times best seller list  epic science fiction and fantasy  \\\n",
       "0                                    0                                 0   \n",
       "1                                    0                                 0   \n",
       "2                                    0                                 0   \n",
       "3                                    0                                 0   \n",
       "4                                    0                                 0   \n",
       "...                                ...                               ...   \n",
       "12836                                0                                 0   \n",
       "12837                                0                                 0   \n",
       "12838                                0                                 0   \n",
       "12839                                0                                 0   \n",
       "12840                                0                                 0   \n",
       "\n",
       "       alien invasion  prose  pastiche  \n",
       "0                   0      0         0  \n",
       "1                   0      0         0  \n",
       "2                   0      0         0  \n",
       "3                   0      0         0  \n",
       "4                   0      0         0  \n",
       "...               ...    ...       ...  \n",
       "12836               0      0         0  \n",
       "12837               0      0         0  \n",
       "12838               0      0         0  \n",
       "12839               0      0         0  \n",
       "12840               0      0         0  \n",
       "\n",
       "[12841 rows x 227 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_CgvEkARp7Dz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8p08_5W-p7D0"
   },
   "source": [
    "## Split the words##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "Xtaj1BWMqWKE",
    "outputId": "c81481a4-2d99-459c-f1a6-13cc3a998dda"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/yuting/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yuting/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "g4kaZS71p7D1",
    "outputId": "6b005c47-78aa-4d48-e15d-d01e691bf7a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total number of books: 12841\n",
      "\n",
      "1301\n",
      "(12840, 227)\n",
      "1708\n",
      "(12839, 227)\n",
      "2487\n",
      "(12838, 227)\n",
      "2553\n",
      "(12837, 227)\n",
      "2898\n",
      "(12836, 227)\n",
      "3243\n",
      "(12835, 227)\n",
      "3382\n",
      "(12834, 227)\n",
      "3561\n",
      "(12833, 227)\n",
      "4383\n",
      "(12832, 227)\n",
      "4663\n",
      "(12831, 227)\n",
      "4747\n",
      "(12830, 227)\n",
      "4912\n",
      "(12829, 227)\n",
      "4992\n",
      "(12828, 227)\n",
      "5317\n",
      "(12827, 227)\n",
      "5382\n",
      "(12826, 227)\n",
      "5440\n",
      "(12825, 227)\n",
      "5442\n",
      "(12824, 227)\n",
      "5550\n",
      "(12823, 227)\n",
      "5576\n",
      "(12822, 227)\n",
      "5581\n",
      "(12821, 227)\n",
      "5585\n",
      "(12820, 227)\n",
      "6609\n",
      "(12819, 227)\n",
      "7354\n",
      "(12818, 227)\n",
      "7372\n",
      "(12817, 227)\n",
      "7600\n",
      "(12816, 227)\n",
      "7805\n",
      "(12815, 227)\n",
      "8059\n",
      "(12814, 227)\n",
      "8104\n",
      "(12813, 227)\n",
      "8372\n",
      "(12812, 227)\n",
      "8386\n",
      "(12811, 227)\n",
      "8542\n",
      "(12810, 227)\n",
      "8543\n",
      "(12809, 227)\n",
      "8544\n",
      "(12808, 227)\n",
      "8545\n",
      "(12807, 227)\n",
      "8546\n",
      "(12806, 227)\n",
      "8547\n",
      "(12805, 227)\n",
      "8548\n",
      "(12804, 227)\n",
      "8549\n",
      "(12803, 227)\n",
      "8550\n",
      "(12802, 227)\n",
      "8551\n",
      "(12801, 227)\n",
      "8562\n",
      "(12800, 227)\n",
      "8563\n",
      "(12799, 227)\n",
      "8769\n",
      "(12798, 227)\n",
      "8770\n",
      "(12797, 227)\n",
      "8771\n",
      "(12796, 227)\n",
      "8772\n",
      "(12795, 227)\n",
      "8785\n",
      "(12794, 227)\n",
      "8821\n",
      "(12793, 227)\n",
      "9081\n",
      "(12792, 227)\n",
      "9089\n",
      "(12791, 227)\n",
      "9118\n",
      "(12790, 227)\n",
      "9128\n",
      "(12789, 227)\n",
      "9157\n",
      "(12788, 227)\n",
      "9159\n",
      "(12787, 227)\n",
      "9160\n",
      "(12786, 227)\n",
      "9244\n",
      "(12785, 227)\n",
      "9256\n",
      "(12784, 227)\n",
      "9347\n",
      "(12783, 227)\n",
      "9552\n",
      "(12782, 227)\n",
      "9656\n",
      "(12781, 227)\n",
      "9719\n",
      "(12780, 227)\n",
      "9733\n",
      "(12779, 227)\n",
      "9814\n",
      "(12778, 227)\n",
      "9825\n",
      "(12777, 227)\n",
      "9832\n",
      "(12776, 227)\n",
      "9834\n",
      "(12775, 227)\n",
      "9849\n",
      "(12774, 227)\n",
      "9856\n",
      "(12773, 227)\n",
      "9989\n",
      "(12772, 227)\n",
      "9991\n",
      "(12771, 227)\n",
      "9994\n",
      "(12770, 227)\n",
      "10008\n",
      "(12769, 227)\n",
      "10013\n",
      "(12768, 227)\n",
      "10014\n",
      "(12767, 227)\n",
      "10022\n",
      "(12766, 227)\n",
      "10026\n",
      "(12765, 227)\n",
      "10032\n",
      "(12764, 227)\n",
      "10040\n",
      "(12763, 227)\n",
      "10060\n",
      "(12762, 227)\n",
      "10065\n",
      "(12761, 227)\n",
      "10072\n",
      "(12760, 227)\n",
      "10087\n",
      "(12759, 227)\n",
      "10101\n",
      "(12758, 227)\n",
      "10105\n",
      "(12757, 227)\n",
      "10113\n",
      "(12756, 227)\n",
      "10126\n",
      "(12755, 227)\n",
      "10217\n",
      "(12754, 227)\n",
      "10239\n",
      "(12753, 227)\n",
      "10246\n",
      "(12752, 227)\n",
      "10254\n",
      "(12751, 227)\n",
      "10263\n",
      "(12750, 227)\n",
      "10543\n",
      "(12749, 227)\n",
      "10563\n",
      "(12748, 227)\n",
      "10590\n",
      "(12747, 227)\n",
      "10755\n",
      "(12746, 227)\n",
      "11137\n",
      "(12745, 227)\n",
      "11196\n",
      "(12744, 227)\n",
      "11319\n",
      "(12743, 227)\n",
      "11512\n",
      "(12742, 227)\n",
      "12162\n",
      "(12741, 227)\n",
      "12293\n",
      "(12740, 227)\n",
      "12359\n",
      "(12739, 227)\n",
      "12677\n",
      "(12738, 227)\n",
      "12678\n",
      "(12737, 227)\n",
      "12826\n",
      "(12736, 227)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "#def words_process(new_file):\n",
    "book_summaries = new_file['summary']\n",
    "summary_list = [summary for summary in book_summaries]\n",
    "summary_num = len(summary_list)\n",
    "#summaries = ''.join(summary_list)\n",
    "print(\"the total number of books: {}\\n\".format(summary_num))\n",
    "\n",
    "all_docs = []\n",
    "i_index = 0\n",
    "for doc in summary_list:\n",
    "    # Tokenize the string into words\n",
    "    tokens = word_tokenize(doc)\n",
    "    # Remove non-alphabetic tokens, such as punctuation\n",
    "    words = [word.lower() for word in tokens if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if not word in stop_words]\n",
    "    if len(words) >= 9:\n",
    "      all_docs.append(words)\n",
    "    else:\n",
    "      print(i_index)\n",
    "      one_hot = one_hot.drop(i_index, axis=0)\n",
    "      print(one_hot.shape)\n",
    "    i_index += 1\n",
    "    #return all_docs, one_hot_delt\n",
    "\n",
    "one_hot = one_hot.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 576
    },
    "colab_type": "code",
    "id": "cFlg1asUnghh",
    "outputId": "3798e64b-033b-4068-d277-8dd4494f6e9b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len</th>\n",
       "      <th>#books</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>11</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>13</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>14</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>15</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>16</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>17</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>18</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>19</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>20</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>21</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>22</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>23</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>24</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>25</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     len  #books\n",
       "925    9      18\n",
       "883   10      31\n",
       "597   11      31\n",
       "732   12      31\n",
       "636   13      32\n",
       "393   14      33\n",
       "823   15      35\n",
       "746   16      27\n",
       "795   17      33\n",
       "726   18      28\n",
       "768   19      37\n",
       "418   20      54\n",
       "854   21      55\n",
       "813   22      44\n",
       "715   23      37\n",
       "559   24      38\n",
       "461   25      51"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = []\n",
    "for i in range(len(all_docs)):\n",
    "  l1.append(len(all_docs[i]))\n",
    "#print(l1.index(6))\n",
    "len_counts = Counter(l1)\n",
    "df_len = pd.DataFrame(len_counts.items(), columns=['len', '#books'])\n",
    "df_len = df_len.sort_values(by = 'len', ascending = True)\n",
    "#print(min(l1))\n",
    "df_len.head(17)\n",
    "\n",
    "#one_hot = one_hot.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "kGpKgbGwj0wU",
    "outputId": "20abda67-5a32-4193-fc03-a661bbb06f8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12736, 227)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(all_docs))\n",
    "\n",
    "\n",
    "one_hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7tn7GXCqp7D4"
   },
   "source": [
    "## outliners ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aRZEVHYrp7D5"
   },
   "source": [
    "## Using a Pre-Trained Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "xCtCbDx_p7D5",
    "outputId": "e8e70a11-0023-46ae-8d41-8732af972ee5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51247"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load word2vec model (trained on an enormous Google corpus)\n",
    "google_vecs = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary = True)\n",
    "#google_vecs = KeyedVectors.load_word2vec_format('/content/drive/My Drive/Colab Notebooks/final project/GoogleNews-vectors-negative300.bin', binary = True)\n",
    "embedding_dim_dim = google_vecs.vector_size\n",
    "\n",
    "# def words_embedding(docs):\n",
    "   \n",
    "all_words = [ word for doc in all_docs for word in doc]\n",
    "all_words_nodup = list(dict.fromkeys(all_words))\n",
    "\n",
    "def words_embedding(doc):\n",
    "    # Filter the list of vectors to include only those that Word2Vec has a vector for\n",
    "    vector_list = [google_vecs[word] for word in doc if word in google_vecs.vocab]\n",
    "    #google_vectors = np.asarray(vector_list)\n",
    "    # Create a list of the words corresponding to these vectors\n",
    "    words_filtered = [word for word in doc if word in google_vecs.vocab]\n",
    "    #Zip the words together with their vector representations\n",
    "    word_vec_zip = zip(words_filtered, vector_list)\n",
    "\n",
    "    # Cast to a dict so we can turn it into a DataFrame\n",
    "    word_vec_dict = dict(word_vec_zip)\n",
    "    word_vec = pd.DataFrame.from_dict(word_vec_dict, orient='index')\n",
    "    return word_vec\n",
    "\n",
    "word_vec  = words_embedding(all_words_nodup)\n",
    "word_vec_array = np.array(word_vec)\n",
    "word_vec.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OPJrtUy8Qg1p",
    "outputId": "8f1af7cc-27ca-4f01-8ccb-fe148825d81c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101896"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(all_words_nodup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "colab_type": "code",
    "id": "4mxTtNdaS4zY",
    "outputId": "633f5d31-a35a-4324-8e06-04eed3d15943"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>old</th>\n",
       "      <td>0.257812</td>\n",
       "      <td>0.116699</td>\n",
       "      <td>0.198242</td>\n",
       "      <td>-0.176758</td>\n",
       "      <td>0.116211</td>\n",
       "      <td>0.153320</td>\n",
       "      <td>-0.073242</td>\n",
       "      <td>-0.108398</td>\n",
       "      <td>0.065430</td>\n",
       "      <td>0.120605</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019531</td>\n",
       "      <td>-0.019897</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>-0.136719</td>\n",
       "      <td>-0.106934</td>\n",
       "      <td>-0.116699</td>\n",
       "      <td>0.012756</td>\n",
       "      <td>-0.017700</td>\n",
       "      <td>0.028076</td>\n",
       "      <td>0.082031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>major</th>\n",
       "      <td>-0.130859</td>\n",
       "      <td>0.006134</td>\n",
       "      <td>0.013855</td>\n",
       "      <td>0.090820</td>\n",
       "      <td>-0.173828</td>\n",
       "      <td>-0.244141</td>\n",
       "      <td>-0.061035</td>\n",
       "      <td>-0.111328</td>\n",
       "      <td>0.255859</td>\n",
       "      <td>0.165039</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.150391</td>\n",
       "      <td>-0.123535</td>\n",
       "      <td>0.046143</td>\n",
       "      <td>-0.102539</td>\n",
       "      <td>0.052002</td>\n",
       "      <td>-0.046875</td>\n",
       "      <td>-0.121582</td>\n",
       "      <td>0.026855</td>\n",
       "      <td>0.131836</td>\n",
       "      <td>0.023315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boar</th>\n",
       "      <td>-0.012695</td>\n",
       "      <td>-0.029419</td>\n",
       "      <td>-0.248047</td>\n",
       "      <td>0.135742</td>\n",
       "      <td>0.166016</td>\n",
       "      <td>0.298828</td>\n",
       "      <td>0.114258</td>\n",
       "      <td>0.146484</td>\n",
       "      <td>0.024902</td>\n",
       "      <td>0.197266</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.159180</td>\n",
       "      <td>0.109863</td>\n",
       "      <td>-0.200195</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>-0.048340</td>\n",
       "      <td>-0.069336</td>\n",
       "      <td>-0.038818</td>\n",
       "      <td>-0.019775</td>\n",
       "      <td>0.134766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manor</th>\n",
       "      <td>0.039795</td>\n",
       "      <td>-0.123535</td>\n",
       "      <td>-0.105957</td>\n",
       "      <td>-0.208984</td>\n",
       "      <td>0.267578</td>\n",
       "      <td>0.200195</td>\n",
       "      <td>-0.027344</td>\n",
       "      <td>0.104492</td>\n",
       "      <td>-0.039795</td>\n",
       "      <td>-0.132812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.202148</td>\n",
       "      <td>0.133789</td>\n",
       "      <td>-0.151367</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>-0.058594</td>\n",
       "      <td>-0.055908</td>\n",
       "      <td>-0.033447</td>\n",
       "      <td>-0.039307</td>\n",
       "      <td>0.279297</td>\n",
       "      <td>0.128906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>farm</th>\n",
       "      <td>0.075684</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>-0.035156</td>\n",
       "      <td>-0.024048</td>\n",
       "      <td>-0.065918</td>\n",
       "      <td>0.185547</td>\n",
       "      <td>0.015991</td>\n",
       "      <td>0.062012</td>\n",
       "      <td>-0.019531</td>\n",
       "      <td>-0.140625</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.296875</td>\n",
       "      <td>0.139648</td>\n",
       "      <td>-0.114746</td>\n",
       "      <td>0.125977</td>\n",
       "      <td>-0.099121</td>\n",
       "      <td>0.104980</td>\n",
       "      <td>-0.222656</td>\n",
       "      <td>-0.104980</td>\n",
       "      <td>0.285156</td>\n",
       "      <td>-0.148438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "old    0.257812  0.116699  0.198242 -0.176758  0.116211  0.153320 -0.073242   \n",
       "major -0.130859  0.006134  0.013855  0.090820 -0.173828 -0.244141 -0.061035   \n",
       "boar  -0.012695 -0.029419 -0.248047  0.135742  0.166016  0.298828  0.114258   \n",
       "manor  0.039795 -0.123535 -0.105957 -0.208984  0.267578  0.200195 -0.027344   \n",
       "farm   0.075684  0.078125 -0.035156 -0.024048 -0.065918  0.185547  0.015991   \n",
       "\n",
       "            7         8         9    ...       290       291       292  \\\n",
       "old   -0.108398  0.065430  0.120605  ... -0.019531 -0.019897  0.027344   \n",
       "major -0.111328  0.255859  0.165039  ... -0.150391 -0.123535  0.046143   \n",
       "boar   0.146484  0.024902  0.197266  ... -0.159180  0.109863 -0.200195   \n",
       "manor  0.104492 -0.039795 -0.132812  ... -0.202148  0.133789 -0.151367   \n",
       "farm   0.062012 -0.019531 -0.140625  ... -0.296875  0.139648 -0.114746   \n",
       "\n",
       "            293       294       295       296       297       298       299  \n",
       "old   -0.136719 -0.106934 -0.116699  0.012756 -0.017700  0.028076  0.082031  \n",
       "major -0.102539  0.052002 -0.046875 -0.121582  0.026855  0.131836  0.023315  \n",
       "boar   0.156250  0.074219 -0.048340 -0.069336 -0.038818 -0.019775  0.134766  \n",
       "manor  0.218750 -0.058594 -0.055908 -0.033447 -0.039307  0.279297  0.128906  \n",
       "farm   0.125977 -0.099121  0.104980 -0.222656 -0.104980  0.285156 -0.148438  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TyVlCnwUQGFh"
   },
   "outputs": [],
   "source": [
    "# word_vec.to_csv(r'./wor_vec.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zAE7B160QJHI"
   },
   "outputs": [],
   "source": [
    "#word_vec = pd.read_csv(r\"/content/drive/My Drive/Colab Notebooks/final project/wor_vec.csv\", header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1zaxCSpzp7D8"
   },
   "source": [
    "# Generate the integer vectors of all summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VjTTRo8pp7D8"
   },
   "outputs": [],
   "source": [
    "word_index = pd.DataFrame(index = word_vec.index)\n",
    "wordlen_list = range(1, word_vec.shape[0]+1)\n",
    "word_index['index'] = wordlen_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Aoi4c-sTp7D-",
    "outputId": "28c483d1-661a-4a6e-d813-77c2fe71bd18"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[322,\n",
       " 323,\n",
       " 324,\n",
       " 15,\n",
       " 325,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 322,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 336,\n",
       " 326,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 342,\n",
       " 343,\n",
       " 332,\n",
       " 249,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 322,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 344,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 169,\n",
       " 333,\n",
       " 107,\n",
       " 332,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 251,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 192,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 112,\n",
       " 326,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 18,\n",
       " 390,\n",
       " 324,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 375,\n",
       " 394,\n",
       " 395,\n",
       " 392,\n",
       " 396,\n",
       " 166,\n",
       " 397,\n",
       " 362,\n",
       " 398,\n",
       " 399,\n",
       " 322,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 333,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 397,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 322,\n",
       " 410,\n",
       " 335,\n",
       " 411,\n",
       " 412,\n",
       " 307,\n",
       " 326,\n",
       " 300,\n",
       " 413,\n",
       " 369,\n",
       " 414,\n",
       " 415,\n",
       " 322,\n",
       " 342,\n",
       " 416,\n",
       " 353,\n",
       " 417,\n",
       " 418,\n",
       " 419,\n",
       " 126,\n",
       " 420,\n",
       " 421,\n",
       " 322,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 311,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 322,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 58,\n",
       " 407,\n",
       " 433,\n",
       " 415,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 322,\n",
       " 439,\n",
       " 332,\n",
       " 440,\n",
       " 441,\n",
       " 338,\n",
       " 442,\n",
       " 322,\n",
       " 50,\n",
       " 326,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 322,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 335,\n",
       " 449,\n",
       " 450,\n",
       " 338,\n",
       " 144,\n",
       " 451,\n",
       " 58,\n",
       " 409,\n",
       " 322,\n",
       " 452,\n",
       " 311,\n",
       " 338,\n",
       " 87,\n",
       " 453,\n",
       " 369,\n",
       " 454,\n",
       " 1,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 322,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 455,\n",
       " 464,\n",
       " 465,\n",
       " 335,\n",
       " 148,\n",
       " 322,\n",
       " 374,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 473,\n",
       " 322,\n",
       " 474,\n",
       " 445,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 353,\n",
       " 479,\n",
       " 480,\n",
       " 181,\n",
       " 481,\n",
       " 482,\n",
       " 472,\n",
       " 483,\n",
       " 484,\n",
       " 322,\n",
       " 485,\n",
       " 486,\n",
       " 487,\n",
       " 488,\n",
       " 322,\n",
       " 489,\n",
       " 42,\n",
       " 490,\n",
       " 93,\n",
       " 491,\n",
       " 322,\n",
       " 492,\n",
       " 493,\n",
       " 494,\n",
       " 391,\n",
       " 495,\n",
       " 496,\n",
       " 196,\n",
       " 497,\n",
       " 498,\n",
       " 499,\n",
       " 500,\n",
       " 362,\n",
       " 501,\n",
       " 501,\n",
       " 502,\n",
       " 503,\n",
       " 504,\n",
       " 322,\n",
       " 505,\n",
       " 506,\n",
       " 125,\n",
       " 507,\n",
       " 508,\n",
       " 509,\n",
       " 510,\n",
       " 93,\n",
       " 511,\n",
       " 297,\n",
       " 512,\n",
       " 513,\n",
       " 514,\n",
       " 515,\n",
       " 516,\n",
       " 517,\n",
       " 518,\n",
       " 421,\n",
       " 519,\n",
       " 520,\n",
       " 521,\n",
       " 147,\n",
       " 522,\n",
       " 322,\n",
       " 523,\n",
       " 524,\n",
       " 525,\n",
       " 416,\n",
       " 353,\n",
       " 526,\n",
       " 501,\n",
       " 527,\n",
       " 528,\n",
       " 529,\n",
       " 322,\n",
       " 167,\n",
       " 530,\n",
       " 531,\n",
       " 18,\n",
       " 455,\n",
       " 532,\n",
       " 533,\n",
       " 534,\n",
       " 535,\n",
       " 536,\n",
       " 537,\n",
       " 153,\n",
       " 472,\n",
       " 483,\n",
       " 110,\n",
       " 538,\n",
       " 539,\n",
       " 322,\n",
       " 540,\n",
       " 262,\n",
       " 541,\n",
       " 542,\n",
       " 543,\n",
       " 544,\n",
       " 322,\n",
       " 545,\n",
       " 268,\n",
       " 546,\n",
       " 433,\n",
       " 547,\n",
       " 548,\n",
       " 549,\n",
       " 322,\n",
       " 550,\n",
       " 551,\n",
       " 552,\n",
       " 370,\n",
       " 371,\n",
       " 553,\n",
       " 202,\n",
       " 554,\n",
       " 555,\n",
       " 556,\n",
       " 557,\n",
       " 558,\n",
       " 559,\n",
       " 1,\n",
       " 367,\n",
       " 560,\n",
       " 561,\n",
       " 562,\n",
       " 563,\n",
       " 564,\n",
       " 565,\n",
       " 322,\n",
       " 566,\n",
       " 331,\n",
       " 567,\n",
       " 568,\n",
       " 322,\n",
       " 186,\n",
       " 24,\n",
       " 569,\n",
       " 335,\n",
       " 570,\n",
       " 326,\n",
       " 112,\n",
       " 17,\n",
       " 567,\n",
       " 170,\n",
       " 322,\n",
       " 571,\n",
       " 572,\n",
       " 573,\n",
       " 574,\n",
       " 376,\n",
       " 322,\n",
       " 167,\n",
       " 575,\n",
       " 387,\n",
       " 388,\n",
       " 576,\n",
       " 577,\n",
       " 578,\n",
       " 332,\n",
       " 579,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 326,\n",
       " 583,\n",
       " 584,\n",
       " 366,\n",
       " 396,\n",
       " 585,\n",
       " 322,\n",
       " 396,\n",
       " 532,\n",
       " 229,\n",
       " 586,\n",
       " 587,\n",
       " 588,\n",
       " 322,\n",
       " 589,\n",
       " 512,\n",
       " 590,\n",
       " 586,\n",
       " 587,\n",
       " 591,\n",
       " 164,\n",
       " 592,\n",
       " 392,\n",
       " 593,\n",
       " 594,\n",
       " 324,\n",
       " 595,\n",
       " 596,\n",
       " 597,\n",
       " 163,\n",
       " 598,\n",
       " 141,\n",
       " 587,\n",
       " 599,\n",
       " 262,\n",
       " 553,\n",
       " 600,\n",
       " 322,\n",
       " 601,\n",
       " 538,\n",
       " 602,\n",
       " 603,\n",
       " 310,\n",
       " 604,\n",
       " 262,\n",
       " 297,\n",
       " 107,\n",
       " 605,\n",
       " 322,\n",
       " 606,\n",
       " 607,\n",
       " 364,\n",
       " 17,\n",
       " 200,\n",
       " 608,\n",
       " 147,\n",
       " 587,\n",
       " 609,\n",
       " 610,\n",
       " 611,\n",
       " 612,\n",
       " 613,\n",
       " 322,\n",
       " 614,\n",
       " 587,\n",
       " 369,\n",
       " 615,\n",
       " 616,\n",
       " 570,\n",
       " 369,\n",
       " 322,\n",
       " 617,\n",
       " 618,\n",
       " 619,\n",
       " 416,\n",
       " 353,\n",
       " 620,\n",
       " 621,\n",
       " 557,\n",
       " 622,\n",
       " 623,\n",
       " 624,\n",
       " 322,\n",
       " 625,\n",
       " 190,\n",
       " 626,\n",
       " 262,\n",
       " 541,\n",
       " 627,\n",
       " 628,\n",
       " 629,\n",
       " 630,\n",
       " 631,\n",
       " 557,\n",
       " 621,\n",
       " 587,\n",
       " 632,\n",
       " 633,\n",
       " 634,\n",
       " 635,\n",
       " 322,\n",
       " 636,\n",
       " 445,\n",
       " 497,\n",
       " 637,\n",
       " 262,\n",
       " 638,\n",
       " 639,\n",
       " 640,\n",
       " 322,\n",
       " 641,\n",
       " 420,\n",
       " 421,\n",
       " 642,\n",
       " 512,\n",
       " 643,\n",
       " 644,\n",
       " 645,\n",
       " 646,\n",
       " 647,\n",
       " 322,\n",
       " 648,\n",
       " 649,\n",
       " 332,\n",
       " 439,\n",
       " 650,\n",
       " 651,\n",
       " 344,\n",
       " 421,\n",
       " 652,\n",
       " 653,\n",
       " 340,\n",
       " 654,\n",
       " 655,\n",
       " 656,\n",
       " 322,\n",
       " 657,\n",
       " 375,\n",
       " 658,\n",
       " 659,\n",
       " 660,\n",
       " 661,\n",
       " 662,\n",
       " 349,\n",
       " 663,\n",
       " 322,\n",
       " 664,\n",
       " 665]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def words_index(all_docs):\n",
    "    all_words_index = []\n",
    "    for doc in all_docs:\n",
    "        inds = []\n",
    "        for word in doc:\n",
    "            try:\n",
    "                inds.append(word_index.at[word, 'index'])\n",
    "            except: \n",
    "                continue\n",
    "            \n",
    "        all_words_index.append(inds)\n",
    "    return all_words_index\n",
    "\n",
    "all_words_index = words_index(all_docs)\n",
    "all_words_index[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rlA2ucnkp7EA"
   },
   "outputs": [],
   "source": [
    "# l = []\n",
    "# for i in range(len(all_words_index)):\n",
    "#     l.append(len(all_words_index[i]))\n",
    "# l.index(max(l))\n",
    "# max(l)\n",
    "# #len(all_words_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "85Jk49Tfp7EC"
   },
   "source": [
    "# Visualize the cluster of books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sLZd4ZUqp7EC"
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# def avg_doc_embedding(all_docs):\n",
    "#     doc_vec = []\n",
    "#     for doc in all_docs:\n",
    "#         if len(doc) != 0:\n",
    "#             vector =  words_embedding(doc)\n",
    "#             doc_vec.append(np.mean(np.array(vector), axis=0))\n",
    "    \n",
    "#     summary_vec = np.array(doc_vec)\n",
    "#     return summary_vec\n",
    "\n",
    "# summary_vec = avg_doc_embedding(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s9f1TtHUp7EE"
   },
   "outputs": [],
   "source": [
    "# summary_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-8YShqDVtORv"
   },
   "outputs": [],
   "source": [
    "# pip install adjustText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6QoaEMczp7EF"
   },
   "outputs": [],
   "source": [
    "# from sklearn.manifold import TSNE\n",
    "# from adjustText import adjust_text\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# book_titles = new_file['book name']\n",
    "# titles_list = [title for title in book_titles]\n",
    "# titles_list = titles_list[:400]\n",
    "# def t_SNE_plot(df_data):\n",
    "#     # Initialize t-SNE\n",
    "#     tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 100)\n",
    "\n",
    "#     # Use only 400 rows to shorten processing time\n",
    "#     tsne_df = tsne.fit_transform(df_data)#tsne.fit_transform(df_data[:400])\n",
    "\n",
    "#     sns.set()\n",
    "#     # Initialize figure\n",
    "#     fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
    "#     sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
    "\n",
    "#     # Import adjustText, initialize list of texts\n",
    "\n",
    "#     texts = []\n",
    "#     data_to_plot = list(np.arange(0, 400, 40))#list(np.arange(0, 400, 40))\n",
    "\n",
    "#     # Append words to list\n",
    "#     for data in data_to_plot:\n",
    "#         texts.append(plt.text(tsne_df[data, 0], tsne_df[data, 1], titles_list[data], fontsize = 14))\n",
    "\n",
    "#     # Plot text using adjust_text (because overlapping text is hard to read)\n",
    "#     adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
    "#                 expand_points = (2,1), expand_text = (1,2),\n",
    "#                 arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "# t_SNE_plot(summary_vec[:400])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2joV1EE6p7EH"
   },
   "source": [
    "## Padding sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "enefEKmQp7EH",
    "outputId": "fd74cc10-a26b-460b-98ae-2d42e043528a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4250"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_Length = 0  \n",
    "docs_length = []\n",
    "for x in all_words_index:\n",
    "    docs_length.append(len(x))\n",
    "    if len(x) > max_Length:\n",
    "        max_Length = len(x)\n",
    "\n",
    "docs_length = np.array(docs_length)\n",
    "max_Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Sa5MKaO-cKk7",
    "outputId": "1f14f74c-af5e-4fab-baa2-8b3738117034"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64),)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(docs_length<1)\n",
    "#docs_length[3382]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zVhVxD4Yp7EJ"
   },
   "outputs": [],
   "source": [
    "def pad_sequences(all_words_index, seq_length):\n",
    "    \n",
    "    # getting the correct rows x cols shape\n",
    "    sequences = np.zeros((len(all_words_index), seq_length), dtype=int)\n",
    " \n",
    "    # for each review, I grab that review and \n",
    "    for i, row in enumerate(all_words_index):\n",
    "        if len(row)>0:\n",
    "            sequences[i, 0:len(row)] =  np.array(row)[:seq_length]\n",
    "    \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "y6dnQuwrp7EL",
    "outputId": "38bf8d19-51be-4315-83d5-cf6205dede21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[666 667 668 ...   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "sequences = pad_sequences(all_words_index, max_Length)\n",
    "\n",
    "assert len(sequences)==len(all_words_index), \"Sequences should have as many rows as reviews.\"\n",
    "assert len(sequences[0])==max_Length, \"Each sequence row should contain seq_length values.\"\n",
    "\n",
    "print(sequences[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tM4vXOwrp7EN"
   },
   "source": [
    "# Training, Validation, and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "g2q-yFvfp7EN",
    "outputId": "61bc1f35-7452-4afa-95ec-a128fa765e33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tSequences Shapes:\n",
      "Train set: \t\t(10188, 4250) \n",
      "Validation set: \t(1274, 4250) \n",
      "Test set: \t\t(1274, 4250)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(sequences)*split_frac)\n",
    "train_x, remaining_x = sequences[:split_idx], sequences[split_idx:]\n",
    "train_y, remaining_y = one_hot[:split_idx], one_hot[split_idx:]\n",
    "train_len, remaining_len = docs_length[:split_idx], docs_length[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "val_len, test_len = remaining_len[:test_idx], remaining_len[test_idx:]\n",
    "\n",
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tSequences Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UDvw1Oi6hBOv",
    "outputId": "1d132995-6ff3-43a6-f946-7fc7e788b5a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_len.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iwJXkJ8zzrvt"
   },
   "outputs": [],
   "source": [
    "# split_frac = 0.8\n",
    "\n",
    "# ## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "# split_idx = int(len(all_words_index)*split_frac)\n",
    "# train_x, remaining_x = all_words_index[:split_idx], all_words_index[split_idx:]\n",
    "# train_y, remaining_y = one_hot[:split_idx], one_hot[split_idx:]\n",
    "# #train_len, remaining_len = docs_length[:split_idx], docs_length[split_idx:]\n",
    "\n",
    "# test_idx = int(len(remaining_x)*0.5)\n",
    "# val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "# val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "# #val_len, test_len = remaining_len[:test_idx], remaining_len[test_idx:]\n",
    "\n",
    "# ## print out the shapes of your resultant feature data\n",
    "# print(\"\\t\\t\\tSequences Shapes:\")\n",
    "# print(\"Train set: \\t\\t{}\".format(len(train_x)), \n",
    "#       \"\\nValidation set: \\t{}\".format(len(val_x)),\n",
    "#       \"\\nTest set: \\t\\t{}\".format(len(test_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KyHBQ3l0p7ER"
   },
   "source": [
    "### Above only 2,0,1 because I only used  3 datasample to do all of these"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tbUPadNHp7ES"
   },
   "source": [
    "# DataLoaders and Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lcP-6vOJp7ES"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "# # create Tensor datasets\n",
    "\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(np.array(train_x)), torch.from_numpy(np.array(train_y)), torch.from_numpy(train_len)) \n",
    "valid_data = TensorDataset(torch.from_numpy(np.array(val_x)), torch.from_numpy(np.array(val_y)), torch.from_numpy(val_len))\n",
    "test_data = TensorDataset(torch.from_numpy(np.array(test_x)), torch.from_numpy(np.array(test_y)), torch.from_numpy(test_len)) \n",
    "\n",
    "# # train_data = Dataset(train_x, train_y) #, torch.from_numpy(train_len)\n",
    "# # valid_data = Dataset(val_x, val_y)\n",
    "# # test_data = Dataset(test_x, test_y) \n",
    "\n",
    "# # dataloaders\n",
    "batch_size = 128\n",
    "# #RANDOM_SEED = 1\n",
    "\n",
    "# def pad_collate(batch):\n",
    "#   (xx, yy) = zip(*batch)\n",
    "#   x_lens = [len(x) for x in xx]\n",
    "#   y_lens = [len(y) for y in yy]\n",
    "\n",
    "#   xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)\n",
    "#   yy_pad = pad_sequence(yy, batch_first=True, padding_value=0) , collate_fn=pad_collate\n",
    "\n",
    "#   return xx_pad, yy_pad, x_lens, y_lens\n",
    "\n",
    "# shuffling and batching data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, num_workers=4)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size, num_workers=4)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tz_3dnhcp7EU"
   },
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KExMFqzUp7EU"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# import numpy as np\n",
    "# from torch import nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# # def kmax_pooling(x, dim, k):\n",
    "# #     index = x.topk(k, dim = dim)[1].sort(dim = dim)[0]\n",
    "# #     return x.gather(dim, index)\n",
    "\n",
    "# class LSTMText(torch.nn.Module): \n",
    "#     def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, \n",
    "#                 linear_hidden_size, num_classes, freeze_embeddings=True):\n",
    "#         super(LSTMText, self).__init__()\n",
    "        \n",
    "#         self.num_classes = num_classes\n",
    "#         # 1. embedding layer\n",
    "#         self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "#         # set weights to pre-trained\n",
    "#         self.embedding.weight = nn.Parameter(torch.from_numpy(word_vec_array)) # all vectors\n",
    "#         # (optional) freeze embedding weights\n",
    "#         if freeze_embeddings:\n",
    "#             self.embedding.requires_grad = False\n",
    "\n",
    "#         self.lstm =nn.LSTM( input_size = embedding_dim,\n",
    "#                             hidden_size = hidden_size,\n",
    "#                             num_layers = num_layers,\n",
    "#                             bias = True,\n",
    "#                             batch_first = False,\n",
    "#                             # dropout = 0.5,\n",
    "#                             bidirectional = True\n",
    "#                             )\n",
    "\n",
    "#         # self.dropout = nn.Dropout()\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(hidden_size*2, linear_hidden_size),\n",
    "#             nn.BatchNorm1d(linear_hidden_size),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Linear(linear_hidden_size, num_classes)\n",
    "#         )\n",
    "        \n",
    "#          #activation function\n",
    "#         self.act = nn.Sigmoid()\n",
    " \n",
    "#     def forward(self, text):\n",
    "      \n",
    "#         #text = [batch size,sent_length]\n",
    "#         embedded = self.embedding(text)\n",
    "#         #embedded = [batch size, sent_len, emb dim]\n",
    "#         print(embedded.shape)\n",
    "#         #packed sequence\n",
    "#         #packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=True)\n",
    "        \n",
    "#         #packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "#         packed_output = self.lstm(embedded) #.view(len(text), 1, -1)\n",
    "#         #hidden = [batch size, num layers * num directions,hid dim]\n",
    "#         #cell = [batch size, num layers * num directions,hid dim]\n",
    "        \n",
    "#         #concat the final forward and backward hidden state\n",
    "#         #hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "                \n",
    "#         #hidden = [batch size, hid dim * num directions]\n",
    "#         #logits=self.fc(hidden)\n",
    "#         logits=self.fc(packed_output) #.view(len(text),-1)\n",
    "\n",
    "#         #Final activation function\n",
    "#         probas=self.act(logits, dim = 1)\n",
    "\n",
    "#         return logits, probas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kvsMqQ2lv1aO"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# def kmax_pooling(x, dim, k):\n",
    "#     index = x.topk(k, dim = dim)[1].sort(dim = dim)[0]\n",
    "#     return x.gather(dim, index)\n",
    "\n",
    "class LSTMText1(torch.nn.Module): \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, \n",
    "                linear_hidden_size, num_classes, freeze_embeddings=True):\n",
    "        super(LSTMText1, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        # 1. embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # set weights to pre-trained\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(word_vec_array)) # all vectors\n",
    "        # (optional) freeze embedding weights\n",
    "        if freeze_embeddings:\n",
    "            self.embedding.requires_grad = False\n",
    "\n",
    "        self.lstm =nn.LSTM( input_size = embedding_dim,\n",
    "                            hidden_size = hidden_size,\n",
    "                            num_layers = num_layers,\n",
    "                            bias = True,\n",
    "                            batch_first = False,\n",
    "                            # dropout = 0.5,\n",
    "                            bidirectional = True\n",
    "                            )\n",
    "\n",
    "        # self.dropout = nn.Dropout()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size*2, linear_hidden_size),\n",
    "            nn.BatchNorm1d(linear_hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(linear_hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "         #activation function\n",
    "        self.act = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, text, text_lengths):\n",
    "      \n",
    "        #text = [batch size,sent_length]\n",
    "        embedded = self.embedding(text)\n",
    "        #embedded = [batch size, sent_len, emb dim]\n",
    "        print(embedded.shape)\n",
    "        #packed sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        #packed_output = self.lstm(embedded) #.view(len(text), 1, -1)\n",
    "        #hidden = [batch size, num layers * num directions,hid dim]\n",
    "        #cell = [batch size, num layers * num directions,hid dim]\n",
    "        \n",
    "        #concat the final forward and backward hidden state\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "                \n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "        logits=self.fc(hidden)\n",
    "        #logits=self.fc(packed_output) #.view(len(text),-1)\n",
    "\n",
    "        #Final activation function\n",
    "        probas=self.act(logits)\n",
    "\n",
    "        return logits, probas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A7k-ELD2p7EW"
   },
   "source": [
    "# Instantiate the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "JNLvp2Zyp7EW",
    "outputId": "46fc666d-2f4b-4b0a-c476-19da24334910"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMText1(\n",
      "  (embedding): Embedding(51247, 300)\n",
      "  (lstm): LSTM(300, 128, num_layers=2, bidirectional=True)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=20, bias=True)\n",
      "    (1): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=20, out_features=227, bias=True)\n",
      "  )\n",
      "  (act): Sigmoid()\n",
      ")\n",
      "The model has 16,219,631 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# vocab_size, embedding_dim, hidden_size, num_layers, \n",
    "#                  bidirectional, dropout, linear_hidden_size, num_classes\n",
    "    \n",
    "# Instantiate the model with hyperparameters\n",
    "import random\n",
    "vocab_size = word_vec.shape[0]#len(google_vecs.vocab)\n",
    "#num_classes = 1 # binary class (1 or 0)\n",
    "embedding_dim = google_vecs.vector_size # 300-dim vectors\n",
    "hidden_size = 128 #256 #LSTM hidden size\n",
    "num_layers=2 #LSTM layers\n",
    "linear_hidden_size = 20 # units number of full connected \n",
    "num_classes = len(label_list)\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)  ###revise\n",
    "\n",
    "model = LSTMText1(vocab_size, embedding_dim, hidden_size, num_layers, \n",
    "                 linear_hidden_size, num_classes)\n",
    "\n",
    "model = model.float()\n",
    "\n",
    "print(model)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "#No. of trianable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(m.numel() for m in model.parameters() if m.requires_grad)\n",
    "    \n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DDXXgXiNp7EY"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1\n",
    "GRAYSCALE = False\n",
    "intial_lr = 0.1\n",
    "\n",
    "criterion = nn.BCELoss()       \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=intial_lr)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=0.01,\n",
    "                                                   last_epoch=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h-rrGm1Sp7Ea"
   },
   "source": [
    "# Define loss function and evaluation parameters and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "kAqtQ44mp7Ea",
    "outputId": "c4258e05-3c83-4ce3-a9fa-78ade8b3aec5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 140,   56,  189,  266,   49,  480,  386,  108,  775,   29,  156,  137,\n",
      "         102,  433,   98,  318,  143,  209,  428,   73,  111,  220,   27,   72,\n",
      "         229,   27,   20,  364,  678,   95,  281,  127,   56,  205,   18,  189,\n",
      "         101,   96,   51,   69,   30,   95, 1404,   94,  406,   73,  245,   92,\n",
      "         184,   65,  161,   31,  263,  187,  258,  163,   42,  354,  195,   41,\n",
      "         244,   42,  462,  114,   46,   55,  138,  194,   59,  124,  740,  138,\n",
      "          32,   97,  521,  369,   34,  448,  249,  443,  164,  232,  127,  295,\n",
      "         137,   70,  238,  405,   40,  119,   58,   93,  311,  189,   83,  228,\n",
      "          16,  330,  102,  538,   23,  172,  126,   57,  169,   80,   73,  299,\n",
      "         240,   34,  674,  553,   26,    8,  579,  312,   65,  200,   56,   63,\n",
      "          67,  491,  229,   45,   77,  184,  503,  303])\n",
      "torch.Size([128, 4250, 300])\n",
      "Epoch: 001/001 | Batch 000/080 | Cost: 0.7059\n",
      "tensor([ 105,  462,   42,  420,  284,   20,  459,   21,  326,  295,  256,   31,\n",
      "         159,  109,  863,  133,  579,  157,  500,  122,  283,  144,   79,  352,\n",
      "         525,  392,  282,   43,  138,  655,   36,  100,   70,   30,  156,  441,\n",
      "          49,  311,   50,   36,  121,  562,  569,  199,   98,  137,  254,   74,\n",
      "          66,  168,   72,   75,   94,  194,  157,   29,   62,  613,   46,  197,\n",
      "         447,  248, 1175,   32,   56,  642,   83,  257,   46,   48,  350,   94,\n",
      "          45,  287,   95,   23,   68,  134,  182,  350,   38,  340,  309,  225,\n",
      "         117,  138,   30,  296,   74,   48,  160,  123,   64,  270,  257,  243,\n",
      "         125,   54,   62,   92,  271,   31,   40,  588,  179,   46,  138,  127,\n",
      "         366,  210,  106,  236,   55,  639,  234,  207,  629,   59,  138,  173,\n",
      "         140,  276, 1001,  313, 1036,  531,   69,  453])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([  19,   86,  109,   95,  247,   19,  417,  373,  512,  364,  739,   82,\n",
      "         295,  415,  162,   49,  512,   12,  219,  182,  386,  539,  110,   71,\n",
      "          53,  370,  766,    9,  469,  106,   53,   59,  138,  199,  113,    9,\n",
      "         671,  107,  819,  254,   82,   44,  824,  292,  110,  106, 2018,  194,\n",
      "         297,  511,  369,   96,  202,  119,  243,  434,  131,  196,  168,  607,\n",
      "         544,  245,   82,  163,  253,  354,   74,   53,  113,  489,  113,  364,\n",
      "         327,   85,  129,  431,   62,  541,  968,   19,  358,  209,   54,   83,\n",
      "          88,   41,  148,   21,  235, 1206,  111,  150,  196,   26,  199,   80,\n",
      "          32,   39,  261,   93,   52,   33,  279,   20,    8,  297,   27,  130,\n",
      "         332,  116,   62,  139,  504,  150,  439,   47,  114,  492,   20,   59,\n",
      "          23,  277,  127,  323,   69,  473,   36,   50])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([  37,   48,  401,  336,   60,   83,  245,  215,   76,  283,   49,  127,\n",
      "        1337,  256,  397,   21,   75,  316,  488,  222,  338,   64,  140,  219,\n",
      "         123,   74,   42,  103,  118,  160,  745,  151,   69,  182,  175,  850,\n",
      "         190,  140,  290,  177,   25,  344,  346,  235, 1299,  409,   85,  273,\n",
      "          50,   93,  320,   41,  126,  199,  110,   52,  391,  194,   41,  258,\n",
      "         227,   25,  164,  141,   39,  272,  179,  463,  281,  779,  285,   87,\n",
      "         134,  260,  298,  196,  182,  330,  259,   89,  122,  152,   13,   43,\n",
      "          91,  117,   75,  137,  202,   98,  387,  254,  130,  500,  111,    9,\n",
      "         494,  303,   57,  119,  370,   46, 1243,   91,  249,   58,  135,   62,\n",
      "          93,  103,   59,  109,  235,   13,  137,   85,  223,  220,  204,  293,\n",
      "         353,  367,  284,  376,  173,  313,   44,   66])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([ 86, 133, 248, 422,  37, 609, 141, 311,  59,  94,  12, 205, 404, 302,\n",
      "        335, 647, 492, 335,  83, 131,  41,  53, 518,  15,  73,  20,  34,  39,\n",
      "        337,  17, 541, 171, 178,  78, 114, 200, 187, 237, 189, 143, 367, 468,\n",
      "        101,  55, 996, 210, 384,  49,  52,  15,  62, 198, 116,  72,  74,  29,\n",
      "        125, 175,  15, 308,  42, 413,  63,  40,  21, 113, 366, 370,  98,  58,\n",
      "        793,  18,  50,  66,  68, 106, 298, 267,  26,  53, 218, 447,  79, 460,\n",
      "        273,  90,  53,  63, 187, 589,  81, 212, 397, 192,  42, 170, 129, 169,\n",
      "        740,  83, 184, 115, 196,  20, 519, 575,  76,  53, 375, 180,  64,  50,\n",
      "        167, 213,  50, 433, 100, 188,  29, 114, 241, 267,  87,  38, 207, 310,\n",
      "        277, 120])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([ 667,   97,   59,   72,   45,   61,  453,   97,  169,   70,  296,  552,\n",
      "          30,  353,   60,   22,  510,  390, 1072,  380,   59,  152,   92,   95,\n",
      "          52,  551,  527,  670,  275,   30,  424,   74,   25,  164,   34,  248,\n",
      "         225,  585,  324,  187,  111,  617,   66,  396,  102,  340,  422,  509,\n",
      "         295,  259,  686,   94,  134,   39,   94,  383,  491,  204,   48,  293,\n",
      "         332,  367,   67,  590,  358,  175,  463,  214,   20,  149,  253,   23,\n",
      "          69,  545,  427,   42,  261,   65,   58,  222,   36,  213,  146,   38,\n",
      "         107,  333,  597,  313,  104,  235,   21,  150,   28,   44,   51,  105,\n",
      "         603,   48,   87,   98,   67,   34,  303,   55,  518,  345,  227,   85,\n",
      "          90,  592,  365,  149,  385,  596,  344,  800,  101,   92,  410,  467,\n",
      "         428, 1027,  271,  282,   93,  248,  394,  144])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([ 353,  157,  304,  418,  159,  172,  148,  981,  303,  259,  162,   96,\n",
      "         249,  249,   71,   19,  311,  464,  156,  182,   36,   39,  584,  119,\n",
      "         265,  581,  168,  127,  215,  105,   12,  280,   35,  426,  360,   50,\n",
      "         195,  279,   32,  446,  124,  524,   68,  299, 1344,  547,  321,  260,\n",
      "         230,  141,   45,   83,   70, 2856,  478,   26,   50, 1293,   37,   48,\n",
      "         178,  347,  118,   67,   65,  109,   31,  466,  193,  171,  434,  521,\n",
      "         192,  101,  119,   58,  942,   57,  175,   45,  205,  386,  258,  165,\n",
      "         103,   74,  115,   71,   66,  149,   74,  171,   54,   53,  129,  327,\n",
      "         627, 1496,  158,  186,   46,  285,  312,   41,  301,   74,  129,  394,\n",
      "          83,  160,  810, 1048,   56,  318,  196,  120,  530,   24,   54,  622,\n",
      "          80,  260,  218,  257,   85,   48,  202,  101])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([ 117,  219,   68,   28,   38,  221,   20,   35,  122,  388,   54,   56,\n",
      "         107,  262,   81,   73,   52,  384,  263,  145,   62,  336,  102,   58,\n",
      "          41,  459,   80,   43,   83,  179,   35,  107,   67,  125,   41,  170,\n",
      "         876,   28,  355,  109,  175,   41,  141,   50,  123,  214,   55,   35,\n",
      "         525,  348,  206,  904,  219,   28,   39,   12,  215,  134,   56,  264,\n",
      "         184,  228,  298,  160,   32, 1015,  180,   97,  501,   32,   83,   91,\n",
      "         591,   75,   95,  178,  569,   26,   46,  136,  150,   48,  434,  756,\n",
      "          75,  335,   85,  832,  374,  250,  113,  910,  151,  136,   49,  157,\n",
      "          93,   32,  147,  132,   94,  306,   71,  340,  291,  202,  200,  115,\n",
      "         110,  338,  124,  334,   75,  100,  438,   10,   12,  102,  263,  208,\n",
      "          50,  195,  502, 1054,  151,   43,  122,  108])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([  31,  702,  151,   49,  433,  574,   76,  442,  193,   17,  133,   17,\n",
      "         297,  213,   65,  149,   32,   98,   51,  306,  127,  305,  216,  351,\n",
      "         420,   80,  104,  166,  102,  117,  361,  205,   52,  586,  678,   73,\n",
      "          88,   94,  354,  508,  120,  269,   68,   86, 1008,   51,   12,   82,\n",
      "         370,   49,   37,  311,  167,  240,  545,  252,   76,   48,  138,  393,\n",
      "          94,   19,   94,   83,  114,  397,  225,  299,   32,  107,  100,  197,\n",
      "         401,   95,  139,  445,  283,  358,  516,  284,  164,  679,   11,  701,\n",
      "         319,  264,  338,   62, 4250,  211,   27,   63,  412,   65,  758,   81,\n",
      "         439,   70,  300,  347,  156,   47,  133,   53,   43,  359,   52,  106,\n",
      "         552,  120,  202,  872,  197,   31,  112,  119,   90,  267,  377,  660,\n",
      "         106,  204,   25,  129,  295,   69,  271,  520])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([152, 185, 237, 333,  52, 219,  65, 329, 209, 451,  41,  20, 101,  90,\n",
      "         58, 115, 168, 532, 208, 194, 201, 211, 607,  43, 150, 246,  76,  81,\n",
      "         69, 370, 107, 238,  11,  58, 528, 295,  56, 493,  39, 214, 140, 219,\n",
      "         31,  72,  21, 567, 324, 231, 166, 178, 134,  39, 132,  61, 164,  74,\n",
      "        103, 578,  43, 490, 476, 121,  63, 237,  68,  29, 175, 183,  67,  90,\n",
      "         27,  68, 582,  79, 597, 101, 142,  43,  44, 944, 138, 603,  20, 997,\n",
      "         84, 365,  54, 179, 306, 143, 254,  97, 238,  67, 153, 447, 115, 254,\n",
      "        117, 474,  62, 400, 186, 239, 185,  85,  32, 199,  91, 118, 139,  93,\n",
      "         37, 210, 210, 144, 307,  91, 172, 183, 581,  36,  52,  54, 348,  25,\n",
      "        161, 290])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 4250, 300])\n",
      "tensor([ 446,  266,  220,  277,   55,  128,  239,   78,  245,  184,  252,  286,\n",
      "         108,  491,  737,  520,  268,  205,  389,  314,  261,  607,  223,   63,\n",
      "          32,   40,  351,   30,  487,  148,   63,  236,  232,   44,  104,   56,\n",
      "          19,   34,  177,  381,  112,   54,  275,  309,   99,  476,   40,  104,\n",
      "          55,  197,   85,  129,   40,  279,  101,  132,  253,   48,  227,  308,\n",
      "         720,   97,   23,   39,   33,   10,  237,  197,   47,   46,  607,  936,\n",
      "         529,   63,   13,   89,  358,   37,  159,   99,  586,  141,   52,  234,\n",
      "         109,  410,   37,   74,  354,   21,  280,   83,   26,   99,   66,  126,\n",
      "          63,  137,   42,  203,  441,   91,   89,   31,   63,   23,  185,  221,\n",
      "          73,   31,   32,   11,   75,  184,   15,  246,   85,   80,  123,  102,\n",
      "          33, 1083,  166,   83,  121,  245, 1240,  161])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([  39,  645,  185,  142,   57,   61,   86,  573,  162,   51,   83,  177,\n",
      "         500,  352,  475,  212,   51,  387,   86,  242,   85,  246,  320,   42,\n",
      "          73,   38,   34,  180,  236,  133,   77,   67,  596,  317,   53,  172,\n",
      "          25,   39,   51,  167,  275,  122,  204,  667,  669,   64,   78,   57,\n",
      "         342,  203,   21,  148,   48,   77,  453,  183,  370,  119,  119,  559,\n",
      "         126,  359,   32,  102,   86,   61,   60,  336,  727,  496,   91,  194,\n",
      "         162,  164,  689,  371, 1854,   51,  210,  200,   76,  191,  475,  551,\n",
      "          26,  112,   93,  185,   27,  408,   23,  220,  309,   61,  212,   79,\n",
      "         427,   36,  257,   61,   70,   14,  536,   43,   61,  188,   44,   46,\n",
      "          45,   28,  194,  218,  153,  280,   52,  225,    9,  118,  165,  548,\n",
      "         147,  123,  303,  177,   93,  110,   44,  177])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([ 346,   75,  127,  141,   48,  202,  975,  516,  649,  133,  633,  152,\n",
      "         173,  424,  222,   76,   87,  160,  127,  116,  321,   95,   45,  273,\n",
      "         303,  396,  207,   55,   76,   42,   98,  617,  281,   27,   44,  392,\n",
      "          49,  564,  137,   13,  763,  249,   23,  171,   46,  159,   66,  576,\n",
      "         215,  167,    9,  202,  169,   44,  101, 1007,  154,  332,  926,  825,\n",
      "          36,  344,  157,  613,  194,  391, 1115,  160,  123,   35,   63,  215,\n",
      "         364,  632,  432,  425,   30,   37,   30,   18,  494,   46,  233,  134,\n",
      "          88,  245,  187,  317,   34,  213,  399,   23,   42,  161,  669,   91,\n",
      "         286,  278,  236,  121,   42,   84,  261,  193,  478,   41,  840,   91,\n",
      "         344,  150,  282,  255,   56,   53,  526,  227,  351, 1200,  131,  246,\n",
      "         422,  333,   63,  144,   22,  387,  816,  130])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([ 171,   63,  340,  547,   63,  143,  171,  165,  145,   42,   78,  231,\n",
      "          29, 1208, 2307,  205,  569,  159,   29,   50,  159,  429,  143,  242,\n",
      "          47,  240,  280,  164,  247,  318,  174,   58,   39,  108,  299,  371,\n",
      "        1035,  334,  106,  718,  242,   79,  112,   21,  396,  295,   70,  195,\n",
      "          80,  111,  559,  347,   58,   28,  284,  158,   65,   76,  201,  218,\n",
      "         674,   88,  231,  232,  247,   37,  376,  499,   50,   61,  558,   70,\n",
      "          26,  242,  282,  312,   42,   18,   29,  411,   71,   81,   25,   91,\n",
      "          21,  192,  312,  867,  660,  280,  187,  137,   98,  362,   73,  379,\n",
      "         117,  692,  106,   68,   91,  167,   76,  401,  275,   99,  158,  117,\n",
      "         125,  394,   98,   44,   66,  117,   19,  145,  594,  186,  593,  336,\n",
      "          68,   51,  207,  172,  434,   80,   44,  503])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([ 501,   62,  357,  303,  445,  170,   80,  227,   50,  231,  768,   80,\n",
      "          21,    8,  600,   39,  732,   97,   66,  262,  198,   41,  218,  180,\n",
      "          59,  155,   91,  127,  167,  116,  144,  273,  204,  724,  182,  604,\n",
      "         576,  516,   74,   15,   52,   24,  295,  742,  303,   69,  178,   70,\n",
      "         325,  152,   78,  401,  181,   45,  151,  162,   37,  537,  127,  253,\n",
      "         402,  292,   48,   33,  465,  107,   24,  155,   10,   37,   67,  228,\n",
      "         269,  231,   57,  219,  231,   20,  112,  116, 1466,   47,   55,  263,\n",
      "         554,  411,  377,  159,  112,  288,   79,  257,  546,  354, 1493,  167,\n",
      "         114,  276,  197,  103,  238,  263,   62,   12,  787,  161,  129,  224,\n",
      "          43,  435,   94,   99,  365,   30,  918,  203,  348,   38,  138,  211,\n",
      "          25,  646,   87,   38,   58,  106,  265,   15])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([ 273,  229,   58,  228,   44,   32,   15,  515,   36,   75,  248,  149,\n",
      "         174,  802,  222,  279,  285,   41,  133,  122,   19, 1303,  336,  124,\n",
      "         149,  318,  116,   84,  118,   21,  333,  224,   87,   80,   66,  102,\n",
      "         298,   21,  382,  304,  112,   40,  149,  285,  224,   56,  220,  274,\n",
      "         266,   20,  174,  175,  107,  148,   21,  270,  130,   55,  227,  420,\n",
      "         255,   18,  238,  315,  115,  108,   63,   95,  671,  248,   44,   94,\n",
      "          77,  153,  632,   75,   54,   99,  287,  444,   32,  106,  419,  197,\n",
      "          35,  325,  126,  264,  192,  306,  376,  123,   20,  377,  957,  112,\n",
      "         336,   54,   29,  150,  226,  110,  315,  162,   45,  262,   19,   12,\n",
      "         507,  247,   86,  549,  458,  191,  189, 1111,  165,  436,   54,   47,\n",
      "          40,  154,  118,   92,   52,  107,  216,  174])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([ 181,   62,  311,  253,  179,   63,  757,  543,  115,  317,   38,   46,\n",
      "         292,  211,  128,   67,   62,  287,  343,  286,  258,  236,  310,   69,\n",
      "          62,  162,   60,   79,  257,   38,  137,  198,  360,  714,   86,  146,\n",
      "         130,  225,   93,  670,  218,   18,   87,  342,   28,   38,   62,  261,\n",
      "         368,  444,   67,  103,  249,  142,  138,   61,  308,   62,  150,   90,\n",
      "          75,  448,   26,  387,   97,  574,  215,  104,   95,  107,  460,   96,\n",
      "          72,   74,   60,  129,  262,  132,  302,  649, 1353,   42,   23,  198,\n",
      "         170,  196,   26,  150, 1421,   89,  146,   69,   33,   70,  348,   72,\n",
      "          80,  110,  177,   29,   11,  429,  331,   27,   99,  374,  106,  164,\n",
      "          33,   48,   60,   87,  286,  385,  167,   47,  574,  257,  280,  401,\n",
      "          95,  142,  137,   93,   65,   94,  208,  355])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([ 433,  367, 1002,  158,  221,   33, 1714,  361,   56,  227,   85,  615,\n",
      "         297,  146,   55,  254,   58,  677,  186,   66,   25,   42,  345,  248,\n",
      "         208,   40,  712,  138,   70,   93,  294,  460,  711,  513,  437,  425,\n",
      "         129,   98,   15,  504,   21,  750,  431,   43,  330,   48,   11,  248,\n",
      "         116,  356,   61,   43,  197,  146,  422,  130,   62,  169,  430,   73,\n",
      "          67,   42,  266,   10,   71,  292,  325,   69,   60,  212,   64,  109,\n",
      "          83,   24,  105,   37,   69,  177,   99,  148,  131,   36,  655,   54,\n",
      "         221,  136,  312,   36,  159,  422,  288,  538,  206,  116,   28,  370,\n",
      "         283,  266,   40,   44,  104,   33,   53,  487,  742,   69,  696,  244,\n",
      "         579,  201,   56,  321,  124,  551,  119,   43,  495,  595,   83,  190,\n",
      "         335,   78,  470,   54,  353,  181,  301,   47])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([  28,  669,  221,  407,   69,   64,   75,   34,  410,  418,  407,  174,\n",
      "         646,  322,  254,  245,  312,  202,  112,   86,  109,   38,  637,   38,\n",
      "          48,   77,  163,  447,  295,  142,  127,  407,  334,   25,   63,   57,\n",
      "          23,   25,  419,  142,   21,   71,  145,  724,  222,   17,   54,  366,\n",
      "         329,  136,  207,   38,  314,   63,  190,   26,  452,  145,  346,  659,\n",
      "         200,  139,  187,  108,   47,  206,  492,   80,  319,   54,   30,  194,\n",
      "          31,   62,  124,  132,   72,   59,  164,  456,  203,   35,   26,  233,\n",
      "         540,   90,  123,  132,  347,   52,   54,  151,   68,   87,   93,  757,\n",
      "         161,  430,  159,  178,  190,   85,  150,  139,  509, 1059,   66,   47,\n",
      "         552,   86,   58,  758,   71,  467,   53,  830,  161,  406,   36,  148,\n",
      "          48,   58,   56,  193,   70,   40,  147,  309])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([  57,   38,  131,  132,  656,  265,  181,  359,  147,  152,  252,  140,\n",
      "         238,  252,   47,   41,   61,  248,  299,   85,   26,  229,  118,  416,\n",
      "          46,   38,  403,  242,  128,   33, 1300,   76,  201,  202,   82,  158,\n",
      "         213,  107,  225,  135,   65,  120,   88,  360,  451,   82,  218,  266,\n",
      "          24,   40,   52,  313,  465,  179,   98,  187,  392,  397,  123,  667,\n",
      "         220,  507,   34,  156,  536,   80,  316, 2453,  349,  580,  208,  171,\n",
      "          77,   50,  102,  322,  198,  167,  163,  291,   78,  163,  151,  162,\n",
      "         233,  306,  264,  332,  145,  533,  119,   52,   53,   72,  132,  111,\n",
      "         285,  141,  297,  786,  156,   85,  303,  230,   83,  395,  219,  272,\n",
      "          94,  499,  101,  451,   91,  141,   87,   90,  149,   28,   88,   52,\n",
      "          69,  961,  748,  159,   55,   31,   65,  103])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 4250, 300])\n",
      "tensor([  29,  300,  148,   28,   12,  163,   49,  368,   40,  432,   34,   27,\n",
      "         331,   72,  196,  321,  223,  786,  654,   75, 2141,   10,  117,  223,\n",
      "         139,  199,   83,   86,  184,   96,  860,   34,   74,  260,   70,  157,\n",
      "         633,  120,   23,  193,   81,   53,  328,  199,   72,   90,  150,  145,\n",
      "         459,   62,  222,  296,   38,   28,   63,   46,   63,  469,   40,  119,\n",
      "         238,  497,  135,  112,  130,  371,  108,  437,  197,   98,   97,  202,\n",
      "         236,  376,   15,  197,  332,  112,  223,  535,  362,  468,  245,   55,\n",
      "         285,   33,  280,   99,  933,  445, 1318,   36,  844,  162,  408,  217,\n",
      "          22,   47,  368,  506,   53,  109,   77,   98,  399,  384,  270,  153,\n",
      "          62,   40,  273,   57,   54,   22,  263, 1002,   83,   66,  123,   64,\n",
      "          64,   85,  659,  422,  195,   28,  205,   85])\n",
      "torch.Size([128, 4250, 300])\n",
      "Epoch: 001/001 | Batch 020/080 | Cost: 0.3998\n",
      "tensor([ 37,  85, 193, 225, 668,  79, 123, 339,  33, 200, 188,  89, 441, 129,\n",
      "        309, 130,  79,  85, 289, 178, 519, 231, 486,  48, 437, 352,  99, 831,\n",
      "        286, 295, 444,  55, 238, 138, 245, 247,  85,  23, 137,  52, 148, 706,\n",
      "         28, 376,  64, 245, 115,  49,  68, 138, 247, 747,  62, 391,  42, 115,\n",
      "        494,  44, 590,  50,  88, 163, 228,  42, 241, 377,  80,  32,  67,  98,\n",
      "        389, 369, 219, 171, 538,  28,   8, 111, 311, 496,  42,  84, 619,  68,\n",
      "        152, 100, 105, 287,  81,  35,  61, 245, 245, 494, 383, 152, 217,  20,\n",
      "        112, 264, 128, 193, 293,  20, 191, 243, 195, 470, 586,  66,  60, 129,\n",
      "         28, 145,  21, 284,  40,  15, 273, 182, 718,  87,  48,  17, 120, 121,\n",
      "         78,  52])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([ 192,   79,  590,   46,  318,  289, 1517,  274,  419,  192,   40,   45,\n",
      "          43,  445,   50,   25,  469,  422,   92,  118,   41,  182,  185,   71,\n",
      "          74,  182,  479,  240,  178,   34,   65,  374,   59,   38,  182,  116,\n",
      "         506,   46,  459,  269,   66,  508,  614,  308,  286,   79,   42,  165,\n",
      "          41,  106,  412,  204,  301,  407,  532,  123,  588,  254,  112,  155,\n",
      "          26,  359,   92,  121,  184,  361,   71,  162,   26,  510,   74,  394,\n",
      "          83,  342,   77,   18,  180,  267,  187,   71,   19,   18,  204,  297,\n",
      "          84,   41,  137,  665,   75,  339,   53,   85,  278,   37,   64,  121,\n",
      "          29,  370,  269,   16,   79,  298,  404,  257,  330,  311,   47,  117,\n",
      "         788,  540,   84,  352,   14,   70,  293,  286,  104,  229,  978,   13,\n",
      "         238,  148,   73,  295,   29,  167,  123,  116])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([ 190,  535,  539,  401,   99,  114,  117,   64,  131,  213,  231,  670,\n",
      "         354,  111, 1162,  710,   80,  432,  306,  670,   14,   21,   66,  409,\n",
      "         452,  410,   81,   14,  149,   87,   46,  280,   70,  102,  207,  134,\n",
      "          24,  151,   35,  390,   97,  108,  621,  318,   74,   32,   57,   69,\n",
      "          90,  254,  105,  112,   68,  262,   94,  294,   59,   40,  346,  238,\n",
      "          48,   71,   72,  387,   30,  316,  214,   91, 1129,  111,  154,  343,\n",
      "         192,   33,  291,  143,  167,   82,  353,  151,   64,  407,  182,  186,\n",
      "         171,   31,  363,  103,  145,   63,  352,  210,   47,  662,   12,  380,\n",
      "         161,  164,   78,  121,  120,  106,  128,  118,  312,  268,  231,  227,\n",
      "         576,  191,  135,  147,  706,   29,  225,  535,  266,   38,  245,  429,\n",
      "         234,   83,  110,  303,  287,   45,   85,   44])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([  89,   53,  127,   10,  320,  419,   64,   57,   94,   52,   45,  527,\n",
      "          19,  143,  160,   76,  273,  707,   71,   87,   24,  230,  311,  253,\n",
      "         236,  176,  110,   45,   49,  223,  145,  274,  153,  240,  448,   47,\n",
      "         130,  111,  250,  402,  350, 1253,  163,  109,  152,  369,   11,   44,\n",
      "         173,  151,  241,    9,  154, 1741,  308,  146,  100,   84,   77,  125,\n",
      "         646,   59,  108,   50,  144,  182,  391,  365,  535,   98,  684,  441,\n",
      "         100,   80,  463,  320,   61,  135,  532,  361,  219,  146,   71,   18,\n",
      "         676,  554,   89,  254,  227,  435,  104,  210,  100,   49,  696,  197,\n",
      "          70,  684,   30,   36,  264,  109,   82,   76,  147,   32,   31,  508,\n",
      "         218,   80,  251,   46,   51,   10,  212,   32,  101,  183,   22,  179,\n",
      "         118, 1444,  726,  168,  422,  286,  219,  488])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([262, 157, 434, 204,  47,  74,  28, 308, 349,  27, 182,  73,  68, 148,\n",
      "        420, 389,  15, 154, 253,  65, 357, 251, 156, 143, 163, 375, 437, 488,\n",
      "        442, 164, 425,  49, 133,  56,  48, 440, 104,  15, 418, 140, 274,  49,\n",
      "        129, 306,  62, 179, 386,  18, 357, 241,  53,  56, 468, 293, 203,  35,\n",
      "        236, 161, 167, 196,  50, 179,  39, 258,  19,  48, 103, 134, 311, 192,\n",
      "        129, 589, 135,  43, 402,  99, 170, 179, 964, 135, 159,  94, 116,  61,\n",
      "        418, 242, 126,  29, 446, 176,  82, 106, 327,  36, 161, 161, 194, 160,\n",
      "        316,  96, 144, 206, 212,  32, 611, 323,  73, 412, 232,  57, 484, 141,\n",
      "         89, 127,  33, 177, 101, 394,  54, 182, 226, 232,  62, 146, 119,  96,\n",
      "        224,  31])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([250, 498, 402, 133, 166, 155,  83, 564,  86, 216, 530,  76, 174,  10,\n",
      "        456, 350, 681,  59,  56, 120, 546,  88, 229,  29, 217, 110,  41, 130,\n",
      "        161, 407, 515, 500, 116, 462, 487, 234, 250,  95, 427, 184,  33,  75,\n",
      "        166, 249,  77,  98,  83, 107, 458,  65, 125,  44,  68, 573,  64, 702,\n",
      "         71,  50,  22, 319, 158,  74,  92,  56,  32, 246, 131,  44, 365, 135,\n",
      "         39, 367, 499, 112,  92,  69, 719, 214, 106, 107,  99, 484,  48, 467,\n",
      "         43,  77,  49, 207,  76, 201,  43, 878, 106, 239,  31,  31, 123, 111,\n",
      "        474, 178,  63, 344, 107, 128, 270,  86, 144, 237,  85, 129,  56, 160,\n",
      "         68, 204, 486, 120, 387, 370, 173, 154,  34, 917, 335,  70, 138, 354,\n",
      "         26, 102])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([  77,  250,   72,   70,  209, 1697,  206,  258,  594,  389,  108,  197,\n",
      "          44,  411,   82,   43,  486,   93,   85,   80,   72,  168,   83,   96,\n",
      "         185,    9,   33,   91,  201,  326,  199,  137,   40,  510,   57,  832,\n",
      "         102,  152,  591,   41,  192,  188,   31,   15,  250,   44,  187,  164,\n",
      "         286,   29,  348,  127,  172,  183,   65,  198,   74,  234,  245,   70,\n",
      "         403,  427,   74,  325,  116,   62,   36,  333,  362,  472,  539,  369,\n",
      "         197,  458,   93,   98,  565,  229,  486,   67,  172,  559,  193,  314,\n",
      "          44,  196,  125,   78,  438, 1271,  234,  907,   19, 1029,  130,  522,\n",
      "         225,  262,   84,   81,  642,  353,  829,  212,  112,  132,   89,   48,\n",
      "         189,   80,  213,   56,  145,  459,  227,  499,   77,  475,  394,  285,\n",
      "          54,  111,  329,   80,  345,  202,  506,  215])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([  86,  152,  105,   91,  111,  620,  129,  233,  222,   78,   32,  139,\n",
      "         249,  121,   37,  102,   68,  219,   24,  400,   31,   43,   67,  381,\n",
      "        1241,  169,   50,   33,  107,  190,   46,  322,   55,  314,  106,  127,\n",
      "         132,  441,  310,  453,   57,   86,   16,  234,  799,  325,  734,  115,\n",
      "         185,  230,  110,  127,   25,   62,  180,  186,  411,   72,  187,  403,\n",
      "         168,   68,  217,  335,   61,   89,  540,   70,  358,  200,  147,   35,\n",
      "         158,  194,   39,  147,  425,   27,   56,   10,   97,  175,  153,  255,\n",
      "         587,  194,  131,  323,   22,   92,   63,  133,  156,  218,  453,   62,\n",
      "         102,  193,   50,  754,  312,   21,  152,  219,   63,   36,   81,  781,\n",
      "         147,  477,  145,   52,  155,  174,   71,  525,   89,   26,  513,  350,\n",
      "          80,  140,   12,  234,  490,  402,   66,  240])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([276, 458, 142,  40, 304, 182, 226,  80,  30, 535,  83, 349, 923,  46,\n",
      "         62, 368, 525, 791, 369, 165,  79,  37,  58, 324, 258,  51, 359, 276,\n",
      "         53,  54, 126,  42, 325, 225,  40,  47, 219, 121, 112,  77, 103, 108,\n",
      "         83,  70, 269, 450, 272,  39,  47,  95, 166,  68,  58, 175, 151, 127,\n",
      "         93, 133, 217, 156, 245, 100, 159, 143, 306, 131, 229, 188,  94,   9,\n",
      "        571, 853,  31,  83, 233, 216,  38, 129, 217, 360,  22, 143, 287, 160,\n",
      "         36, 159,  59, 399,  84,  59, 117,  52,  62, 419, 127,  14,  79,  47,\n",
      "        297,  66, 169, 158, 472, 101, 121,  83,  57, 252, 147, 128, 124, 467,\n",
      "         16, 397, 133, 630, 115, 182,  49, 181,  16, 339, 144, 520, 348, 158,\n",
      "         58, 330])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 4250, 300])\n",
      "tensor([ 365,  487,  297,   66,   59,  126,   64,   47,  501,  375,   89,  196,\n",
      "         501,  136,  385,   12,  101,  152,  104,  328,   62,   71,  405,   19,\n",
      "         434,   92,   40,  162,   59,  397,   63,   36,   32,  183,  102,  361,\n",
      "          77,   51,  801,   65,  122,   99,  288,   28,  132, 1137,  303,  275,\n",
      "         381,  499,  126,  679,  455,   11,  559,  180,   59,  278,   58,  712,\n",
      "         937,   47,   36,   60,  172,  177,   25,  352,  126,   28,  196,  324,\n",
      "          20,  163,  410,   66,  107,   32,  353,  433,  733,  140,   80, 1136,\n",
      "         133,  648,  206,  235,  460,  663,  433,  226,  727,   72,  479,   51,\n",
      "         303,  107,   23,  259,  770,  145,  248,  165,  352,  241,  145,  284,\n",
      "          57,  235,   72,  181,  527,  143,   19,  275,  103,  194,  399,  154,\n",
      "          37,  442,   96,  177,  110,  504,  236,  334])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([  44,  277,   73,  293,   32,  270,   48, 1013,  328,  455,  120,   29,\n",
      "          18,   70,  303,  207,  288,   15,  119,  422,  622,   77,  202,    9,\n",
      "         137,   44,  152,   51,   80,   77,  528,  366,  148,   15,  426,   58,\n",
      "          73,  518,  572,   55,   50,  639,  423,   52,   21,  221,  518,  556,\n",
      "          66,  511,   45,  138,   66,  689,  114,   73,  173,   21,  206,   11,\n",
      "          97,  194,  550,   62,   76,  556,   54,   18,  432,  200,  544,  615,\n",
      "          25,   78,  743,  287,  500,   35,   12,  155,  171,   60,  423,   59,\n",
      "         316,  332,   53,  109,   87,  203,   50,  216,  244,  235,  660,   10,\n",
      "          93,  749,  581,  164,   35,  155, 1093,   91,  165,  185,  371,  437,\n",
      "         296,  331,  157,   64,   55,  449,  335,  305,   59,   74,   46,  854,\n",
      "          73,   71,  303,  198,   22,  132,  313,  505])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([ 182,  252,   15,  114,   42,   57,   73,   70,   30,  222, 1086,   50,\n",
      "          37,  236,  915,  131,  539,  120,   94,   41,  338,  485,  160,   41,\n",
      "         446,   34,  123,  166,   56,  331,   62,   44,  211,   89,  200, 1196,\n",
      "         132,  468,    8,   86,  142,  381,  153,  322,   56,  307,  144,   81,\n",
      "         258,  270,   86,   14,  271,  705,  101,  262,   34,  110,  220,   82,\n",
      "          91,  102,  139,  156,   41,  105,   96,  281,   78,  203,  255,  168,\n",
      "         318,  120,  263,   14,  203, 1467,   20,   75,  327,   57,  157,  305,\n",
      "         295,  385,   90,   87,   46,  319,  170,  591,  102,  318,  363,   50,\n",
      "          92,   87,  103,  104, 1382,  278,  275,  165,   25,  245,   66,   84,\n",
      "         416,   55,  286,  691,  420,  238,  363,  194,   37,  515,   20,  424,\n",
      "          76,  271,  732,  143,  433,  355,  596,   84])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([  45,   31,  270,   13,  198,   86,  242,  115,  198,   79,   38,   42,\n",
      "         208,  271,  118,   49,   41,   51,   40,  491,  402,  200,  655,  194,\n",
      "         270,  156,  130,  139,  146,   29,  114,   71,  114,  163,   84,  612,\n",
      "          24,   91,  138,  204,  490,   67,   17,   14,  627,  529,   39,  439,\n",
      "         350,  217,  225,   31,  274,  489,   27,   46,   94,  183,  256,  181,\n",
      "         122,   76,  176,  114, 1342,   83,  519,   81,   92,  190,  134,   17,\n",
      "         127,   69,  236,   53,   42,  767,   29,   80,  168,   57,  420,  220,\n",
      "         273,   21,  131,   74,   51,  662,  142,   71,  874,   84,  519,  198,\n",
      "          65,  135,  871,  120,  219,  138,  268,  261,  157,  343,  185,   43,\n",
      "         111,   75,   51,  156,  123,  446,   42,   23,  118,   67,   43,   50,\n",
      "         214,   85,   34,  291,  110,  108,   22,  166])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([ 24, 448, 339, 104,  50, 432,  71, 177, 706, 260, 382, 496, 331, 167,\n",
      "         68,  51, 164, 571,  19,  41, 575,  35, 335, 190, 175, 474, 109,  54,\n",
      "         65,  16, 421,  15,  40, 115,  48,  31,  80,  60, 113,  51, 158,  89,\n",
      "         23,  36, 326, 158, 170, 225, 258,  94,  86, 165, 427,  41, 137, 892,\n",
      "        885,  42,  37, 485, 833, 340,  64, 309, 383,  58, 165,  87,  78, 276,\n",
      "        405, 448, 200, 300,  38,  72, 424, 534,  85, 346,  59,  44, 147, 231,\n",
      "        193,  46,  16, 264,  36,  61,  41, 612,  37,  45,  51, 348, 467, 110,\n",
      "         65, 346,  50, 141,  36, 110,  80,  48,  44,  19,  89,  63, 407,  65,\n",
      "        138, 122, 540, 225,  22,  77, 287,  28, 809,  11, 314, 833, 367, 120,\n",
      "        120, 220])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([ 434,  617, 1265,   39,  472,  144,  175,   10,  315,   69,  415,  111,\n",
      "         236,   81,   89,   97,   81,  407,  429,  109,  130,  105,  510,  147,\n",
      "         239,   94,  983,  278,  139,  124,  300,  492,  694,   73,  123,   50,\n",
      "         164,  127,  162,  280,   55,  292,  174,  160,   97,   58,   20,   81,\n",
      "         519,  193,   97,   56,   75,  130,  145,  107,  256,   41,   15,   90,\n",
      "          38,  643,   34,   90,  138,  261,  390,  471,  390,  420,   20,  292,\n",
      "         417,  124,  829,  404,   12,   60,  208,  361,   87,   76,   41,  427,\n",
      "         936,   61,   41,  107,  151,  274, 2700,  283,  145,   59,  369,   72,\n",
      "          38,   83,   96,  946,  442,  536,   49,   47,  354,  113,   33,  270,\n",
      "         134,  201,  360,   46,   25,  463,  133,  152,  212,  115,  578,  434,\n",
      "          29,  171,  503,   66,   32,   66,  130,   86])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([ 22, 422,  37, 145,  31,  49, 373,  90, 133, 235, 122, 341, 180, 103,\n",
      "         92,  12, 118,  95, 330,  69, 314,  99, 160, 141, 759,  64,  38, 105,\n",
      "        208, 221, 221, 361, 562, 106, 728,  69,  38,  92, 121,  26,  22, 117,\n",
      "         64,  12, 496,  84, 266, 933, 407,  38, 301, 689, 426, 138, 224, 285,\n",
      "        855,  38, 444,  50,  92,  45,  85,  43, 916, 627, 337,  42,  20,  72,\n",
      "        747,  63,  52, 116,  95, 212,  62, 319, 163,  35, 249, 136, 103, 339,\n",
      "         68,  93, 143, 240,  97, 875, 352, 157, 432, 209, 276, 191,  58, 293,\n",
      "         64, 126, 355, 144, 473, 106, 250, 197, 319,  11,  28, 367, 107,  56,\n",
      "        213,  97,  60, 407, 360,  17, 276,  22, 218,  92, 865, 266, 155,  51,\n",
      "        205,  56])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([  94,  115,   88,   87,  218,  131,  146,  228,   57,   31,  890,  323,\n",
      "          36,  341,  796,  153,  319,  578,   60,  269,  131,  111,  216, 2135,\n",
      "         204,  100,  209,  156,  121,  306,  294,   70,   40,  294,  568,   82,\n",
      "         206,  158,   95,   70,  103,  351,   25,  217,  395,  222,  602,  539,\n",
      "         188,  242,  340,  324,  236,  472,   91,  278,  157,  471,   78,  288,\n",
      "         715,  341,  302,  354,   54,   92,  779,  137,   73,  353,    9,   18,\n",
      "         113,  293,   54,  525,  671,   48,  140,  778,  380,   64,  123,   69,\n",
      "          42,   61,   55,   78,  269,   51,  162,  161,  151,  138,  113,  223,\n",
      "          91,   11,   59,   91,  487,  125,  181,  218,  148,  103,  121,  366,\n",
      "         166,  938,  183,   40,   56,  242,  292,  330,   95,  211,   66,   79,\n",
      "          27,  203,  388,  237,   76,   59,  247,  251])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([ 441,  147,   91,  134,   41,   36,   35,  203,  169,  241,    9,  302,\n",
      "         709,   76,   86,  130,   22,  763,   38,   38,  405,   86,  397,  293,\n",
      "         295,  131,  206,   90,  552,  239,  111,  160,  104,   81,  138,   80,\n",
      "          87,  286,  106,  132,   36,  469,   88,  244,  233,  510,  372,  309,\n",
      "         179,   20,  242,  245,  185,   56,   42,  319,  151,  294,  250,  447,\n",
      "         153,  409,  180,  103,  174,  944,   90,  260,   71,   59,   38,  103,\n",
      "          32,  284,  200,  147,  260,  476,   46,   70,   47,  673,  101,  160,\n",
      "          30,   12,  230,   20,  134,   47,  165,  191,   52,  148,  159,   23,\n",
      "         623,  179,  511,   62,   52,  217,   62,   59,   88,   78,  246,  132,\n",
      "        1127,  189,  497,  181,  123,   25,  109,  141,  380,  121,  415,   20,\n",
      "         629,  323,  927,  538,   43,   35,   88,   63])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([  19,  155,  136,  326,  214,   62,  157,   31,   38,   52,  289,   40,\n",
      "         399,  457,   48,  159,  494,   76,   36,  302,  351,  215,   31,  542,\n",
      "         758,   39,   92,  266,  198,   99,  295,   60,  236,   38,   45,   17,\n",
      "          65,   61,  724,  589,  284,   75,   44,  470,  231,   77,   20,  596,\n",
      "         182,   20,  412,  153,  148,  255,  178,  171,  276,   28,  110,  180,\n",
      "        1108,  345,   57,  651,   14,  540,  145,  128,  338,  379,   68,  180,\n",
      "         179,  213,  231,   29,  138,   94,   81,  233,  428,   73,  339,   96,\n",
      "          31,  187,   36,  140,   88,   55, 1105,    8,   33,  422,   25,   51,\n",
      "          71,  115,  193,  181,   84,   99,  169,  928,  153,  111,   67,   26,\n",
      "         205,   78,  249,  101,  198,   72,  233,   63, 1169,  570,  676,  166,\n",
      "         366,  148,  539,  631,   51,   12,   45,  379])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 4250, 300])\n",
      "tensor([  26,  320,  100,  259,   66,  189,  198,   60,   34,  208,  230,   21,\n",
      "         430,  336,  451,   63,   91,   51,  235,  405,   11,  195,  488,   28,\n",
      "         304,  378,  106,  343,   83,   57,   23,  426,   58,  232,  381,   65,\n",
      "         172,   63,  488,  687,  248,  235,   58,  110,   23,  393,  345,   96,\n",
      "         106,  173,  217,   69,   64,  198,   37,   58,   55,  117,   75,   91,\n",
      "         245,   36,   84,   30,  404,   87,  236,   84,  459,  403,   87,  629,\n",
      "         239,  138,   70,   88,  156, 1024,  105,  215,  189,   43,  715,   39,\n",
      "         418,  111,   33,   43,  171,  485,  851,   39,  154,   79,  153,   66,\n",
      "          76,  255,   24,   65,  103,   74,   83,  150,  631,   36,   89,   15,\n",
      "          58,  258,  169,   94,  629,   68,  357,  122,  734,  237,   41,   79,\n",
      "         451,   30,  514,  143,   71,  292,  135,  191])\n",
      "torch.Size([128, 4250, 300])\n",
      "Epoch: 001/001 | Batch 040/080 | Cost: 0.4005\n",
      "tensor([  20,  184,  423,  194,  157,  169,  300,  330,  427,  132,  192,   59,\n",
      "          34,   92,  327,  396,   79,  429,  717,  504,   26,   80,  233,  135,\n",
      "          99,   76,  410,   58,   14,  152,  310,  236,  461,   58,  159,  202,\n",
      "         462,  879,  156,  120,  228,  137,  111,  282,  477,   87,  192,  485,\n",
      "         288,  809,  118,  140,  224,  161,  356,   72,  129,  197,   28,  261,\n",
      "        1090,  536,  238,  296,  334,  297,  468,   28,  165,  530,   33,  176,\n",
      "         153,  419,  134,   20,  320,  773,  158,  141,   37,  163,  359,   91,\n",
      "         193,  375,   39,  371,  249,  162,  112,  286,   60,   81,  757,  618,\n",
      "          44,  124,  173,  519,  169,   33,   47,  256,   54,  498,   90,  101,\n",
      "         412,  317,  273,  592,  143,   99,  109,  115,   49,   55,  359,   39,\n",
      "         128,  404,   59,  360,   29,   48,  129,  571])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([  16,   55,   37,   58,  784,   72,   20,  237,   38,  103,  243,   77,\n",
      "          42,   57,   10,  307,  105, 1243,  123,  292,  359,  294,   65,  147,\n",
      "          70,  224,  169,   48,  477, 1192,  533,  266,   67,  252,  211,  262,\n",
      "         594,   70,   56,  327,  376,  237,  435,  678,  201,  191,  322,  157,\n",
      "           8,  289,  117,  470,   87,   31,  211,  128,  615,  135,   72,   30,\n",
      "         775,  218,  147,  104,    9,   91,   73,   56,  395,  763,  111,  410,\n",
      "          46,  102,  230,  636,  151,   50,  137,  764,   86,  193,  316,  430,\n",
      "         248,   71,  781,  216,   45,  293,   62,   91, 1145,  198,   96,  377,\n",
      "         392,   19,  522,   52,  141,  884,  205,  105,   26,   46,   76,  392,\n",
      "         579,  584,   97,   14,   98,  469,   19,   62,  122,   19,  257,  226,\n",
      "         771,  192,  274,  615,   57,  365,   23,   25])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([ 860,  164,   59,  175,  387,  484,   44,   25,   72,   10,  299,  181,\n",
      "          82,  160,  249,  357,  376,   66,  121,   50,   79,   41, 1399,   34,\n",
      "         192,  230,   65,  163,  490,   59,  244,  202,  344,  150,   24,   97,\n",
      "         200,   44,  229,   23,  631,  115, 1925,   62,  193,   21,   42,   75,\n",
      "          79,  361,  112,  128,  293,   78,  350,   91,  446,  512,  253,  181,\n",
      "         371,  241,  148,  774,   67,  295,  363,   82,   71,  576,  336,   33,\n",
      "         362,  340,  117,   96,   86,  625,  224,  129,  432,  696,  235,  776,\n",
      "         582,  119,  342,  321,  126,   50,   39,  632,   77,   22,   83,   53,\n",
      "         255,   50,  241,  477,  251,  396,   23,   34,  311,  196,  484,   12,\n",
      "         446,   21,  279,  169,   15,  419,  990,   55,   95,  336,  205,   26,\n",
      "         214,   96,   61,  545,  343,  139,  274,   63])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([ 257,   86,  408,  372,   27,   23,  387,   10,   64,   45,  641,   98,\n",
      "         152,   52,  304,  101,  263,   19,  190,  319,  519,  105,  265,   99,\n",
      "         718,  199,  278,  445, 1036,  151,   51,  152,   33,   46,   32,  180,\n",
      "          50,  230,   91,  217,  193,  146,  109,   49,  134,  154,   60,   32,\n",
      "          71,  140,   54,   39,  203,  105,  421,  226,  823,   97,  189, 1142,\n",
      "         129,   43,   90,  180,  475,  159,  675,   87,   67,   89,   65,  142,\n",
      "         131,   98,   75,  105,   61,  308,   28,   33,   51,  299,  279,   43,\n",
      "          93,  169,  265,   31,   47,   89,  195,  132,  116,  115,   33,   92,\n",
      "         152,   71,  180,   13,  238,  114,   64,  127,  204,  121,   16,  199,\n",
      "         350,  643, 1304,  111,  222,  199,   90,   52,   61,  140,  163,  157,\n",
      "         365,   40,   68,   57,   23,  569,   41,   50])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([ 294,   48,   21,  341,  571,  123,  708,   57,  173,  308,  146,  121,\n",
      "         291,   63,  125,   54,   56,  509,  296,  285,   96,  135,  124,  173,\n",
      "         343, 1259,   61,  323,   60,   63,   14,  236,   88,  107,   30,    9,\n",
      "          15,   49,  202,  178,  288, 1035,  481,  738, 1068,   75,  355,   69,\n",
      "         231,  750,   14,    9,  319,  104,  300,   73, 1033,  118,  145,  303,\n",
      "         188,  338,  145,  722,  104,  526,  257,  124,   77,  196,  232,   63,\n",
      "         145,  139,  637,   83,   16,  922,   31,   37,   72,   52,   85,  429,\n",
      "         332,   41,  117,   95,  123,   47,  452,   30,   32,   83,  192,  269,\n",
      "         293,  302,  316,  498,   88,   69,   17,  227,  258,   30,  216,  336,\n",
      "          98,  355,   86,  357,  325,  340,   32,  108,   68,  122,  187,  246,\n",
      "          32,   99,  177,   88,  256, 1287,  452,   75])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([ 266,  101,   50,  192, 1759,   25,  431,   46,   60,   50,   44,   42,\n",
      "          84,  135,   37,  575,  319,  205,  221,   95,  191,  141,  494,  131,\n",
      "         144,  335,   73,  461,   38,  155,  218,  798,   39,  238,   69,  130,\n",
      "         163,  241,  217,   28,  109,   69,   94,   80,  321,  340,  328,   45,\n",
      "         110,   88,  367,  164,  166,  308,   48,   74,   54,  216,   24,  294,\n",
      "         198,   82,   63,  388,  145,   11,  245,  254,  103,   87,   40,   37,\n",
      "          13,  302,  677,   70, 1207,   57,  227,   66,  290, 1219,  132,  385,\n",
      "         369,   49,  448,   56,  415,   53,  472,   69,  130,   76,  115,   92,\n",
      "          57,   67,  277,  172,  330,   41,   49,  265,  119,   39,   26,  761,\n",
      "         116,  202,   68,  298,  240,   22,   70,  324,  151,   51,  181,  137,\n",
      "         115,   48,  488, 1285,   68,   98,   39,  195])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([  47,  598,  107,  314,  241,  143,   50,  151,  600,  112,   49,   19,\n",
      "          63,  267,  338,   22,  184,   83,  466,  161,  412,   31,  471,  700,\n",
      "          20,  243,   66,  370,  296,   71,  167,   37,    9,   96,  422,  195,\n",
      "         391,   51,   71,  143,   57,  405,   85,  350,  185,  149,  196,  436,\n",
      "          94,  234,  457,  234,   38,   29,   64,  583,  264,  251,  160,  121,\n",
      "         103,  497,  230,  322,   72,  204,   41,  221,  226,   34,   28,  185,\n",
      "          36,    9,  274,   53,  186,   93,  138,   43,   37,  114,   65,  193,\n",
      "          77,   77,  139,   21,  111,  565,  447,  724,  324,  326,  126,  140,\n",
      "        1406,  118,   69,  219,   57,  205,  155,  247,  246,  115,   16,   84,\n",
      "         253,   48,  475,   87,  600,  534,  157,  206,   23,   82,  372,   48,\n",
      "          38,   15,  263,  221,   97,  429,  192,   83])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([  72,  445,   63,   79,  335,   67,   93,   90,  408,   42,   36,   48,\n",
      "          40,   17,   80,  333,  153,  174,  219,   40,   28,  326,   65,  356,\n",
      "          63,  169,  201,  139,  108,  275,   53,   31,  168,  372,  104,  744,\n",
      "         453,   13,   31,  306,  173,   45,  499,  651,  146,   87,  229,  539,\n",
      "          71,   54,   66,  426,   69,  140,   20,  250,   30,   75,  830,  198,\n",
      "         378,  422,   39,   15,  166,  202,  193, 1015,  260,   38,  101,   96,\n",
      "         104,  122,  339,  296,  422,  364,   37,   19,   29,  255,  242,   58,\n",
      "          58,  568,   64,  287,  139,   56,  193,  347,  646,   68,  470,  162,\n",
      "         178,  201,  298,  325,  279,   37,   38,  243,   63,  635,  317,  269,\n",
      "         277,   14,   31,   91,  447,  137,  108,   13,  630,  374,   84,  298,\n",
      "           9,  174,  269,  392,  444,  116,   53,   50])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([  11,   65,  123,  804,  220,  189,  658,  556,   57,  508,  356,   53,\n",
      "          42,   95,   48,  144,  510,  598,  294,   36,   71,  211,   28,  453,\n",
      "         712,   55,  305,   37,  158,  187,  200,   54,  352,  105,   84,  746,\n",
      "          67,  211,  119,  483,  363,  167,   82,  238,  247,   85,   49,   87,\n",
      "         359,   28,  358,  225,   98,   33,  391,  445,   57,  328,  125,  683,\n",
      "         533,  291,   51,  182,  231,   46,  290,   58,   91,  885,   81,   30,\n",
      "        2742,  475,   29,  786,  250,   67,  237,  112,   51,  108,   44,  118,\n",
      "          15,  161,  379,  101,  145,    8,   75,  359,  277,  361,   70,   27,\n",
      "          99,  639,  337,   31,  128,  246,  229,   92,  235,   63,  405,  463,\n",
      "         138,  272,  166,   46,  130,  478,  125,  540,  121,  151,  719,   40,\n",
      "         102,  333,  142,  176,   74,  109,  429,   40])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 4250, 300])\n",
      "tensor([  78,  471,  283,  107,   56,  248,  115,  119,   27,  266,   63,   78,\n",
      "         294,   69,  371,  260,   69,  663,  350,   85,  137,  308,  209,  319,\n",
      "        1649,   77,   50,  374,   65,  253,   24,   81,  175,  105,   31,   50,\n",
      "         225,   21,  262,  738,  422,  190,   69,  186,  728,   12,   96,   91,\n",
      "         652,  348,  522,   67,   31,   85,  581,   45,   47,  517,  132,  169,\n",
      "          51,  383,  949,  139,  512,  307,  240,  327,  331,  210,  335,  254,\n",
      "         532,  766,  174,  416,  185,   58,  129,  167,  242,  321,  110,  147,\n",
      "         576,  338,   74,   98,   75,  196,  274,   38,  105,  333,  366,  195,\n",
      "         210,  132,   41,  493,   75,   16,   39,  133,   60,  207,  188,  218,\n",
      "         288,   23,  491,  143,  197,  131,  320,  484,  120,  117,   57,  575,\n",
      "         305,   13,  135,  433,   20,  253,  637,   26])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([ 21, 304, 225, 481,  36, 141, 437, 329, 463, 116, 540, 247, 196, 315,\n",
      "        300, 308, 141, 150,  49,  61, 225,  67, 163, 298,  24, 205,  56, 144,\n",
      "         31, 222,  91, 224, 206, 119,  24,  55,  65, 793, 184,  71, 206, 117,\n",
      "         86,  74,  58, 203, 304, 769,  39, 211, 350,  73,  54,  92,  40,  33,\n",
      "         62,  27,  89, 235,  80,  27,  14, 356, 953, 253,  98, 308, 180,  11,\n",
      "         72,  65,  34, 160, 201, 152,  52, 111, 843, 109,  50,  76,  69, 399,\n",
      "         79,  62,  95,  45, 647, 132,  72,  32, 330, 314,  56, 304, 206,  50,\n",
      "        316, 103,  45, 177, 815, 932, 391, 424, 244, 255, 461,  45,  59,  96,\n",
      "        385, 416, 226, 284, 557,  44,  42,  97, 434,  20, 240, 201,  26,  31,\n",
      "        308, 261])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([ 220,  390,   85,  306,  417,  297,  717,   80,  275, 1018,  559,   56,\n",
      "          75,  257,  155,  137,  394,  360,  117,   65,   63,  346,   70,  320,\n",
      "          63,  193,  401,  687,   84,   71,   96,  197,  218,   54,  265,  133,\n",
      "         199,  165,   37,   24,  145,  414,  372,  127,  262,  142,   21,   75,\n",
      "          33,  281,   88,   55,  118,  124,  106,   69,  330,  513,   25,  488,\n",
      "         853,  241,  150,  407,  580,  250,  151,  107,  535,   69,  126,   56,\n",
      "          77,   62,  145,  240,  468,   26,  120,  296,  134,  142,   79,  140,\n",
      "         165,  123, 1371,   89,  290,   36,   20,  273,   22,  205,  531,  193,\n",
      "         212,  237,  129,   58,   96,  118,  679,  320,   43,  153,  125,   91,\n",
      "          66,   46,   88,  158,  485,  823,  146,  175,   11,  213,   12,  453,\n",
      "         129,   73,  111,   19,  112,  282,  204,   52])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([  44,  312,   95,  418,  396,  202,  306,  146,   77,   48,  155,  184,\n",
      "         224,  197,   43,  452,  103,   52,  178,  531,  237,   45,  232,  133,\n",
      "         220,   51,   69,   40,  161,  741,  173,  128,  186,   83,   23,   25,\n",
      "         162,  158,  318,   34,  472,   14,   63,   37,  790,   18,  355,   39,\n",
      "         300, 1047,  286,   43,   68,  253,  203,  120,   52,   23,  191,  217,\n",
      "         335,   76,   59,  280,  100,   55,  543,  159,  211,   57,  118, 1952,\n",
      "         170,   49,   86,  276,  143,  340,  253,  256,  373,  524,   24,  124,\n",
      "         336,  100,   39,  117,  256,  595,  403,  141,  107,   89,   88, 1007,\n",
      "         293,   77,  179,  171,   78,   13,  123,  145,  297,   47,  243,   48,\n",
      "         116,   25,  107,  162,  136,   41,  189,  203,   89,  118,  231,  100,\n",
      "         287,   94,  409,   48,  181,   98,  135,  516])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([ 25, 194, 110, 144, 245,  57, 341,  57, 126,  31, 204,  62, 385, 267,\n",
      "        386,  72, 227,  20, 100, 427, 192,  84, 314, 198, 349, 822,  24,  43,\n",
      "        107, 212,  51, 386,  29, 289, 486, 174, 367,  72, 593, 889,  35, 343,\n",
      "        566,  30, 219, 423, 206, 206,  50, 109, 369,  35, 217, 150, 605,  63,\n",
      "        408, 140, 243,  32, 281, 487, 105, 266,  38,  63,  42, 282, 177, 109,\n",
      "        231,  72, 294,  21, 393, 495, 324,  66, 108, 123, 361, 341, 186, 157,\n",
      "         77, 770,  39,  54, 233, 432, 622, 463, 282,  42, 282, 694, 424,  48,\n",
      "        390,  85,  87,  92, 143, 290, 269, 324, 269, 515, 119, 123, 248, 548,\n",
      "        144,  19, 451,  53,  36,  80, 179, 223, 238, 140, 546,  57, 116, 591,\n",
      "        111, 212])\n",
      "torch.Size([128, 4250, 300])\n",
      "tensor([ 273,  167,   61,   52,  130,   65,   61,   75,  245, 1886,   42,  281,\n",
      "         167,  288,   18,   57,  277,   35,   92,   39,  105, 1283,   69,   27,\n",
      "         475,   43,  668,  642,  216,   98,  100,  496,  206,  319,   77,  675,\n",
      "         250,  115,  103,  144,   20,   85,  243,   66,  106,   66,  627,  285,\n",
      "         166,   58,  615,   85,   83,  643,   41,  399,   61,  776,  210,  603,\n",
      "         485,  250,   36,  233,  348,  397,  195,   49,  217,  253,  155,  128,\n",
      "          50,   59,  193,  149,  186,  147,   42,  301,   40,  143,   42,   33,\n",
      "         521,   38,   73,  267,  145,   38,  279,   20,   70,  165,  115,  764,\n",
      "         110,  259,   48,  305,  200,  221,  263,   83,   84,  449,   78,  157,\n",
      "         235,   93,  268,  557,  754,  234,   96, 1120,  248,  116,  185,  402,\n",
      "         173,  443,   13,   83,  222,  188,   23,   68])\n",
      "torch.Size([128, 4250, 300])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_accuracy_and_loss(model, data_loader, device):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    loss = 0.\n",
    "    for i, (features, targets, features_len) in enumerate(data_loader):\n",
    "            \n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        features_len = features_len.to(device)\n",
    "        print(features_len)\n",
    "        logits, probas = model(features, features_len)\n",
    "        loss += criterion(probas, targets.float()).item()\n",
    "        predicted_labels = (probas > 0.5).cpu().numpy()\n",
    "        num_examples += targets.size(0)\n",
    "        \n",
    "    average_loss = loss/ num_examples\n",
    "    accuracy = accuracy_score(targets.cpu().numpy(), predicted_labels)\n",
    "    #ACU = roc_auc_score(targets.cpu().numpy(), predicted_labels)\n",
    "    f_measure = precision_recall_fscore_support(targets.cpu().numpy(), predicted_labels,average='weighted')\n",
    "    return accuracy, average_loss, f_measure\n",
    "    \n",
    "\n",
    "start_time = time.time()\n",
    "train_acc_lst, valid_acc_lst = [], []\n",
    "train_loss_lst, valid_loss_lst = [], []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (features, targets, features_len) in enumerate(train_loader): #, \n",
    "\n",
    "        ### PREPARE MINIBATCH\n",
    "        features = features.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "        features_len = features_len.to(DEVICE)\n",
    "#         features, features_lengths = batch.features\n",
    "#         targets = batch.targets\n",
    "        print(features_len)\n",
    "        #retrieve text and no. of words\n",
    "        #print(features_len)\n",
    "            \n",
    "        ### FORWARD AND BACK PROP\n",
    "        features = features.view(features.size(0), -1)\n",
    "        logits, probas = model(features, features_len)\n",
    "        cost = criterion(probas, targets.float())\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 20:\n",
    "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
    "                   f'Batch {batch_idx:03d}/{len(train_loader):03d} |' \n",
    "                   f' Cost: {cost:.4f}')\n",
    "        if not epoch % 10:\n",
    "            scheduler.step()\n",
    "        \n",
    "    # no need to build the computation graph for backprop when computing accuracy\n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False):\n",
    "        train_acc, train_loss, train_f_measure = compute_accuracy_and_loss(model, train_loader, device=DEVICE)\n",
    "        valid_acc, valid_loss, valid_f_measure = compute_accuracy_and_loss(model, valid_loader, device=DEVICE)\n",
    "        train_acc_lst.append(train_acc)\n",
    "        valid_acc_lst.append(valid_acc)\n",
    "        train_loss_lst.append(train_loss)\n",
    "        valid_loss_lst.append(valid_loss)\n",
    "        print(f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} Train Acc.: {train_acc:.2f}%'\n",
    "              f' | Validation Acc.: {valid_acc:.2f}%')\n",
    "\n",
    "    elapsed = (time.time() - start_time)/60\n",
    "    print(f'Time elapsed: {elapsed:.2f} min')\n",
    "    print('the precision_recall_fscore_support of F-score_train: ', train_f_measure)\n",
    "    print('the precision_recall_fscore_support of F-score_val: ', valid_f_measure)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "VqIyq4Mkp7Ed",
    "outputId": "90ed34ce-1dbe-4d2a-a0a4-c77b196e8900"
   },
   "outputs": [],
   "source": [
    "plt.plot(range(1, NUM_EPOCHS+1), train_loss_lst, label='Training loss')\n",
    "plt.plot(range(1, NUM_EPOCHS+1), valid_loss_lst, label='Validation loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Binary cross entropy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "colab_type": "code",
    "id": "7Qwj0ppfp7Ee",
    "outputId": "37cb88eb-caf8-4b29-8af0-da3fb7000f84"
   },
   "outputs": [],
   "source": [
    "plt.plot(range(1, NUM_EPOCHS+1), train_acc_lst, label='Training accuracy')\n",
    "plt.plot(range(1, NUM_EPOCHS+1), valid_acc_lst, label='Validation accuracy')\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()\n",
    "print(f'Training accuracy: {train_acc_lst[-1]:.2f}%')\n",
    "print(f'Validation accuracy: {valid_acc_lst[-1]:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "f0Nxk_8Up7Eg",
    "outputId": "df2abf9f-8270-4a10-ecc0-d96e04bea5cc"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.set_grad_enabled(False): # save memory during inference\n",
    "    test_acc, test_loss, test_f_measure = compute_accuracy_and_loss(model, test_loader, DEVICE)\n",
    "    print(f'Test accuracy: {test_acc:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "KyHBQ3l0p7ER"
   ],
   "machine_shape": "hm",
   "name": "revised_RNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
