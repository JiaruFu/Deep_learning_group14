{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "revised_RNN_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "KyHBQ3l0p7ER"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9Da6mexrp-7L",
        "outputId": "9d48ba96-2c5c-40da-95bb-f0d25fbaec49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NPnQKgpbp7DO",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import re \n",
        "import math\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LxuiAKnnp7DR",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WEHoFSmDp7DU",
        "colab": {}
      },
      "source": [
        "# import spacy\n",
        "# # Need to load the large model to get the vectors\n",
        "# import en_core_web_sm\n",
        "# nlp = en_core_web_sm.load()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QCC0-w4ap7DW",
        "colab": {}
      },
      "source": [
        "RANDOM_SEED = 123\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4rnUxezBRmFg",
        "outputId": "5398a163-d1e8-447e-eaeb-6aa61c912166",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "DEVICE"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UyUJdT3Gp7DY",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# read the text file and add the column names\n",
        "#read_file = pd.read_csv(r\"./booksummaries.txt\", sep='\t', header=None)\n",
        "read_file = pd.read_csv(r\"/content/drive/My Drive/Colab Notebooks/final project/booksummaries.txt\", sep='\t', header=None)\n",
        "read_file.columns = ['ID', 'm number', 'book name', 'author name', 'date', 'label', 'summary']\n",
        "\n",
        "# clean data\n",
        "read_file['label'] = read_file['label'].str.replace(r'/m/\\S*\\s', '')\n",
        "read_file['label'] = read_file['label'].str.replace(r'{', '')\n",
        "read_file['label'] = read_file['label'].str.replace(r'}', '')\n",
        "read_file['label'] = read_file['label'].str.replace(r'}', '')\n",
        "read_file['label'] = read_file['label'].str.replace(r'\\\\u00e0\\s+clef', '')\n",
        "\n",
        "# select columns\n",
        "new_file = read_file.loc[:, ['book name', 'label', 'summary']]\n",
        "\n",
        "#delete the columns with no labels\n",
        "new_file.dropna(axis = 0, how = 'any', inplace = True)\n",
        "new_file = new_file.iloc[:, [0, 2, 1]]\n",
        "\n",
        "new_file = new_file.reset_index(drop=True)\n",
        "\n",
        "#output data as csv\n",
        "#new_file.to_csv(r'/content/drive/My Drive/Colab Notebooks/final project/booksummries.csv', index=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TABDJ59Rp7Da",
        "outputId": "a3d84fa4-8d57-40a9-8599-ac9224f554b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "new_file.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>book name</th>\n",
              "      <th>summary</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Animal Farm</td>\n",
              "      <td>Old Major, the old boar on the Manor Farm, ca...</td>\n",
              "      <td>\"\"Roman \", \"\"Satire\", \"\"Children's literature\"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A Clockwork Orange</td>\n",
              "      <td>Alex, a teenager living in near-future Englan...</td>\n",
              "      <td>\"\"Science Fiction\", \"\"Novella\", \"\"Speculative ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The Plague</td>\n",
              "      <td>The text of The Plague is divided into five p...</td>\n",
              "      <td>\"\"Existentialism\", \"\"Fiction\", \"\"Absurdist fic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A Fire Upon the Deep</td>\n",
              "      <td>The novel posits that space around the Milky ...</td>\n",
              "      <td>\"\"Hard science fiction\", \"\"Science Fiction\", \"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>All Quiet on the Western Front</td>\n",
              "      <td>The book tells the story of Paul Bäumer, a Ge...</td>\n",
              "      <td>\"\"War novel\", \"\"Roman \"</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        book name  ...                                              label\n",
              "0                     Animal Farm  ...  \"\"Roman \", \"\"Satire\", \"\"Children's literature\"...\n",
              "1              A Clockwork Orange  ...  \"\"Science Fiction\", \"\"Novella\", \"\"Speculative ...\n",
              "2                      The Plague  ...  \"\"Existentialism\", \"\"Fiction\", \"\"Absurdist fic...\n",
              "3            A Fire Upon the Deep  ...  \"\"Hard science fiction\", \"\"Science Fiction\", \"...\n",
              "4  All Quiet on the Western Front  ...                            \"\"War novel\", \"\"Roman \"\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rTtCwMPmp7Dd",
        "colab": {}
      },
      "source": [
        "def text_process(label_list):\n",
        "    has_fiction = False\n",
        "    has_spec_fiction = False\n",
        "    has_novel = False\n",
        "    has_spec_novel = False\n",
        "    for i in range(len(label_list)):\n",
        "        if 'novel' in label_list[i].lower():\n",
        "            if 'novel' == label_list[i].lower():\n",
        "                has_novel = True\n",
        "            else:\n",
        "                has_spec_novel = True\n",
        "        if 'fiction' in label_list[i].lower():\n",
        "            if 'fiction' == label_list[i].lower():\n",
        "                has_fiction = True\n",
        "            else:\n",
        "                has_spec_fiction = True\n",
        "        \n",
        "    if has_spec_fiction and has_spec_novel:\n",
        "        if has_fiction:\n",
        "            label_list.remove('fiction')\n",
        "        if has_novel:\n",
        "            label_list.remove('novel')\n",
        "    elif has_spec_fiction:\n",
        "        if has_fiction:\n",
        "            label_list.remove('fiction')\n",
        "        if has_novel:\n",
        "            label_list.remove('novel')\n",
        "    elif has_spec_novel:\n",
        "        if has_fiction:\n",
        "            label_list.remove('fiction')\n",
        "        if has_novel:\n",
        "            label_list.remove('novel')\n",
        "    elif has_fiction and has_novel:\n",
        "        label_list.remove('fiction')\n",
        "    return label_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C0wSYIeep7Df",
        "colab": {}
      },
      "source": [
        "import re\n",
        "for index in range(len(new_file['label'])):\n",
        "    label = new_file['label'][index].replace('\"', ''). lower()\n",
        "    label_list = re.split(', ', label)  \n",
        "    label_list = text_process(label_list)\n",
        "    new_file.xs(index)['label']= label_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cYRkEBhDp7Dh",
        "colab": {}
      },
      "source": [
        "#output data as csv\n",
        "new_file.to_csv(r'./booksummries.csv', index=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-gDhRTCap7Dj",
        "outputId": "26c0f13a-73b4-4524-a773-1975f06c37e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "new_file.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>book name</th>\n",
              "      <th>summary</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Animal Farm</td>\n",
              "      <td>Old Major, the old boar on the Manor Farm, ca...</td>\n",
              "      <td>[roman , satire, children's literature, specul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A Clockwork Orange</td>\n",
              "      <td>Alex, a teenager living in near-future Englan...</td>\n",
              "      <td>[science fiction, novella, speculative fiction...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The Plague</td>\n",
              "      <td>The text of The Plague is divided into five p...</td>\n",
              "      <td>[existentialism, absurdist fiction]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A Fire Upon the Deep</td>\n",
              "      <td>The novel posits that space around the Milky ...</td>\n",
              "      <td>[hard science fiction, science fiction, specul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>All Quiet on the Western Front</td>\n",
              "      <td>The book tells the story of Paul Bäumer, a Ge...</td>\n",
              "      <td>[war novel, roman ]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        book name  ...                                              label\n",
              "0                     Animal Farm  ...  [roman , satire, children's literature, specul...\n",
              "1              A Clockwork Orange  ...  [science fiction, novella, speculative fiction...\n",
              "2                      The Plague  ...                [existentialism, absurdist fiction]\n",
              "3            A Fire Upon the Deep  ...  [hard science fiction, science fiction, specul...\n",
              "4  All Quiet on the Western Front  ...                                [war novel, roman ]\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k5rWKWOFp7Dl",
        "outputId": "8a791d56-4627-42bc-bae6-880c563bd956",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        }
      },
      "source": [
        "categories = list(new_file['label'].values)\n",
        "all_labels = [ word for labels in categories for word in labels]\n",
        "counts = Counter(all_labels)\n",
        "# for i in categories:\n",
        "#     counts.append((i, new_file['label'][i].sum()))\n",
        "df_stats = pd.DataFrame(counts.items(), columns=['labels', '#books'])\n",
        "df_stats = df_stats.sort_values(by = '#books', ascending = False)\n",
        "df_stats.head(17)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>labels</th>\n",
              "      <th>#books</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>speculative fiction</td>\n",
              "      <td>4314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>science fiction</td>\n",
              "      <td>2870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>fantasy</td>\n",
              "      <td>2413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>children's literature</td>\n",
              "      <td>2122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>novel</td>\n",
              "      <td>1581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>mystery</td>\n",
              "      <td>1396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>fiction</td>\n",
              "      <td>1007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>young adult literature</td>\n",
              "      <td>825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>suspense</td>\n",
              "      <td>765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>crime fiction</td>\n",
              "      <td>753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>historical novel</td>\n",
              "      <td>654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>thriller</td>\n",
              "      <td>568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>horror</td>\n",
              "      <td>511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>romance novel</td>\n",
              "      <td>435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>historical fiction</td>\n",
              "      <td>388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>detective fiction</td>\n",
              "      <td>341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>adventure novel</td>\n",
              "      <td>330</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    labels  #books\n",
              "3      speculative fiction    4314\n",
              "4          science fiction    2870\n",
              "10                 fantasy    2413\n",
              "2    children's literature    2122\n",
              "33                   novel    1581\n",
              "19                 mystery    1396\n",
              "16                 fiction    1007\n",
              "54  young adult literature     825\n",
              "28                suspense     765\n",
              "47           crime fiction     753\n",
              "32        historical novel     654\n",
              "53                thriller     568\n",
              "17                  horror     511\n",
              "41           romance novel     435\n",
              "29      historical fiction     388\n",
              "27       detective fiction     341\n",
              "30         adventure novel     330"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PzGbvwSBp7Dn",
        "outputId": "dd3b651a-5bdd-4167-cc05-2cf2b1c359a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        }
      },
      "source": [
        "df_stats.iloc[:50, :].plot(x='labels', y='#books', kind='bar', legend=False, grid=True, figsize=(15, 8))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f5a9ce6c390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WCODjGEFp7Dp"
      },
      "source": [
        "  ## Encoding the Labels##\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-DQcm51Up7Dq",
        "colab": {}
      },
      "source": [
        "label_list = []\n",
        "for index in range(len(new_file['label'])):\n",
        "    object_label = new_file['label'][index]\n",
        "    for l in object_label:\n",
        "        if l not in label_list:\n",
        "            label_list.append(l)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dD79YUT-p7Dr",
        "outputId": "50cfc5f3-a851-4414-bfcb-b3300c906d05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(label_list)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "227"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NCpBJIs7p7Dt",
        "colab": {}
      },
      "source": [
        "one_hot = pd.DataFrame(np.zeros((12841, 227)), columns=label_list).astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8E5hJZ6Lp7Dv",
        "colab": {}
      },
      "source": [
        "for index in range(len(new_file['label'])):\n",
        "    object_label = new_file['label'][index]\n",
        "    for l in object_label:\n",
        "        one_hot[l][index] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yYEZmlRjp7Dx",
        "outputId": "1ecfc7ba-8cde-4fa9-ea92-3dd0bebf460e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        }
      },
      "source": [
        "one_hot"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>roman</th>\n",
              "      <th>satire</th>\n",
              "      <th>children's literature</th>\n",
              "      <th>speculative fiction</th>\n",
              "      <th>science fiction</th>\n",
              "      <th>novella</th>\n",
              "      <th>utopian and dystopian fiction</th>\n",
              "      <th>existentialism</th>\n",
              "      <th>absurdist fiction</th>\n",
              "      <th>hard science fiction</th>\n",
              "      <th>fantasy</th>\n",
              "      <th>war novel</th>\n",
              "      <th>bildungsroman</th>\n",
              "      <th>religious text</th>\n",
              "      <th>picaresque novel</th>\n",
              "      <th>gothic fiction</th>\n",
              "      <th>fiction</th>\n",
              "      <th>horror</th>\n",
              "      <th>invasion literature</th>\n",
              "      <th>mystery</th>\n",
              "      <th>epistolary novel</th>\n",
              "      <th>parody</th>\n",
              "      <th>psychological novel</th>\n",
              "      <th>farce</th>\n",
              "      <th>philosophy</th>\n",
              "      <th>science</th>\n",
              "      <th>dystopia</th>\n",
              "      <th>detective fiction</th>\n",
              "      <th>suspense</th>\n",
              "      <th>historical fiction</th>\n",
              "      <th>adventure novel</th>\n",
              "      <th>humour</th>\n",
              "      <th>historical novel</th>\n",
              "      <th>novel</th>\n",
              "      <th>sea story</th>\n",
              "      <th>cyberpunk</th>\n",
              "      <th>business</th>\n",
              "      <th>non-fiction</th>\n",
              "      <th>economics</th>\n",
              "      <th>anthropology</th>\n",
              "      <th>...</th>\n",
              "      <th>fable</th>\n",
              "      <th>bangsian fantasy</th>\n",
              "      <th>space western</th>\n",
              "      <th>historical fantasy</th>\n",
              "      <th>edisonade</th>\n",
              "      <th>military history</th>\n",
              "      <th>sword and sorcery</th>\n",
              "      <th>fantastique</th>\n",
              "      <th>youth</th>\n",
              "      <th>photography</th>\n",
              "      <th>modernism</th>\n",
              "      <th>medieval romance</th>\n",
              "      <th>paranormal romance</th>\n",
              "      <th>bit lit</th>\n",
              "      <th>contemporary fantasy</th>\n",
              "      <th>urban fiction</th>\n",
              "      <th>collage</th>\n",
              "      <th>subterranean fiction</th>\n",
              "      <th>superhero fiction</th>\n",
              "      <th>heroic fantasy</th>\n",
              "      <th>marketing</th>\n",
              "      <th>colonial united states romance</th>\n",
              "      <th>creative nonfiction</th>\n",
              "      <th>low fantasy</th>\n",
              "      <th>light novel</th>\n",
              "      <th>police procedural</th>\n",
              "      <th>fairytale fantasy</th>\n",
              "      <th>indian chick lit</th>\n",
              "      <th>autobiographical comics</th>\n",
              "      <th>fictional crossover</th>\n",
              "      <th>encyclopedia</th>\n",
              "      <th>mashup</th>\n",
              "      <th>biopunk</th>\n",
              "      <th>popular culture</th>\n",
              "      <th>neuroscience</th>\n",
              "      <th>new york times best seller list</th>\n",
              "      <th>epic science fiction and fantasy</th>\n",
              "      <th>alien invasion</th>\n",
              "      <th>prose</th>\n",
              "      <th>pastiche</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12836</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12837</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12838</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12839</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12840</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12841 rows × 227 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       roman   satire  children's literature  ...  alien invasion  prose  pastiche\n",
              "0           1       1                      1  ...               0      0         0\n",
              "1           0       1                      0  ...               0      0         0\n",
              "2           0       0                      0  ...               0      0         0\n",
              "3           0       0                      0  ...               0      0         0\n",
              "4           1       0                      0  ...               0      0         0\n",
              "...       ...     ...                    ...  ...             ...    ...       ...\n",
              "12836       0       0                      0  ...               0      0         0\n",
              "12837       0       0                      0  ...               0      0         0\n",
              "12838       0       0                      0  ...               0      0         0\n",
              "12839       0       0                      0  ...               0      0         0\n",
              "12840       0       0                      0  ...               0      0         0\n",
              "\n",
              "[12841 rows x 227 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_CgvEkARp7Dz",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8p08_5W-p7D0"
      },
      "source": [
        "## Split the words##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xtaj1BWMqWKE",
        "outputId": "1dd10a83-37fe-498f-eec3-e12fd123ce84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g4kaZS71p7D1",
        "outputId": "ee19835b-3576-43da-d800-f6fa4828a46f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "#def words_process(new_file):\n",
        "book_summaries = new_file['summary']\n",
        "summary_list = [summary for summary in book_summaries]\n",
        "summary_num = len(summary_list)\n",
        "#summaries = ''.join(summary_list)\n",
        "print(\"the total number of books: {}\\n\".format(summary_num))\n",
        "\n",
        "all_docs = []\n",
        "i_index = 0\n",
        "for doc in summary_list:\n",
        "    # Tokenize the string into words\n",
        "    tokens = word_tokenize(doc)\n",
        "    # Remove non-alphabetic tokens, such as punctuation\n",
        "    words = [word.lower() for word in tokens if word.isalpha()]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if not word in stop_words]\n",
        "    if len(words) >= 9:\n",
        "      all_docs.append(words)\n",
        "    else:\n",
        "      print(i_index)\n",
        "      one_hot = one_hot.drop(i_index, axis=0)\n",
        "      print(one_hot.shape)\n",
        "    i_index += 1\n",
        "    #return all_docs, one_hot_delt\n",
        "\n",
        "one_hot = one_hot.reset_index(drop=True)\n",
        "# all_docs = all_docs[: len(all_docs)]\n",
        "\n",
        "len(all_docs)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the total number of books: 12841\n",
            "\n",
            "1301\n",
            "(12840, 227)\n",
            "1708\n",
            "(12839, 227)\n",
            "2487\n",
            "(12838, 227)\n",
            "2553\n",
            "(12837, 227)\n",
            "2898\n",
            "(12836, 227)\n",
            "3243\n",
            "(12835, 227)\n",
            "3382\n",
            "(12834, 227)\n",
            "3561\n",
            "(12833, 227)\n",
            "4383\n",
            "(12832, 227)\n",
            "4663\n",
            "(12831, 227)\n",
            "4747\n",
            "(12830, 227)\n",
            "4912\n",
            "(12829, 227)\n",
            "4992\n",
            "(12828, 227)\n",
            "5317\n",
            "(12827, 227)\n",
            "5382\n",
            "(12826, 227)\n",
            "5440\n",
            "(12825, 227)\n",
            "5442\n",
            "(12824, 227)\n",
            "5550\n",
            "(12823, 227)\n",
            "5576\n",
            "(12822, 227)\n",
            "5581\n",
            "(12821, 227)\n",
            "5585\n",
            "(12820, 227)\n",
            "6609\n",
            "(12819, 227)\n",
            "7354\n",
            "(12818, 227)\n",
            "7372\n",
            "(12817, 227)\n",
            "7600\n",
            "(12816, 227)\n",
            "7805\n",
            "(12815, 227)\n",
            "8059\n",
            "(12814, 227)\n",
            "8104\n",
            "(12813, 227)\n",
            "8372\n",
            "(12812, 227)\n",
            "8386\n",
            "(12811, 227)\n",
            "8542\n",
            "(12810, 227)\n",
            "8543\n",
            "(12809, 227)\n",
            "8544\n",
            "(12808, 227)\n",
            "8545\n",
            "(12807, 227)\n",
            "8546\n",
            "(12806, 227)\n",
            "8547\n",
            "(12805, 227)\n",
            "8548\n",
            "(12804, 227)\n",
            "8549\n",
            "(12803, 227)\n",
            "8550\n",
            "(12802, 227)\n",
            "8551\n",
            "(12801, 227)\n",
            "8562\n",
            "(12800, 227)\n",
            "8563\n",
            "(12799, 227)\n",
            "8769\n",
            "(12798, 227)\n",
            "8770\n",
            "(12797, 227)\n",
            "8771\n",
            "(12796, 227)\n",
            "8772\n",
            "(12795, 227)\n",
            "8785\n",
            "(12794, 227)\n",
            "8821\n",
            "(12793, 227)\n",
            "9081\n",
            "(12792, 227)\n",
            "9089\n",
            "(12791, 227)\n",
            "9118\n",
            "(12790, 227)\n",
            "9128\n",
            "(12789, 227)\n",
            "9157\n",
            "(12788, 227)\n",
            "9159\n",
            "(12787, 227)\n",
            "9160\n",
            "(12786, 227)\n",
            "9244\n",
            "(12785, 227)\n",
            "9256\n",
            "(12784, 227)\n",
            "9347\n",
            "(12783, 227)\n",
            "9552\n",
            "(12782, 227)\n",
            "9656\n",
            "(12781, 227)\n",
            "9719\n",
            "(12780, 227)\n",
            "9733\n",
            "(12779, 227)\n",
            "9814\n",
            "(12778, 227)\n",
            "9825\n",
            "(12777, 227)\n",
            "9832\n",
            "(12776, 227)\n",
            "9834\n",
            "(12775, 227)\n",
            "9849\n",
            "(12774, 227)\n",
            "9856\n",
            "(12773, 227)\n",
            "9989\n",
            "(12772, 227)\n",
            "9991\n",
            "(12771, 227)\n",
            "9994\n",
            "(12770, 227)\n",
            "10008\n",
            "(12769, 227)\n",
            "10013\n",
            "(12768, 227)\n",
            "10014\n",
            "(12767, 227)\n",
            "10022\n",
            "(12766, 227)\n",
            "10026\n",
            "(12765, 227)\n",
            "10032\n",
            "(12764, 227)\n",
            "10040\n",
            "(12763, 227)\n",
            "10060\n",
            "(12762, 227)\n",
            "10065\n",
            "(12761, 227)\n",
            "10072\n",
            "(12760, 227)\n",
            "10087\n",
            "(12759, 227)\n",
            "10101\n",
            "(12758, 227)\n",
            "10105\n",
            "(12757, 227)\n",
            "10113\n",
            "(12756, 227)\n",
            "10126\n",
            "(12755, 227)\n",
            "10217\n",
            "(12754, 227)\n",
            "10239\n",
            "(12753, 227)\n",
            "10246\n",
            "(12752, 227)\n",
            "10254\n",
            "(12751, 227)\n",
            "10263\n",
            "(12750, 227)\n",
            "10543\n",
            "(12749, 227)\n",
            "10563\n",
            "(12748, 227)\n",
            "10590\n",
            "(12747, 227)\n",
            "10755\n",
            "(12746, 227)\n",
            "11137\n",
            "(12745, 227)\n",
            "11196\n",
            "(12744, 227)\n",
            "11319\n",
            "(12743, 227)\n",
            "11512\n",
            "(12742, 227)\n",
            "12162\n",
            "(12741, 227)\n",
            "12293\n",
            "(12740, 227)\n",
            "12359\n",
            "(12739, 227)\n",
            "12677\n",
            "(12738, 227)\n",
            "12678\n",
            "(12737, 227)\n",
            "12826\n",
            "(12736, 227)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12736"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUaD7A96dFTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# one_hot = one_hot.drop(len(all_docs)-1)\n",
        "# one_hot.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cFlg1asUnghh",
        "outputId": "21e42204-9946-4637-d1e4-48dc7a60b4fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        }
      },
      "source": [
        "l1 = []\n",
        "for i in range(len(all_docs)):\n",
        "  l1.append(len(all_docs[i]))\n",
        "#print(l1.index(6))\n",
        "len_counts = Counter(l1)\n",
        "df_len = pd.DataFrame(len_counts.items(), columns=['len', '#books'])\n",
        "df_len = df_len.sort_values(by = 'len', ascending = True)\n",
        "#print(min(l1))\n",
        "df_len.head(17)\n",
        "\n",
        "#one_hot = one_hot.reset_index(drop=True)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>len</th>\n",
              "      <th>#books</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>928</th>\n",
              "      <td>9</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>885</th>\n",
              "      <td>10</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>599</th>\n",
              "      <td>11</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>733</th>\n",
              "      <td>12</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>638</th>\n",
              "      <td>13</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>393</th>\n",
              "      <td>14</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>824</th>\n",
              "      <td>15</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>746</th>\n",
              "      <td>16</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797</th>\n",
              "      <td>17</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>727</th>\n",
              "      <td>18</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>769</th>\n",
              "      <td>19</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>418</th>\n",
              "      <td>20</td>\n",
              "      <td>54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>855</th>\n",
              "      <td>21</td>\n",
              "      <td>55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>814</th>\n",
              "      <td>22</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>716</th>\n",
              "      <td>23</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>559</th>\n",
              "      <td>24</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>462</th>\n",
              "      <td>25</td>\n",
              "      <td>51</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     len  #books\n",
              "928    9      18\n",
              "885   10      31\n",
              "599   11      31\n",
              "733   12      31\n",
              "638   13      32\n",
              "393   14      33\n",
              "824   15      35\n",
              "746   16      27\n",
              "797   17      34\n",
              "727   18      27\n",
              "769   19      37\n",
              "418   20      54\n",
              "855   21      55\n",
              "814   22      44\n",
              "716   23      37\n",
              "559   24      38\n",
              "462   25      51"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDAnGolUQwpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# all_docs[10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kGpKgbGwj0wU",
        "colab": {}
      },
      "source": [
        "# print(len(all_docs))\n",
        "\n",
        "\n",
        "# one_hot.shape\n",
        "\n",
        "# l3 = []\n",
        "# for doc in all_docs:\n",
        "#     l3.append(len(doc))\n",
        "# print(l3[10:20])\n",
        "# sum(l3[10:20])\n",
        "\n",
        "# list1 = ['nine',\n",
        "#  'years',\n",
        "#  'emperor',\n",
        "#  'paul',\n",
        "#  'walked',\n",
        "#  'desert',\n",
        "#  'blind',\n",
        "#  'ecological',\n",
        "#  'transformation',\n",
        "#  'dune',\n",
        "#  'reached',\n",
        "#  'point',\n",
        "#  'fremen',\n",
        "#  'living',\n",
        "#  'without',\n",
        "#  'stillsuits',\n",
        "#  'less',\n",
        "#  'arid',\n",
        "#  'climate',\n",
        "#  'started',\n",
        "#  'move',\n",
        "#  'sietches',\n",
        "#  'villages',\n",
        "#  'cities',\n",
        "#  'old',\n",
        "#  'ways',\n",
        "#  'erode',\n",
        "#  'pilgrims',\n",
        "#  'arrive',\n",
        "#  'experience',\n",
        "#  'planet',\n",
        "#  'imperial',\n",
        "#  'high',\n",
        "#  'council',\n",
        "#  'lost',\n",
        "#  'political',\n",
        "#  'initiative',\n",
        "#  'powerless',\n",
        "#  'control',\n",
        "#  'jihad',\n",
        "#  'paul',\n",
        "#  'twin',\n",
        "#  'young',\n",
        "#  'children',\n",
        "#  'leto',\n",
        "#  'ii',\n",
        "#  'ghanima',\n",
        "#  'sharing',\n",
        "#  'prescience',\n",
        "#  'concluded',\n",
        "#  'guardian',\n",
        "#  'alia',\n",
        "#  'succumbed',\n",
        "#  'possession',\n",
        "#  'one',\n",
        "#  'ancestors',\n",
        "#  'fear',\n",
        "#  'similar',\n",
        "#  'fate',\n",
        "#  'awaits',\n",
        "#  'alia',\n",
        "#  'also',\n",
        "#  'realize',\n",
        "#  'terraforming',\n",
        "#  'dune',\n",
        "#  'kill',\n",
        "#  'sandworms',\n",
        "#  'thus',\n",
        "#  'destroying',\n",
        "#  'source',\n",
        "#  'spice',\n",
        "#  'leto',\n",
        "#  'also',\n",
        "#  'fears',\n",
        "#  'like',\n",
        "#  'father',\n",
        "#  'trapped',\n",
        "#  'prescience',\n",
        "#  'possessed',\n",
        "#  'persona',\n",
        "#  'grandfather',\n",
        "#  'baron',\n",
        "#  'vladimir',\n",
        "#  'harkonnen',\n",
        "#  'alia',\n",
        "#  'fears',\n",
        "#  'mother',\n",
        "#  'return',\n",
        "#  'arrakis',\n",
        "#  'recognize',\n",
        "#  'abomination',\n",
        "#  'new',\n",
        "#  'religious',\n",
        "#  'figure',\n",
        "#  'called',\n",
        "#  'preacher',\n",
        "#  'risen',\n",
        "#  'desert',\n",
        "#  'railing',\n",
        "#  'religious',\n",
        "#  'government',\n",
        "#  'injustices',\n",
        "#  'changes',\n",
        "#  'among',\n",
        "#  'fremen',\n",
        "#  'fremen',\n",
        "#  'believe',\n",
        "#  'paul',\n",
        "#  'atreides',\n",
        "#  'fallen',\n",
        "#  'house',\n",
        "#  'corrino',\n",
        "#  'salusa',\n",
        "#  'secundus',\n",
        "#  'plots',\n",
        "#  'assassinate',\n",
        "#  'twins',\n",
        "#  'regain',\n",
        "#  'power',\n",
        "#  'lady',\n",
        "#  'jessica',\n",
        "#  'returns',\n",
        "#  'arrakis',\n",
        "#  'recognizes',\n",
        "#  'daughter',\n",
        "#  'possessed',\n",
        "#  'finds',\n",
        "#  'signs',\n",
        "#  'abomination',\n",
        "#  'twins',\n",
        "#  'plans',\n",
        "#  'thwart',\n",
        "#  'alia',\n",
        "#  'plotting',\n",
        "#  'leto',\n",
        "#  'arranges',\n",
        "#  'fremen',\n",
        "#  'leader',\n",
        "#  'stilgar',\n",
        "#  'protect',\n",
        "#  'sister',\n",
        "#  'attempt',\n",
        "#  'lives',\n",
        "#  'alia',\n",
        "#  'attempts',\n",
        "#  'assassinate',\n",
        "#  'jessica',\n",
        "#  'escapes',\n",
        "#  'desert',\n",
        "#  'duncan',\n",
        "#  'idaho',\n",
        "#  'help',\n",
        "#  'precipitating',\n",
        "#  'rebellion',\n",
        "#  'among',\n",
        "#  'fremen',\n",
        "#  'twins',\n",
        "#  'anticipate',\n",
        "#  'survive',\n",
        "#  'corrino',\n",
        "#  'assassination',\n",
        "#  'plot',\n",
        "#  'leto',\n",
        "#  'leaves',\n",
        "#  'seek',\n",
        "#  'preacher',\n",
        "#  'ghanima',\n",
        "#  'masking',\n",
        "#  'memory',\n",
        "#  'reports',\n",
        "#  'falsely',\n",
        "#  'brother',\n",
        "#  'murdered',\n",
        "#  'duncan',\n",
        "#  'jessica',\n",
        "#  'flee',\n",
        "#  'salusa',\n",
        "#  'secundus',\n",
        "#  'jessica',\n",
        "#  'begins',\n",
        "#  'mentor',\n",
        "#  'corrino',\n",
        "#  'heir',\n",
        "#  'seizes',\n",
        "#  'power',\n",
        "#  'regent',\n",
        "#  'mother',\n",
        "#  'allies',\n",
        "#  'bene',\n",
        "#  'gesserit',\n",
        "#  'promise',\n",
        "#  'marry',\n",
        "#  'ghanima',\n",
        "#  'support',\n",
        "#  'bid',\n",
        "#  'coronation',\n",
        "#  'emperor',\n",
        "#  'band',\n",
        "#  'fremen',\n",
        "#  'outlaws',\n",
        "#  'capture',\n",
        "#  'leto',\n",
        "#  'force',\n",
        "#  'undergo',\n",
        "#  'spice',\n",
        "#  'trance',\n",
        "#  'suggestion',\n",
        "#  'one',\n",
        "#  'alia',\n",
        "#  'agents',\n",
        "#  'infiltrated',\n",
        "#  'group',\n",
        "#  'visions',\n",
        "#  'show',\n",
        "#  'myriad',\n",
        "#  'possible',\n",
        "#  'futures',\n",
        "#  'humanity',\n",
        "#  'become',\n",
        "#  'extinct',\n",
        "#  'one',\n",
        "#  'humanity',\n",
        "#  'survives',\n",
        "#  'names',\n",
        "#  'future',\n",
        "#  'golden',\n",
        "#  'path',\n",
        "#  'resolves',\n",
        "#  'bring',\n",
        "#  'fruition',\n",
        "#  'escapes',\n",
        "#  'captors',\n",
        "#  'sacrifices',\n",
        "#  'humanity',\n",
        "#  'pursuit',\n",
        "#  'golden',\n",
        "#  'path',\n",
        "#  'requires',\n",
        "#  'physically',\n",
        "#  'fuse',\n",
        "#  'school',\n",
        "#  'sandtrout',\n",
        "#  'gaining',\n",
        "#  'superhuman',\n",
        "#  'strength',\n",
        "#  'travels',\n",
        "#  'across',\n",
        "#  'desert',\n",
        "#  'confronts',\n",
        "#  'preacher',\n",
        "#  'fact',\n",
        "#  'prove',\n",
        "#  'father',\n",
        "#  'paul',\n",
        "#  'atreides',\n",
        "#  'duncan',\n",
        "#  'idaho',\n",
        "#  'returns',\n",
        "#  'arrakis',\n",
        "#  'provokes',\n",
        "#  'stilgar',\n",
        "#  'killing',\n",
        "#  'stilgar',\n",
        "#  'neutrality',\n",
        "#  'untenable',\n",
        "#  'seizes',\n",
        "#  'ghanima',\n",
        "#  'flees',\n",
        "#  'alia',\n",
        "#  'recaptures',\n",
        "#  'ghanima',\n",
        "#  'arranges',\n",
        "#  'marriage',\n",
        "#  'planning',\n",
        "#  'exploit',\n",
        "#  'expected',\n",
        "#  'chaos',\n",
        "#  'ghanima',\n",
        "#  'kills',\n",
        "#  'avenge',\n",
        "#  'brother',\n",
        "#  'murder',\n",
        "#  'preacher',\n",
        "#  'leto',\n",
        "#  'return',\n",
        "#  'capital',\n",
        "#  'confront',\n",
        "#  'alia',\n",
        "#  'preacher',\n",
        "#  'murdered',\n",
        "#  'revealing',\n",
        "#  'true',\n",
        "#  'identity',\n",
        "#  'leto',\n",
        "#  'reveals',\n",
        "#  'display',\n",
        "#  'superhuman',\n",
        "#  'strength',\n",
        "#  'triggers',\n",
        "#  'return',\n",
        "#  'ghanima',\n",
        "#  'genuine',\n",
        "#  'memories',\n",
        "#  'confronts',\n",
        "#  'alia',\n",
        "#  'offers',\n",
        "#  'help',\n",
        "#  'overcome',\n",
        "#  'possession',\n",
        "#  'overwhelmed',\n",
        "#  'ancestral',\n",
        "#  'personae',\n",
        "#  'elects',\n",
        "#  'commit',\n",
        "#  'suicide',\n",
        "#  'leto',\n",
        "#  'declares',\n",
        "#  'emperor',\n",
        "#  'asserts',\n",
        "#  'control',\n",
        "#  'fremen',\n",
        "#  'enlists',\n",
        "#  'service',\n",
        "#  'delivers',\n",
        "#  'control',\n",
        "#  'corrino',\n",
        "#  'armies',\n",
        "#  'seemingly',\n",
        "#  'immortal',\n",
        "#  'omnipotent',\n",
        "#  'leto',\n",
        "#  'left',\n",
        "#  'emperor',\n",
        "#  'known',\n",
        "#  'universe',\n",
        "#  'ghanima',\n",
        "#  'side']\n",
        "# for i in range(len(list1)):\n",
        "#     if list1[i] != all_docs[10][i]:\n",
        "#         print(i)\n",
        "#         print(list1[i])\n",
        "#         print(all_docs[10][i])\n",
        "# set(all_docs[10])-set(list1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7tn7GXCqp7D4"
      },
      "source": [
        "## outliners ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aRZEVHYrp7D5"
      },
      "source": [
        "## Using a Pre-Trained Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xCtCbDx_p7D5",
        "outputId": "81f3a41a-6996-46ac-d138-ef83db6b8bad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Load word2vec model (trained on an enormous Google corpus)\n",
        "#google_vecs = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary = True)\n",
        "google_vecs = KeyedVectors.load_word2vec_format('/content/drive/My Drive/Colab Notebooks/final project/GoogleNews-vectors-negative300.bin', binary = True)\n",
        "embedding_dim_dim = google_vecs.vector_size\n",
        "\n",
        "# def words_embedding(docs):\n",
        "   \n",
        "all_words = [ word for doc in all_docs for word in doc]\n",
        "all_words_nodup = list(dict.fromkeys(all_words))\n",
        "\n",
        "def words_embedding(doc):\n",
        "    # Filter the list of vectors to include only those that Word2Vec has a vector for\n",
        "    vector_list = [google_vecs[word] for word in doc if word in google_vecs.vocab]\n",
        "    #google_vectors = np.asarray(vector_list)\n",
        "    # Create a list of the words corresponding to these vectors\n",
        "    words_filtered = [word for word in doc if word in google_vecs.vocab]\n",
        "    #Zip the words together with their vector representations\n",
        "    word_vec_zip = zip(words_filtered, vector_list)\n",
        "\n",
        "    # Cast to a dict so we can turn it into a DataFrame\n",
        "    word_vec_dict = dict(word_vec_zip)\n",
        "    word_vec = pd.DataFrame.from_dict(word_vec_dict, orient='index')\n",
        "    return word_vec\n",
        "\n",
        "word_vec  = words_embedding(all_words_nodup)\n",
        "word_vec_array = np.array(word_vec)\n",
        "word_vec.shape[0]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51242"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OPJrtUy8Qg1p",
        "colab": {}
      },
      "source": [
        "\n",
        "len(all_words)\n",
        "#word_vec_array.shape\n",
        "word_vec_array = np.insert(word_vec_array, 0, np.zeros(word_vec_array.shape[1]), 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4mxTtNdaS4zY",
        "outputId": "4f72180a-aef2-4d7a-b39c-b3febcaa8a59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "word_vec.tail()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>260</th>\n",
              "      <th>261</th>\n",
              "      <th>262</th>\n",
              "      <th>263</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>266</th>\n",
              "      <th>267</th>\n",
              "      <th>268</th>\n",
              "      <th>269</th>\n",
              "      <th>270</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "      <th>281</th>\n",
              "      <th>282</th>\n",
              "      <th>283</th>\n",
              "      <th>284</th>\n",
              "      <th>285</th>\n",
              "      <th>286</th>\n",
              "      <th>287</th>\n",
              "      <th>288</th>\n",
              "      <th>289</th>\n",
              "      <th>290</th>\n",
              "      <th>291</th>\n",
              "      <th>292</th>\n",
              "      <th>293</th>\n",
              "      <th>294</th>\n",
              "      <th>295</th>\n",
              "      <th>296</th>\n",
              "      <th>297</th>\n",
              "      <th>298</th>\n",
              "      <th>299</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>candlelit</th>\n",
              "      <td>0.039062</td>\n",
              "      <td>-0.004700</td>\n",
              "      <td>0.054932</td>\n",
              "      <td>0.410156</td>\n",
              "      <td>0.108887</td>\n",
              "      <td>-0.030273</td>\n",
              "      <td>-0.078613</td>\n",
              "      <td>-0.380859</td>\n",
              "      <td>0.090820</td>\n",
              "      <td>0.302734</td>\n",
              "      <td>-0.118652</td>\n",
              "      <td>-0.013062</td>\n",
              "      <td>0.000847</td>\n",
              "      <td>-0.039062</td>\n",
              "      <td>0.392578</td>\n",
              "      <td>0.194336</td>\n",
              "      <td>0.044434</td>\n",
              "      <td>-0.177734</td>\n",
              "      <td>0.009033</td>\n",
              "      <td>-0.186523</td>\n",
              "      <td>0.181641</td>\n",
              "      <td>0.238281</td>\n",
              "      <td>-0.164062</td>\n",
              "      <td>-0.158203</td>\n",
              "      <td>-0.014709</td>\n",
              "      <td>-0.023193</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.060303</td>\n",
              "      <td>-0.166016</td>\n",
              "      <td>-0.347656</td>\n",
              "      <td>-0.318359</td>\n",
              "      <td>-0.132812</td>\n",
              "      <td>-0.028320</td>\n",
              "      <td>-0.255859</td>\n",
              "      <td>-0.318359</td>\n",
              "      <td>-0.566406</td>\n",
              "      <td>-0.140625</td>\n",
              "      <td>-0.519531</td>\n",
              "      <td>-0.198242</td>\n",
              "      <td>-0.043945</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.237305</td>\n",
              "      <td>-0.343750</td>\n",
              "      <td>0.063965</td>\n",
              "      <td>0.019409</td>\n",
              "      <td>-0.189453</td>\n",
              "      <td>0.113770</td>\n",
              "      <td>-0.125000</td>\n",
              "      <td>0.103516</td>\n",
              "      <td>0.101074</td>\n",
              "      <td>-0.066895</td>\n",
              "      <td>0.277344</td>\n",
              "      <td>0.122559</td>\n",
              "      <td>0.050537</td>\n",
              "      <td>0.183594</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.021118</td>\n",
              "      <td>-0.014648</td>\n",
              "      <td>0.217773</td>\n",
              "      <td>-0.005768</td>\n",
              "      <td>-0.064941</td>\n",
              "      <td>0.212891</td>\n",
              "      <td>-0.129883</td>\n",
              "      <td>0.235352</td>\n",
              "      <td>-0.033936</td>\n",
              "      <td>0.048096</td>\n",
              "      <td>0.012146</td>\n",
              "      <td>-0.087402</td>\n",
              "      <td>0.108398</td>\n",
              "      <td>-0.095703</td>\n",
              "      <td>0.271484</td>\n",
              "      <td>-0.119141</td>\n",
              "      <td>-0.015625</td>\n",
              "      <td>-0.306641</td>\n",
              "      <td>0.320312</td>\n",
              "      <td>-0.332031</td>\n",
              "      <td>-0.090820</td>\n",
              "      <td>0.088867</td>\n",
              "      <td>-0.139648</td>\n",
              "      <td>-0.014587</td>\n",
              "      <td>0.455078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stillbirth</th>\n",
              "      <td>-0.251953</td>\n",
              "      <td>0.058350</td>\n",
              "      <td>-0.162109</td>\n",
              "      <td>0.396484</td>\n",
              "      <td>0.145508</td>\n",
              "      <td>0.341797</td>\n",
              "      <td>0.359375</td>\n",
              "      <td>0.212891</td>\n",
              "      <td>0.546875</td>\n",
              "      <td>0.086914</td>\n",
              "      <td>0.503906</td>\n",
              "      <td>0.032471</td>\n",
              "      <td>-0.088867</td>\n",
              "      <td>0.267578</td>\n",
              "      <td>0.170898</td>\n",
              "      <td>0.289062</td>\n",
              "      <td>0.000378</td>\n",
              "      <td>0.257812</td>\n",
              "      <td>-0.087402</td>\n",
              "      <td>-0.259766</td>\n",
              "      <td>0.113281</td>\n",
              "      <td>-0.515625</td>\n",
              "      <td>0.036377</td>\n",
              "      <td>-0.206055</td>\n",
              "      <td>-0.291016</td>\n",
              "      <td>-0.109863</td>\n",
              "      <td>0.041016</td>\n",
              "      <td>-0.041504</td>\n",
              "      <td>-0.267578</td>\n",
              "      <td>-0.457031</td>\n",
              "      <td>-0.137695</td>\n",
              "      <td>-0.124023</td>\n",
              "      <td>-0.180664</td>\n",
              "      <td>-0.562500</td>\n",
              "      <td>-0.294922</td>\n",
              "      <td>-0.181641</td>\n",
              "      <td>-0.201172</td>\n",
              "      <td>0.013733</td>\n",
              "      <td>-0.386719</td>\n",
              "      <td>0.168945</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.302734</td>\n",
              "      <td>-0.060547</td>\n",
              "      <td>0.137695</td>\n",
              "      <td>0.265625</td>\n",
              "      <td>-0.396484</td>\n",
              "      <td>0.142578</td>\n",
              "      <td>-0.135742</td>\n",
              "      <td>-0.080078</td>\n",
              "      <td>-0.285156</td>\n",
              "      <td>-0.215820</td>\n",
              "      <td>-0.188477</td>\n",
              "      <td>0.291016</td>\n",
              "      <td>-0.380859</td>\n",
              "      <td>0.198242</td>\n",
              "      <td>-0.069336</td>\n",
              "      <td>0.157227</td>\n",
              "      <td>0.144531</td>\n",
              "      <td>0.431641</td>\n",
              "      <td>0.019531</td>\n",
              "      <td>-0.212891</td>\n",
              "      <td>-0.038818</td>\n",
              "      <td>-0.330078</td>\n",
              "      <td>0.105469</td>\n",
              "      <td>-0.032715</td>\n",
              "      <td>-0.152344</td>\n",
              "      <td>-0.261719</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>-0.207031</td>\n",
              "      <td>-0.070312</td>\n",
              "      <td>0.078613</td>\n",
              "      <td>0.369141</td>\n",
              "      <td>-0.341797</td>\n",
              "      <td>-0.098145</td>\n",
              "      <td>0.063477</td>\n",
              "      <td>-0.214844</td>\n",
              "      <td>0.190430</td>\n",
              "      <td>-0.059570</td>\n",
              "      <td>0.217773</td>\n",
              "      <td>-0.074707</td>\n",
              "      <td>0.041504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bleaches</th>\n",
              "      <td>0.015381</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>-0.021729</td>\n",
              "      <td>-0.189453</td>\n",
              "      <td>-0.347656</td>\n",
              "      <td>0.283203</td>\n",
              "      <td>-0.072754</td>\n",
              "      <td>-0.429688</td>\n",
              "      <td>-0.027100</td>\n",
              "      <td>0.172852</td>\n",
              "      <td>-0.112305</td>\n",
              "      <td>-0.402344</td>\n",
              "      <td>0.021362</td>\n",
              "      <td>0.339844</td>\n",
              "      <td>0.040283</td>\n",
              "      <td>0.259766</td>\n",
              "      <td>-0.052246</td>\n",
              "      <td>0.291016</td>\n",
              "      <td>0.115234</td>\n",
              "      <td>-0.138672</td>\n",
              "      <td>0.175781</td>\n",
              "      <td>-0.162109</td>\n",
              "      <td>0.126953</td>\n",
              "      <td>-0.083496</td>\n",
              "      <td>-0.304688</td>\n",
              "      <td>-0.170898</td>\n",
              "      <td>0.066895</td>\n",
              "      <td>0.179688</td>\n",
              "      <td>-0.071289</td>\n",
              "      <td>0.042236</td>\n",
              "      <td>-0.312500</td>\n",
              "      <td>0.115234</td>\n",
              "      <td>-0.225586</td>\n",
              "      <td>-0.041992</td>\n",
              "      <td>-0.150391</td>\n",
              "      <td>-0.312500</td>\n",
              "      <td>-0.050049</td>\n",
              "      <td>0.064941</td>\n",
              "      <td>-0.294922</td>\n",
              "      <td>-0.041748</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.064453</td>\n",
              "      <td>-0.324219</td>\n",
              "      <td>-0.328125</td>\n",
              "      <td>-0.179688</td>\n",
              "      <td>0.335938</td>\n",
              "      <td>0.145508</td>\n",
              "      <td>-0.083984</td>\n",
              "      <td>-0.217773</td>\n",
              "      <td>0.142578</td>\n",
              "      <td>-0.224609</td>\n",
              "      <td>-0.067871</td>\n",
              "      <td>-0.019165</td>\n",
              "      <td>-0.161133</td>\n",
              "      <td>0.188477</td>\n",
              "      <td>0.365234</td>\n",
              "      <td>-0.031006</td>\n",
              "      <td>-0.168945</td>\n",
              "      <td>0.120117</td>\n",
              "      <td>-0.040039</td>\n",
              "      <td>-0.053711</td>\n",
              "      <td>0.079590</td>\n",
              "      <td>0.055176</td>\n",
              "      <td>0.116699</td>\n",
              "      <td>-0.020264</td>\n",
              "      <td>-0.033203</td>\n",
              "      <td>-0.055176</td>\n",
              "      <td>0.267578</td>\n",
              "      <td>0.015564</td>\n",
              "      <td>-0.234375</td>\n",
              "      <td>0.080566</td>\n",
              "      <td>-0.017334</td>\n",
              "      <td>-0.012695</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.108398</td>\n",
              "      <td>-0.292969</td>\n",
              "      <td>0.172852</td>\n",
              "      <td>0.181641</td>\n",
              "      <td>-0.184570</td>\n",
              "      <td>0.453125</td>\n",
              "      <td>0.148438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rapping</th>\n",
              "      <td>0.017578</td>\n",
              "      <td>0.074219</td>\n",
              "      <td>-0.031738</td>\n",
              "      <td>0.034912</td>\n",
              "      <td>0.163086</td>\n",
              "      <td>0.077148</td>\n",
              "      <td>-0.316406</td>\n",
              "      <td>-0.048584</td>\n",
              "      <td>-0.031006</td>\n",
              "      <td>0.110840</td>\n",
              "      <td>0.107422</td>\n",
              "      <td>-0.109375</td>\n",
              "      <td>0.104980</td>\n",
              "      <td>0.189453</td>\n",
              "      <td>-0.433594</td>\n",
              "      <td>-0.077148</td>\n",
              "      <td>-0.039795</td>\n",
              "      <td>-0.048096</td>\n",
              "      <td>-0.110840</td>\n",
              "      <td>-0.202148</td>\n",
              "      <td>-0.076172</td>\n",
              "      <td>0.275391</td>\n",
              "      <td>-0.106934</td>\n",
              "      <td>-0.083984</td>\n",
              "      <td>-0.367188</td>\n",
              "      <td>-0.047607</td>\n",
              "      <td>0.041260</td>\n",
              "      <td>-0.048096</td>\n",
              "      <td>0.238281</td>\n",
              "      <td>0.191406</td>\n",
              "      <td>-0.094238</td>\n",
              "      <td>0.057861</td>\n",
              "      <td>0.113770</td>\n",
              "      <td>0.156250</td>\n",
              "      <td>0.101074</td>\n",
              "      <td>-0.001442</td>\n",
              "      <td>0.183594</td>\n",
              "      <td>-0.065918</td>\n",
              "      <td>0.125977</td>\n",
              "      <td>0.067383</td>\n",
              "      <td>...</td>\n",
              "      <td>0.080566</td>\n",
              "      <td>0.074219</td>\n",
              "      <td>0.070312</td>\n",
              "      <td>-0.006714</td>\n",
              "      <td>-0.156250</td>\n",
              "      <td>-0.051025</td>\n",
              "      <td>-0.038574</td>\n",
              "      <td>-0.067871</td>\n",
              "      <td>-0.320312</td>\n",
              "      <td>0.210938</td>\n",
              "      <td>-0.053223</td>\n",
              "      <td>0.058350</td>\n",
              "      <td>0.306641</td>\n",
              "      <td>-0.048340</td>\n",
              "      <td>0.149414</td>\n",
              "      <td>-0.143555</td>\n",
              "      <td>-0.320312</td>\n",
              "      <td>-0.046631</td>\n",
              "      <td>-0.001335</td>\n",
              "      <td>-0.107910</td>\n",
              "      <td>0.095703</td>\n",
              "      <td>-0.049805</td>\n",
              "      <td>0.209961</td>\n",
              "      <td>0.059326</td>\n",
              "      <td>0.084961</td>\n",
              "      <td>-0.302734</td>\n",
              "      <td>0.112793</td>\n",
              "      <td>0.167969</td>\n",
              "      <td>0.312500</td>\n",
              "      <td>0.009094</td>\n",
              "      <td>0.057861</td>\n",
              "      <td>0.155273</td>\n",
              "      <td>-0.105469</td>\n",
              "      <td>-0.347656</td>\n",
              "      <td>-0.166992</td>\n",
              "      <td>-0.097656</td>\n",
              "      <td>0.214844</td>\n",
              "      <td>-0.419922</td>\n",
              "      <td>-0.016602</td>\n",
              "      <td>0.183594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>attesting</th>\n",
              "      <td>-0.343750</td>\n",
              "      <td>-0.145508</td>\n",
              "      <td>-0.150391</td>\n",
              "      <td>-0.127930</td>\n",
              "      <td>0.137695</td>\n",
              "      <td>0.180664</td>\n",
              "      <td>0.144531</td>\n",
              "      <td>-0.355469</td>\n",
              "      <td>0.056152</td>\n",
              "      <td>0.425781</td>\n",
              "      <td>0.009277</td>\n",
              "      <td>-0.539062</td>\n",
              "      <td>-0.198242</td>\n",
              "      <td>0.472656</td>\n",
              "      <td>0.025513</td>\n",
              "      <td>0.082520</td>\n",
              "      <td>0.507812</td>\n",
              "      <td>0.480469</td>\n",
              "      <td>-0.466797</td>\n",
              "      <td>-0.132812</td>\n",
              "      <td>-0.246094</td>\n",
              "      <td>-0.131836</td>\n",
              "      <td>-0.410156</td>\n",
              "      <td>-0.137695</td>\n",
              "      <td>-0.091309</td>\n",
              "      <td>-0.460938</td>\n",
              "      <td>-0.104980</td>\n",
              "      <td>0.143555</td>\n",
              "      <td>0.001656</td>\n",
              "      <td>-0.026855</td>\n",
              "      <td>0.120605</td>\n",
              "      <td>-0.435547</td>\n",
              "      <td>0.124023</td>\n",
              "      <td>0.578125</td>\n",
              "      <td>-0.078125</td>\n",
              "      <td>0.112793</td>\n",
              "      <td>0.129883</td>\n",
              "      <td>0.043701</td>\n",
              "      <td>-0.246094</td>\n",
              "      <td>-0.414062</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.039551</td>\n",
              "      <td>0.306641</td>\n",
              "      <td>-0.234375</td>\n",
              "      <td>-0.102051</td>\n",
              "      <td>0.157227</td>\n",
              "      <td>0.102539</td>\n",
              "      <td>0.060791</td>\n",
              "      <td>0.068848</td>\n",
              "      <td>0.227539</td>\n",
              "      <td>-0.257812</td>\n",
              "      <td>0.030396</td>\n",
              "      <td>-0.044678</td>\n",
              "      <td>-0.095215</td>\n",
              "      <td>-0.186523</td>\n",
              "      <td>-0.052734</td>\n",
              "      <td>-0.245117</td>\n",
              "      <td>-0.349609</td>\n",
              "      <td>-0.107422</td>\n",
              "      <td>-0.022949</td>\n",
              "      <td>-0.031982</td>\n",
              "      <td>-0.186523</td>\n",
              "      <td>-0.175781</td>\n",
              "      <td>0.175781</td>\n",
              "      <td>0.114746</td>\n",
              "      <td>-0.054443</td>\n",
              "      <td>-0.007080</td>\n",
              "      <td>0.158203</td>\n",
              "      <td>0.000931</td>\n",
              "      <td>0.062988</td>\n",
              "      <td>-0.011169</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.257812</td>\n",
              "      <td>-0.271484</td>\n",
              "      <td>0.173828</td>\n",
              "      <td>0.117188</td>\n",
              "      <td>-0.186523</td>\n",
              "      <td>-0.316406</td>\n",
              "      <td>-0.302734</td>\n",
              "      <td>-0.085449</td>\n",
              "      <td>0.253906</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 300 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0         1         2    ...       297       298       299\n",
              "candlelit   0.039062 -0.004700  0.054932  ... -0.139648 -0.014587  0.455078\n",
              "stillbirth -0.251953  0.058350 -0.162109  ...  0.217773 -0.074707  0.041504\n",
              "bleaches    0.015381  0.250000 -0.021729  ... -0.184570  0.453125  0.148438\n",
              "rapping     0.017578  0.074219 -0.031738  ... -0.419922 -0.016602  0.183594\n",
              "attesting  -0.343750 -0.145508 -0.150391  ... -0.302734 -0.085449  0.253906\n",
              "\n",
              "[5 rows x 300 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C76sCrv1Qwpw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8d4f00cb-8548-4f33-cd8d-7d438cbbd9f7"
      },
      "source": [
        "all_docs[-1]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['makar',\n",
              " 'devushkin',\n",
              " 'varvara',\n",
              " 'dobroselova',\n",
              " 'second',\n",
              " 'cousins',\n",
              " 'live',\n",
              " 'across',\n",
              " 'street',\n",
              " 'terrible',\n",
              " 'apartments',\n",
              " 'devushkin',\n",
              " 'example',\n",
              " 'merely',\n",
              " 'section',\n",
              " 'kitchen',\n",
              " 'lives',\n",
              " 'several',\n",
              " 'tenants',\n",
              " 'gorshkovs',\n",
              " 'whose',\n",
              " 'son',\n",
              " 'dies',\n",
              " 'groans',\n",
              " 'agonizing',\n",
              " 'hunger',\n",
              " 'almost',\n",
              " 'entire',\n",
              " 'story',\n",
              " 'devushkin',\n",
              " 'dobroselova',\n",
              " 'exchange',\n",
              " 'letters',\n",
              " 'attesting',\n",
              " 'terrible',\n",
              " 'living',\n",
              " 'conditions',\n",
              " 'former',\n",
              " 'frequently',\n",
              " 'squanders',\n",
              " 'money',\n",
              " 'gifts',\n",
              " 'latter',\n",
              " 'reader',\n",
              " 'progressively',\n",
              " 'learns',\n",
              " 'history',\n",
              " 'dobroselova',\n",
              " 'originally',\n",
              " 'lived',\n",
              " 'country',\n",
              " 'moved',\n",
              " 'petersburg',\n",
              " 'hates',\n",
              " 'father',\n",
              " 'lost',\n",
              " 'job',\n",
              " 'father',\n",
              " 'violent',\n",
              " 'losing',\n",
              " 'job',\n",
              " 'mother',\n",
              " 'became',\n",
              " 'severely',\n",
              " 'depressed',\n",
              " 'father',\n",
              " 'dies',\n",
              " 'move',\n",
              " 'anna',\n",
              " 'fyodorovna',\n",
              " 'landlady',\n",
              " 'previously',\n",
              " 'cruel',\n",
              " 'least',\n",
              " 'pretends',\n",
              " 'feel',\n",
              " 'sympathy',\n",
              " 'situation',\n",
              " 'dobroselova',\n",
              " 'tutored',\n",
              " 'poor',\n",
              " 'student',\n",
              " 'named',\n",
              " 'pokrovsky',\n",
              " 'whose',\n",
              " 'drunken',\n",
              " 'father',\n",
              " 'occasionally',\n",
              " 'visits',\n",
              " 'eventually',\n",
              " 'falls',\n",
              " 'love',\n",
              " 'pokrovsky',\n",
              " 'struggles',\n",
              " 'save',\n",
              " 'measly',\n",
              " 'amount',\n",
              " 'money',\n",
              " 'purchase',\n",
              " 'complete',\n",
              " 'works',\n",
              " 'pushkin',\n",
              " 'market',\n",
              " 'birthday',\n",
              " 'present',\n",
              " 'allows',\n",
              " 'father',\n",
              " 'give',\n",
              " 'books',\n",
              " 'instead',\n",
              " 'claiming',\n",
              " 'knowing',\n",
              " 'received',\n",
              " 'books',\n",
              " 'enough',\n",
              " 'happiness',\n",
              " 'pokrovsky',\n",
              " 'falls',\n",
              " 'ill',\n",
              " 'soon',\n",
              " 'dying',\n",
              " 'wish',\n",
              " 'see',\n",
              " 'sun',\n",
              " 'world',\n",
              " 'outside',\n",
              " 'dobroselova',\n",
              " 'obliges',\n",
              " 'opening',\n",
              " 'blinds',\n",
              " 'reveal',\n",
              " 'grey',\n",
              " 'clouds',\n",
              " 'dirty',\n",
              " 'rain',\n",
              " 'response',\n",
              " 'pokrovsky',\n",
              " 'shakes',\n",
              " 'head',\n",
              " 'passes',\n",
              " 'away',\n",
              " 'dobroselova',\n",
              " 'mother',\n",
              " 'dies',\n",
              " 'shortly',\n",
              " 'afterwards',\n",
              " 'dobroselova',\n",
              " 'left',\n",
              " 'care',\n",
              " 'anna',\n",
              " 'time',\n",
              " 'abuse',\n",
              " 'becomes',\n",
              " 'much',\n",
              " 'goes',\n",
              " 'live',\n",
              " 'fedora',\n",
              " 'across',\n",
              " 'street',\n",
              " 'devushkin',\n",
              " 'works',\n",
              " 'lowly',\n",
              " 'copyist',\n",
              " 'frequently',\n",
              " 'belittled',\n",
              " 'picked',\n",
              " 'work',\n",
              " 'colleagues',\n",
              " 'clothing',\n",
              " 'worn',\n",
              " 'dirty',\n",
              " 'living',\n",
              " 'conditions',\n",
              " 'perhaps',\n",
              " 'worse',\n",
              " 'dobroselova',\n",
              " 'considers',\n",
              " 'rat',\n",
              " 'society',\n",
              " 'dobroselova',\n",
              " 'exchange',\n",
              " 'letters',\n",
              " 'occasional',\n",
              " 'visits',\n",
              " 'never',\n",
              " 'detailed',\n",
              " 'eventually',\n",
              " 'also',\n",
              " 'begin',\n",
              " 'exchange',\n",
              " 'books',\n",
              " 'devushkin',\n",
              " 'becomes',\n",
              " 'offended',\n",
              " 'sends',\n",
              " 'copy',\n",
              " 'overcoat',\n",
              " 'finds',\n",
              " 'main',\n",
              " 'character',\n",
              " 'living',\n",
              " 'life',\n",
              " 'similar',\n",
              " 'dobroselova',\n",
              " 'considers',\n",
              " 'moving',\n",
              " 'another',\n",
              " 'part',\n",
              " 'city',\n",
              " 'work',\n",
              " 'governess',\n",
              " 'money',\n",
              " 'risks',\n",
              " 'evicted',\n",
              " 'devushkin',\n",
              " 'stroke',\n",
              " 'luck',\n",
              " 'boss',\n",
              " 'takes',\n",
              " 'pity',\n",
              " 'gives',\n",
              " 'rubles',\n",
              " 'buy',\n",
              " 'new',\n",
              " 'clothes',\n",
              " 'devushkin',\n",
              " 'pays',\n",
              " 'debts',\n",
              " 'sends',\n",
              " 'dobroselova',\n",
              " 'sends',\n",
              " 'rubles',\n",
              " 'back',\n",
              " 'need',\n",
              " 'future',\n",
              " 'looks',\n",
              " 'bright',\n",
              " 'start',\n",
              " 'save',\n",
              " 'money',\n",
              " 'may',\n",
              " 'possible',\n",
              " 'move',\n",
              " 'together',\n",
              " 'devushkin',\n",
              " 'finds',\n",
              " 'liked',\n",
              " 'even',\n",
              " 'writer',\n",
              " 'ratazyayev',\n",
              " 'using',\n",
              " 'character',\n",
              " 'one',\n",
              " 'stories',\n",
              " 'sad',\n",
              " 'condition',\n",
              " 'even',\n",
              " 'gorshkovs',\n",
              " 'come',\n",
              " 'across',\n",
              " 'money',\n",
              " 'father',\n",
              " 'case',\n",
              " 'court',\n",
              " 'generous',\n",
              " 'settlement',\n",
              " 'seem',\n",
              " 'destined',\n",
              " 'perfectly',\n",
              " 'happy',\n",
              " 'father',\n",
              " 'dies',\n",
              " 'leaving',\n",
              " 'family',\n",
              " 'shambles',\n",
              " 'despite',\n",
              " 'money',\n",
              " 'soon',\n",
              " 'dobroselova',\n",
              " 'announces',\n",
              " 'bykov',\n",
              " 'dealings',\n",
              " 'anna',\n",
              " 'fyodorovna',\n",
              " 'pokrovsky',\n",
              " 'father',\n",
              " 'proposed',\n",
              " 'decides',\n",
              " 'leave',\n",
              " 'last',\n",
              " 'letters',\n",
              " 'attest',\n",
              " 'slowly',\n",
              " 'becoming',\n",
              " 'used',\n",
              " 'new',\n",
              " 'money',\n",
              " 'devushkin',\n",
              " 'find',\n",
              " 'linen',\n",
              " 'begins',\n",
              " 'talk',\n",
              " 'various',\n",
              " 'luxuries',\n",
              " 'leaving',\n",
              " 'alone',\n",
              " 'end',\n",
              " 'despite',\n",
              " 'fact',\n",
              " 'lot',\n",
              " 'improving',\n",
              " 'story',\n",
              " 'ends',\n",
              " 'final',\n",
              " 'letter',\n",
              " 'desperate',\n",
              " 'plea',\n",
              " 'come',\n",
              " 'back',\n",
              " 'least',\n",
              " 'write',\n",
              " 'new',\n",
              " 'life']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TyVlCnwUQGFh",
        "colab": {}
      },
      "source": [
        "# word_vec.to_csv(r'./wor_vec.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zAE7B160QJHI",
        "colab": {}
      },
      "source": [
        "#word_vec = pd.read_csv(r\"/content/drive/My Drive/Colab Notebooks/final project/wor_vec.csv\", header=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1zaxCSpzp7D8"
      },
      "source": [
        "# Generate the integer vectors of all summaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VjTTRo8pp7D8",
        "colab": {}
      },
      "source": [
        "word_index = pd.DataFrame(index = word_vec.index)\n",
        "wordlen_list = range(1, word_vec.shape[0]+1)\n",
        "word_index['index'] = wordlen_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDlPBL19Qwp9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0b216451-23e8-40ca-f1a1-7f77fa737d00"
      },
      "source": [
        "wordlen_list[-1]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51242"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Aoi4c-sTp7D-",
        "outputId": "fe730373-4b5a-4a39-f96d-4b08a9362995",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "def words_index(all_docs):\n",
        "    all_words_index = []\n",
        "    for doc in all_docs:\n",
        "        inds = []\n",
        "        for word in doc:\n",
        "            try:\n",
        "                inds.append(word_index.at[word, 'index'])\n",
        "            except: \n",
        "                continue\n",
        "            \n",
        "        all_words_index.append(inds)\n",
        "    return all_words_index\n",
        "\n",
        "all_words_index = words_index(all_docs)\n",
        "all_words_index[-1]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[40288,\n",
              " 928,\n",
              " 6584,\n",
              " 1128,\n",
              " 1535,\n",
              " 4158,\n",
              " 5200,\n",
              " 18966,\n",
              " 6161,\n",
              " 4593,\n",
              " 2860,\n",
              " 3861,\n",
              " 92,\n",
              " 1209,\n",
              " 9490,\n",
              " 532,\n",
              " 662,\n",
              " 16,\n",
              " 24658,\n",
              " 7346,\n",
              " 11291,\n",
              " 2273,\n",
              " 1633,\n",
              " 582,\n",
              " 2320,\n",
              " 2667,\n",
              " 51242,\n",
              " 5200,\n",
              " 324,\n",
              " 1229,\n",
              " 570,\n",
              " 6601,\n",
              " 18113,\n",
              " 2148,\n",
              " 7203,\n",
              " 1957,\n",
              " 3389,\n",
              " 19596,\n",
              " 1077,\n",
              " 120,\n",
              " 6054,\n",
              " 4064,\n",
              " 220,\n",
              " 4019,\n",
              " 5284,\n",
              " 9622,\n",
              " 811,\n",
              " 1274,\n",
              " 445,\n",
              " 811,\n",
              " 93,\n",
              " 2018,\n",
              " 445,\n",
              " 1311,\n",
              " 2553,\n",
              " 2917,\n",
              " 10295,\n",
              " 811,\n",
              " 16,\n",
              " 1815,\n",
              " 15571,\n",
              " 9379,\n",
              " 6100,\n",
              " 6480,\n",
              " 218,\n",
              " 3051,\n",
              " 507,\n",
              " 4355,\n",
              " 723,\n",
              " 19400,\n",
              " 140,\n",
              " 2960,\n",
              " 82,\n",
              " 532,\n",
              " 29,\n",
              " 811,\n",
              " 2459,\n",
              " 3533,\n",
              " 297,\n",
              " 824,\n",
              " 1283,\n",
              " 4445,\n",
              " 921,\n",
              " 32614,\n",
              " 3892,\n",
              " 2148,\n",
              " 9666,\n",
              " 2274,\n",
              " 1761,\n",
              " 6181,\n",
              " 3601,\n",
              " 997,\n",
              " 3378,\n",
              " 811,\n",
              " 945,\n",
              " 1433,\n",
              " 1477,\n",
              " 2625,\n",
              " 2774,\n",
              " 2437,\n",
              " 1433,\n",
              " 756,\n",
              " 6605,\n",
              " 824,\n",
              " 2513,\n",
              " 248,\n",
              " 1193,\n",
              " 2137,\n",
              " 852,\n",
              " 4866,\n",
              " 1039,\n",
              " 571,\n",
              " 21369,\n",
              " 980,\n",
              " 3186,\n",
              " 2196,\n",
              " 3541,\n",
              " 1228,\n",
              " 6968,\n",
              " 3791,\n",
              " 5136,\n",
              " 1465,\n",
              " 2471,\n",
              " 69,\n",
              " 1311,\n",
              " 16,\n",
              " 1093,\n",
              " 2565,\n",
              " 916,\n",
              " 174,\n",
              " 15571,\n",
              " 920,\n",
              " 113,\n",
              " 105,\n",
              " 1326,\n",
              " 271,\n",
              " 1128,\n",
              " 30074,\n",
              " 1535,\n",
              " 4158,\n",
              " 1761,\n",
              " 27167,\n",
              " 25509,\n",
              " 6601,\n",
              " 23190,\n",
              " 10093,\n",
              " 88,\n",
              " 11668,\n",
              " 5054,\n",
              " 7896,\n",
              " 1228,\n",
              " 324,\n",
              " 1229,\n",
              " 5170,\n",
              " 5304,\n",
              " 2599,\n",
              " 10492,\n",
              " 268,\n",
              " 2320,\n",
              " 2667,\n",
              " 4953,\n",
              " 3533,\n",
              " 1328,\n",
              " 3553,\n",
              " 297,\n",
              " 350,\n",
              " 676,\n",
              " 2320,\n",
              " 1433,\n",
              " 105,\n",
              " 3790,\n",
              " 168,\n",
              " 4108,\n",
              " 29556,\n",
              " 439,\n",
              " 404,\n",
              " 695,\n",
              " 324,\n",
              " 922,\n",
              " 1833,\n",
              " 2599,\n",
              " 3193,\n",
              " 810,\n",
              " 1323,\n",
              " 799,\n",
              " 88,\n",
              " 12143,\n",
              " 2148,\n",
              " 7615,\n",
              " 1994,\n",
              " 7219,\n",
              " 1572,\n",
              " 3609,\n",
              " 58,\n",
              " 6991,\n",
              " 902,\n",
              " 26935,\n",
              " 5338,\n",
              " 648,\n",
              " 208,\n",
              " 4823,\n",
              " 3014,\n",
              " 168,\n",
              " 168,\n",
              " 26935,\n",
              " 407,\n",
              " 2161,\n",
              " 661,\n",
              " 3303,\n",
              " 1366,\n",
              " 973,\n",
              " 921,\n",
              " 2148,\n",
              " 808,\n",
              " 946,\n",
              " 1815,\n",
              " 309,\n",
              " 439,\n",
              " 4025,\n",
              " 126,\n",
              " 396,\n",
              " 80,\n",
              " 695,\n",
              " 147,\n",
              " 1241,\n",
              " 11030,\n",
              " 894,\n",
              " 126,\n",
              " 568,\n",
              " 1535,\n",
              " 2148,\n",
              " 811,\n",
              " 3083,\n",
              " 3617,\n",
              " 8307,\n",
              " 7934,\n",
              " 1262,\n",
              " 9935,\n",
              " 5632,\n",
              " 2194,\n",
              " 811,\n",
              " 16,\n",
              " 374,\n",
              " 658,\n",
              " 20886,\n",
              " 163,\n",
              " 2148,\n",
              " 248,\n",
              " 64,\n",
              " 985,\n",
              " 15571,\n",
              " 811,\n",
              " 8380,\n",
              " 855,\n",
              " 805,\n",
              " 1580,\n",
              " 2667,\n",
              " 50047,\n",
              " 2149,\n",
              " 2395,\n",
              " 733,\n",
              " 648,\n",
              " 2148,\n",
              " 95,\n",
              " 18271,\n",
              " 107,\n",
              " 1298,\n",
              " 794,\n",
              " 9460,\n",
              " 374,\n",
              " 2545,\n",
              " 918,\n",
              " 163,\n",
              " 1903,\n",
              " 4505,\n",
              " 7921,\n",
              " 582,\n",
              " 458,\n",
              " 646,\n",
              " 2436,\n",
              " 744,\n",
              " 23467,\n",
              " 568,\n",
              " 407,\n",
              " 218,\n",
              " 3585,\n",
              " 648,\n",
              " 922]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRe6PFEFQwqF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3b4434cf-ffa4-4af4-b4fd-2c0470b7db33"
      },
      "source": [
        "len(all_words_index)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12736"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jjdc2lJZQwqJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5c88dd4a-47ca-4427-8ab5-b1766d86b1e8"
      },
      "source": [
        "max(all_words_index[-1])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51242"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rlA2ucnkp7EA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "91c184da-924d-487b-aece-196ce34ba559"
      },
      "source": [
        "l = []\n",
        "for i in range(len(all_words_index)):\n",
        "    l.append(len(all_words_index[i]))\n",
        "l.index(max(l))\n",
        "max(l)\n",
        "#len(all_words_index)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4250"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "85Jk49Tfp7EC"
      },
      "source": [
        "# Visualize the cluster of books"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sLZd4ZUqp7EC",
        "colab": {}
      },
      "source": [
        "# import numpy as np\n",
        "# def avg_doc_embedding(all_docs):\n",
        "#     doc_vec = []\n",
        "#     for doc in all_docs:\n",
        "#         if len(doc) != 0:\n",
        "#             vector =  words_embedding(doc)\n",
        "#             doc_vec.append(np.mean(np.array(vector), axis=0))\n",
        "    \n",
        "#     summary_vec = np.array(doc_vec)\n",
        "#     return summary_vec\n",
        "\n",
        "# summary_vec = avg_doc_embedding(all_docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s9f1TtHUp7EE",
        "colab": {}
      },
      "source": [
        "# summary_vec.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-8YShqDVtORv",
        "colab": {}
      },
      "source": [
        "# pip install adjustText"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6QoaEMczp7EF",
        "colab": {}
      },
      "source": [
        "# from sklearn.manifold import TSNE\n",
        "# from adjustText import adjust_text\n",
        "# import seaborn as sns\n",
        "# import matplotlib.pyplot as plt\n",
        "# book_titles = new_file['book name']\n",
        "# titles_list = [title for title in book_titles]\n",
        "# titles_list = titles_list[:400]\n",
        "# def t_SNE_plot(df_data):\n",
        "#     # Initialize t-SNE\n",
        "#     tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 100)\n",
        "\n",
        "#     # Use only 400 rows to shorten processing time\n",
        "#     tsne_df = tsne.fit_transform(df_data)#tsne.fit_transform(df_data[:400])\n",
        "\n",
        "#     sns.set()\n",
        "#     # Initialize figure\n",
        "#     fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
        "#     sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
        "\n",
        "#     # Import adjustText, initialize list of texts\n",
        "\n",
        "#     texts = []\n",
        "#     data_to_plot = list(np.arange(0, 400, 40))#list(np.arange(0, 400, 40))\n",
        "\n",
        "#     # Append words to list\n",
        "#     for data in data_to_plot:\n",
        "#         texts.append(plt.text(tsne_df[data, 0], tsne_df[data, 1], titles_list[data], fontsize = 14))\n",
        "\n",
        "#     # Plot text using adjust_text (because overlapping text is hard to read)\n",
        "#     adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
        "#                 expand_points = (2,1), expand_text = (1,2),\n",
        "#                 arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
        "\n",
        "#     plt.show()\n",
        "\n",
        "# t_SNE_plot(summary_vec[:400])    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2joV1EE6p7EH"
      },
      "source": [
        "## Padding sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "enefEKmQp7EH",
        "outputId": "d35d0130-6ddd-4414-da02-cabf442e4be3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max_Length = 0  \n",
        "docs_length = []\n",
        "for x in all_words_index:\n",
        "    docs_length.append(len(x))\n",
        "    if len(x) > max_Length:\n",
        "        max_Length = len(x)\n",
        "\n",
        "docs_length = np.array(docs_length)\n",
        "max_Length"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4250"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sa5MKaO-cKk7",
        "outputId": "42667da7-0aed-4ba4-cc79-5cd100f656c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.where(docs_length<1)\n",
        "#docs_length[3382]"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([], dtype=int64),)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zVhVxD4Yp7EJ",
        "colab": {}
      },
      "source": [
        "def pad_sequences(all_words_index, seq_length):\n",
        "    \n",
        "    # getting the correct rows x cols shape\n",
        "    sequences = np.zeros((len(all_words_index), seq_length), dtype=int)\n",
        " \n",
        "    # for each review, I grab that review and \n",
        "    for i, row in enumerate(all_words_index):\n",
        "        if len(row)>0:\n",
        "            sequences[i, 0:len(row)] =  np.array(row)[:seq_length]\n",
        "    \n",
        "    return sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y6dnQuwrp7EL",
        "outputId": "8c60afaa-50fb-4112-83d2-87ed5bd4a448",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "sequences = pad_sequences(all_words_index, max_Length)\n",
        "\n",
        "assert len(sequences)==len(all_words_index), \"Sequences should have as many rows as reviews.\"\n",
        "assert len(sequences[0])==max_Length, \"Each sequence row should contain seq_length values.\"\n",
        "\n",
        "print(sequences[-1])\n",
        "np.where(sequences[-1] ==51242 )"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[40288   928  6584 ...     0     0     0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([26]),)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tM4vXOwrp7EN"
      },
      "source": [
        "# Training, Validation, and Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g2q-yFvfp7EN",
        "outputId": "239916e9-7799-4595-aeef-f9189b41828b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "split_frac = 0.8\n",
        "\n",
        "## split data into training, validation, and test data (features and labels, x and y)\n",
        "\n",
        "split_idx = int(len(sequences)*split_frac)\n",
        "train_x, remaining_x = sequences[:split_idx], sequences[split_idx:]\n",
        "train_y, remaining_y = one_hot[:split_idx], one_hot[split_idx:]\n",
        "train_len, remaining_len = docs_length[:split_idx], docs_length[split_idx:]\n",
        "\n",
        "# train_x, remaining_x = sequences[:128], sequences[128:384]\n",
        "# train_y, remaining_y = one_hot[:128], one_hot[128:384]\n",
        "# train_len, remaining_len = docs_length[:128], docs_length[128:384]\n",
        "\n",
        "test_idx = int(len(remaining_x)*0.5)\n",
        "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
        "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
        "val_len, test_len = remaining_len[:test_idx], remaining_len[test_idx:]\n",
        "\n",
        "## print out the shapes of your resultant feature data\n",
        "print(\"\\t\\t\\tSequences Shapes:\")\n",
        "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
        "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
        "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\t\tSequences Shapes:\n",
            "Train set: \t\t(10188, 4250) \n",
            "Validation set: \t(1274, 4250) \n",
            "Test set: \t\t(1274, 4250)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UDvw1Oi6hBOv",
        "outputId": "472a4a00-a983-445d-ec32-825982939b57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_len.min"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function ndarray.min>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iwJXkJ8zzrvt",
        "colab": {}
      },
      "source": [
        "# split_frac = 0.8\n",
        "\n",
        "# ## split data into training, validation, and test data (features and labels, x and y)\n",
        "\n",
        "# split_idx = int(len(all_words_index)*split_frac)\n",
        "# train_x, remaining_x = all_words_index[:split_idx], all_words_index[split_idx:]\n",
        "# train_y, remaining_y = one_hot[:split_idx], one_hot[split_idx:]\n",
        "# #train_len, remaining_len = docs_length[:split_idx], docs_length[split_idx:]\n",
        "\n",
        "# test_idx = int(len(remaining_x)*0.5)\n",
        "# val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
        "# val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
        "# #val_len, test_len = remaining_len[:test_idx], remaining_len[test_idx:]\n",
        "\n",
        "# ## print out the shapes of your resultant feature data\n",
        "# print(\"\\t\\t\\tSequences Shapes:\")\n",
        "# print(\"Train set: \\t\\t{}\".format(len(train_x)), \n",
        "#       \"\\nValidation set: \\t{}\".format(len(val_x)),\n",
        "#       \"\\nTest set: \\t\\t{}\".format(len(test_)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KyHBQ3l0p7ER"
      },
      "source": [
        "### Above only 2,0,1 because I only used  3 datasample to do all of these"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tbUPadNHp7ES"
      },
      "source": [
        "# DataLoaders and Batching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lcP-6vOJp7ES",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "# from torch.nn.utils.rnn import pad_sequence\n",
        "# # create Tensor datasets\n",
        "\n",
        "\n",
        "train_data = TensorDataset(torch.from_numpy(np.array(train_x)), torch.from_numpy(np.array(train_y)), torch.from_numpy(train_len)) \n",
        "valid_data = TensorDataset(torch.from_numpy(np.array(val_x)), torch.from_numpy(np.array(val_y)), torch.from_numpy(val_len))\n",
        "test_data = TensorDataset(torch.from_numpy(np.array(test_x)), torch.from_numpy(np.array(test_y)), torch.from_numpy(test_len)) \n",
        "\n",
        "# # train_data = Dataset(train_x, train_y) #, torch.from_numpy(train_len)\n",
        "# # valid_data = Dataset(val_x, val_y)\n",
        "# # test_data = Dataset(test_x, test_y) \n",
        "\n",
        "# # dataloaders\n",
        "batch_size = 128\n",
        "# #RANDOM_SEED = 1\n",
        "\n",
        "# def pad_collate(batch):\n",
        "#   (xx, yy) = zip(*batch)\n",
        "#   x_lens = [len(x) for x in xx]\n",
        "#   y_lens = [len(y) for y in yy]\n",
        "\n",
        "#   xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)\n",
        "#   yy_pad = pad_sequence(yy, batch_first=True, padding_value=0) , collate_fn=pad_collate\n",
        "\n",
        "#   return xx_pad, yy_pad, x_lens, y_lens\n",
        "\n",
        "# shuffling and batching data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, num_workers=4)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size, num_workers=4)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, num_workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tz_3dnhcp7EU"
      },
      "source": [
        "# LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KExMFqzUp7EU",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# import numpy as np\n",
        "# from torch import nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# # def kmax_pooling(x, dim, k):\n",
        "# #     index = x.topk(k, dim = dim)[1].sort(dim = dim)[0]\n",
        "# #     return x.gather(dim, index)\n",
        "\n",
        "# class LSTMText(torch.nn.Module): \n",
        "#     def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, \n",
        "#                 linear_hidden_size, num_classes, freeze_embeddings=True):\n",
        "#         super(LSTMText, self).__init__()\n",
        "        \n",
        "#         self.num_classes = num_classes\n",
        "#         # 1. embedding layer\n",
        "#         self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "#         # set weights to pre-trained\n",
        "#         self.embedding.weight = nn.Parameter(torch.from_numpy(word_vec_array)) # all vectors\n",
        "#         # (optional) freeze embedding weights\n",
        "#         if freeze_embeddings:\n",
        "#             self.embedding.requires_grad = False\n",
        "\n",
        "#         self.lstm =nn.LSTM( input_size = embedding_dim,\n",
        "#                             hidden_size = hidden_size,\n",
        "#                             num_layers = num_layers,\n",
        "#                             bias = True,\n",
        "#                             batch_first = False,\n",
        "#                             # dropout = 0.5,\n",
        "#                             bidirectional = True\n",
        "#                             )\n",
        "\n",
        "#         # self.dropout = nn.Dropout()\n",
        "#         self.fc = nn.Sequential(\n",
        "#             nn.Linear(hidden_size*2, linear_hidden_size),\n",
        "#             nn.BatchNorm1d(linear_hidden_size),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Linear(linear_hidden_size, num_classes)\n",
        "#         )\n",
        "        \n",
        "#          #activation function\n",
        "#         self.act = nn.Sigmoid()\n",
        " \n",
        "#     def forward(self, text):\n",
        "      \n",
        "#         #text = [batch size,sent_length]\n",
        "#         embedded = self.embedding(text)\n",
        "#         #embedded = [batch size, sent_len, emb dim]\n",
        "#         print(embedded.shape)\n",
        "#         #packed sequence\n",
        "#         #packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=True)\n",
        "        \n",
        "#         #packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
        "#         packed_output = self.lstm(embedded) #.view(len(text), 1, -1)\n",
        "#         #hidden = [batch size, num layers * num directions,hid dim]\n",
        "#         #cell = [batch size, num layers * num directions,hid dim]\n",
        "        \n",
        "#         #concat the final forward and backward hidden state\n",
        "#         #hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
        "                \n",
        "#         #hidden = [batch size, hid dim * num directions]\n",
        "#         #logits=self.fc(hidden)\n",
        "#         logits=self.fc(packed_output) #.view(len(text),-1)\n",
        "\n",
        "#         #Final activation function\n",
        "#         probas=self.act(logits, dim = 1)\n",
        "\n",
        "#         return logits, probas\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kvsMqQ2lv1aO",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "# def kmax_pooling(x, dim, k):\n",
        "#     index = x.topk(k, dim = dim)[1].sort(dim = dim)[0]\n",
        "#     return x.gather(dim, index)\n",
        "\n",
        "class LSTMText1(torch.nn.Module): \n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, \n",
        "                linear_hidden_size, num_classes, freeze_embeddings=True):\n",
        "        super(LSTMText1, self).__init__()\n",
        "        \n",
        "        self.num_classes = num_classes\n",
        "        # 1. embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        # set weights to pre-trained\n",
        "        self.embedding.weight = nn.Parameter(torch.from_numpy(word_vec_array)) # all vectors\n",
        "        # (optional) freeze embedding weights\n",
        "        if freeze_embeddings:\n",
        "            self.embedding.requires_grad = False\n",
        "\n",
        "        self.lstm =nn.LSTM( input_size = embedding_dim,\n",
        "                            hidden_size = hidden_size,\n",
        "                            num_layers = num_layers,\n",
        "                            bias = True,\n",
        "                            batch_first = False,\n",
        "                            # dropout = 0.5,\n",
        "                            bidirectional = True\n",
        "                            )\n",
        "\n",
        "        # self.dropout = nn.Dropout()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_size*2, linear_hidden_size),\n",
        "            nn.BatchNorm1d(linear_hidden_size),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(linear_hidden_size, num_classes)\n",
        "        )\n",
        "        \n",
        "         #activation function\n",
        "        self.act = nn.Sigmoid()\n",
        " \n",
        "    def forward(self, text, text_lengths):\n",
        "      \n",
        "        #text = [batch size,sent_length]\n",
        "        embedded = self.embedding(text)\n",
        "        #embedded = [batch size, sent_len, emb dim]\n",
        "        print(embedded.shape)\n",
        "        #packed sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=True, enforce_sorted=False)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
        "        #packed_output = self.lstm(embedded) #.view(len(text), 1, -1)\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\n",
        "        \n",
        "        #concat the final forward and backward hidden state\n",
        "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
        "                \n",
        "        #hidden = [batch size, hid dim * num directions]\n",
        "        logits=self.fc(hidden)\n",
        "        #logits=self.fc(packed_output) #.view(len(text),-1)\n",
        "\n",
        "        #Final activation function\n",
        "        probas=self.act(logits)\n",
        "\n",
        "        return logits, probas\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "A7k-ELD2p7EW"
      },
      "source": [
        "# Instantiate the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JNLvp2Zyp7EW",
        "outputId": "23558e65-d53b-4477-cfb1-b8e81ce429a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# vocab_size, embedding_dim, hidden_size, num_layers, \n",
        "#                  bidirectional, dropout, linear_hidden_size, num_classes\n",
        "    \n",
        "# Instantiate the model with hyperparameters\n",
        "import random\n",
        "vocab_size = word_vec.shape[0]+1#len(google_vecs.vocab)\n",
        "#num_classes = 1 # binary class (1 or 0)\n",
        "embedding_dim = google_vecs.vector_size # 300-dim vectors\n",
        "hidden_size = 128 #256 #LSTM hidden size\n",
        "num_layers=2 #LSTM layers\n",
        "linear_hidden_size = 20 # units number of full connected \n",
        "num_classes = len(label_list)\n",
        "\n",
        "random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)  ###revise\n",
        "\n",
        "model = LSTMText1(vocab_size, embedding_dim, hidden_size, num_layers, \n",
        "                 linear_hidden_size, num_classes)\n",
        "\n",
        "model = model.float()\n",
        "\n",
        "print(model)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "#No. of trianable parameters\n",
        "def count_parameters(model):\n",
        "    return sum(m.numel() for m in model.parameters() if m.requires_grad)\n",
        "    \n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTMText1(\n",
            "  (embedding): Embedding(51243, 300)\n",
            "  (lstm): LSTM(300, 128, num_layers=2, bidirectional=True)\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=256, out_features=20, bias=True)\n",
            "    (1): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Linear(in_features=20, out_features=227, bias=True)\n",
            "  )\n",
            "  (act): Sigmoid()\n",
            ")\n",
            "The model has 16,218,431 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DDXXgXiNp7EY",
        "colab": {}
      },
      "source": [
        "NUM_EPOCHS = 5\n",
        "GRAYSCALE = False\n",
        "intial_lr = 0.1\n",
        "\n",
        "criterion = nn.BCELoss()       \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=intial_lr)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
        "                                                   gamma=0.01,\n",
        "                                                   last_epoch=-1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "h-rrGm1Sp7Ea"
      },
      "source": [
        "# Define loss function and evaluation parameters and Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19fFy2LrQwrf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_accuracy_and_loss(model, data_loader, device):\n",
        "    correct_pred, num_examples = 0, 0\n",
        "    loss = 0.\n",
        "    for i, (features, targets, features_len) in enumerate(data_loader):\n",
        "            \n",
        "        features = features.to(device)\n",
        "        targets = targets.to(device)\n",
        "        features_len = features_len.to(device)\n",
        "        #print(features_len)\n",
        "        logits, probas = model(features, features_len)\n",
        "        loss += criterion(probas, targets.float()).item()\n",
        "        predicted_labels = (probas > 0.5).cpu().numpy()\n",
        "        num_examples += targets.size(0)\n",
        "        \n",
        "    average_loss = loss/ num_examples\n",
        "    accuracy = accuracy_score(targets.cpu().numpy(), predicted_labels)\n",
        "    #ACU = roc_auc_score(targets.cpu().numpy(), predicted_labels)\n",
        "    f_measure = precision_recall_fscore_support(targets.cpu().numpy(), predicted_labels,average='weighted')\n",
        "    return accuracy, average_loss, f_measure"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kAqtQ44mp7Ea",
        "outputId": "4f028e15-af01-4a43-89df-2a54f7db8644",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "start_time = time.time()\n",
        "train_acc_lst, valid_acc_lst = [], []\n",
        "train_loss_lst, valid_loss_lst = [], []\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch_idx, (features, targets, features_len) in enumerate(train_loader): #, \n",
        "\n",
        "        ### PREPARE MINIBATCH\n",
        "        features = features.to(DEVICE)\n",
        "        targets = targets.to(DEVICE)\n",
        "        features_len = features_len.to(DEVICE)\n",
        "#         features, features_lengths = batch.features\n",
        "#         targets = batch.targets\n",
        "        #print(features_len)\n",
        "        #retrieve text and no. of words\n",
        "        #print(features_len)\n",
        "            \n",
        "        ### FORWARD AND BACK PROP\n",
        "        features = features.view(features.size(0), -1)\n",
        "        logits, probas = model(features, features_len)\n",
        "        cost = criterion(probas, targets.float())\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        cost.backward()\n",
        "        \n",
        "        ### UPDATE MODEL PARAMETERS\n",
        "        optimizer.step()\n",
        "        \n",
        "        ### LOGGING\n",
        "        if not batch_idx % 20:\n",
        "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
        "                   f'Batch {batch_idx:03d}/{len(train_loader):03d} |' \n",
        "                   f' Cost: {cost:.4f}')\n",
        "        if not epoch % 10:\n",
        "            scheduler.step()\n",
        "        \n",
        "    # no need to build the computation graph for backprop when computing accuracy\n",
        "    model.eval()\n",
        "    with torch.set_grad_enabled(False):\n",
        "        train_acc, train_loss, train_f_measure = compute_accuracy_and_loss(model, train_loader, device=DEVICE)\n",
        "        valid_acc, valid_loss, valid_f_measure = compute_accuracy_and_loss(model, valid_loader, device=DEVICE)\n",
        "        train_acc_lst.append(train_acc)\n",
        "        valid_acc_lst.append(valid_acc)\n",
        "        train_loss_lst.append(train_loss)\n",
        "        valid_loss_lst.append(valid_loss)\n",
        "        print(f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} Train Acc.: {train_acc:.2f}%'\n",
        "              f' | Validation Acc.: {valid_acc:.2f}%')\n",
        "\n",
        "    elapsed = (time.time() - start_time)/60\n",
        "    print(f'Time elapsed: {elapsed:.2f} min')\n",
        "    print('the precision_recall_fscore_support of F-score_train: ', train_f_measure)\n",
        "    print('the precision_recall_fscore_support of F-score_val: ', valid_f_measure)\n",
        "\n",
        "\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([  38,  204,  202,  401,  149,  111,  198,  123,  245,  218,  103,  484,\n",
            "         269,   81,  171,  202,  234,  167,  319,  343,  247,  327,  207,  103,\n",
            "         115,   74,  340,   17,   32, 1007,  212,  822,   94,   83,   70,   52,\n",
            "         332,   25,  115,  200,  578, 1295,  375,   60,  175,  366,  182,   77,\n",
            "         747,   55,  370,  647,   78,   56,  196,  130,   95,  135,  304,   63,\n",
            "          34,   91,  390,  739,  205,   19,  333,   33,  416,   16,  740,  568,\n",
            "         154,  277,  392,  280,  222,  115,  283,  325,  135,  651,  781,  515,\n",
            "         319,  466,   73,  261,  249,  359,  434,  111,  144,  304,   45,  130,\n",
            "          65, 1175,   28,   46,   94,  474,   57,  291,   74,  143,  163,  179,\n",
            "         152,  415,   42,  346,   85,  180,  446,  111,   33,   59,   72,  411,\n",
            "          50,  298,   10,  321,  459,  418,  218,  417], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "Epoch: 001/002 | Batch 000/080 | Cost: 0.7084\n",
            "tensor([  99,   93,  437,   52,   92,  299,   97,  270,   42,  116,   59,   20,\n",
            "          26,  102,  202,  415,  684,   55,   65,  224,  205,   29,   36,  488,\n",
            "          72,   61,   85,  245,  178,   81,   23,  223,  266,  709,   80,  691,\n",
            "         221,   45,   41,  194,  472,   60,  402,  272,   93,   52,   13,   58,\n",
            "          66,  269,  724,   30,  372,   59,  410,  250,  162,  427,  200,  133,\n",
            "          25,   90,   10,   66,   94,  140,  293,  249,  110,  165, 1192,  372,\n",
            "         366,  205,  369,  108,   22,  222,  112,  139,  448,  179,  248,  479,\n",
            "          67,  106,   93,   48,  324,  101,   48,   50,  132,  369,  645,   21,\n",
            "          86,   37,  156,  115,  116,  129,   41,  183,  212,  177,  416,  223,\n",
            "         263,  163,   48,  112,  422,  784,  394,  564,   12,   61,  132,  233,\n",
            "         510,  127,   50,  297,   22,   76,  140,  446], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 65, 193, 109, 128, 319, 231, 715,  47, 497, 521, 123,  53, 237, 297,\n",
            "        470,  77, 153,  98,  51,  22, 586,  23,  52, 652,  52, 174,  43, 162,\n",
            "        484, 111, 118, 333, 168, 170, 867,  58, 957, 240, 810, 204, 228, 262,\n",
            "         34, 144,  26,  67, 101, 273,  10, 170, 557, 345, 338,  87,  26, 151,\n",
            "         22,  50, 591,  49, 270, 264, 286,  37, 287,  81, 225, 348, 316, 241,\n",
            "         71,  87,  39,  74,  67, 243, 597, 418, 458, 362,  50, 111, 495, 590,\n",
            "         69, 596,  28, 247, 171,  50, 542, 148,  52,  55, 388,  50,  22, 195,\n",
            "        149,  59, 127, 558,  46,  55, 485, 271, 100, 192,  35, 936,  45,  65,\n",
            "        204,  54,  28,  62, 245, 164,  18, 390, 460, 180, 242, 258,  35,  20,\n",
            "        478,  36], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([418, 247,  75, 312,  33,  80,  75, 148, 154,  72, 115,  79, 420,  48,\n",
            "        229, 120, 396,  30, 263, 332, 165, 563, 114,  83, 427, 137, 131, 578,\n",
            "        366, 154,  32,  23, 124,  19, 266, 569,  51, 243,  15,  66, 292,  80,\n",
            "        150,  79, 605, 166,  83, 164, 123, 533, 304, 124, 103, 286, 353, 410,\n",
            "        148,  78,  88,  88,  31, 927, 125, 213, 133, 114, 102,  55, 323, 153,\n",
            "         77,  91,  99,  57,  29,  61, 190, 183, 193,  40, 137, 422, 183, 169,\n",
            "         55, 365, 448,  78,  69,  92, 102, 744, 164,  41, 123, 118, 346,  69,\n",
            "        673,  15,  57, 184,  41, 242, 374, 275,  51, 673,  94,  26, 248, 320,\n",
            "        230, 118,  81, 886, 240, 318, 462,  49, 374,  18, 679, 240, 214, 219,\n",
            "        252,  96], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  90,  447,  379,  175,   59,   57,   58,  291,  101,  124,  101,  207,\n",
            "          14,   54,   58,  291,  111,   40,  217,  453,   24,   53,  229,   36,\n",
            "          24,  305,   90,  269,  916,   77,   54,  119,  809,   12,  150,  587,\n",
            "         183,  149,  193,   47,   49,   61,  179,  361,  113,  132,   99,   75,\n",
            "         336,  255,   33,   82,   26,  410,  133,   71,  232,  387,   61, 1207,\n",
            "          54,   10,   93,  203,   23,  134,  260,  830,   27,   55,   50,  781,\n",
            "          52,  556,   27,  413,  286,  219,  139,  127,  116,  617,  369,  103,\n",
            "         160,  222,  418,  194,   99,   51,  267, 1090,  337,   38,  137,   98,\n",
            "          63,  148,  377,   78,   73,  394,  165,  224,  181,   92,   95,   53,\n",
            "          32,  390,  361,   71, 1053,  248,  509,  165,  112,   26,  121,  133,\n",
            "         233,  127,  162,   84,   47,  163,  274,  191], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  48,   76,   57,  286,   26,  376, 2005,  677,  170,   41,  168,   78,\n",
            "         463,   83,  110,  171,   38,  293,   47,   72,  216,  399,  119,  377,\n",
            "           9,  330,  126,   64,  134,   80,   21,  216,  123,   36,  352,  341,\n",
            "         124,  362,   42,  234,  197,  492,   34,  233,  123,    8,   72,  227,\n",
            "          84,  679,  137,  403,  981,  200,  370,   29,   87,  541,  222,   24,\n",
            "          66,   42,   89,   71,   58,  671,   47,   60,  319,  229,   57,   38,\n",
            "         220,  244,   50,   33,  251,  156,   47,   89,  107,  298,  802,  430,\n",
            "         104,  509,   81,  485,  173,  197,  631,   15, 1178,  119,  367,  111,\n",
            "          90,  197,   64,   84,  335,   42,  534,  245,  194,  370,  983,  152,\n",
            "         161,  321,   35,  260,  243,   88,  283,    8,   56,   32, 1230,   71,\n",
            "          52,   78,  148,  234,  146,  262,   40,   33], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 413,   21,  476,   59,  942,   48,   15,  191,  257,  131,  479,  175,\n",
            "          87,  366,  327,   22,   64,  222,  436,  445,  308,   76,   24,  126,\n",
            "         415,   25,  259,   78,  389,    8,  600,   70,   90,  733,  222,  225,\n",
            "         314,   54,   75,  111,  434,   91,   36,  231,   32,   44,  115,  302,\n",
            "         500,  287,   36,   92,  184,   50,   90,   66,  275,  358,  217,  706,\n",
            "         159,   98,  889,  105,   71,  213,   17,  105,   81,  347,  117,  118,\n",
            "          46,  409,   37,  619,   98,  147,  459,  264,  243,  353,   28,  187,\n",
            "         426,  231,   30,   47,  173,   27,  511, 1196,   53,  186,   71,   82,\n",
            "         167,   60,  227,   54,  312,  147,   19,  220,   34,  110,  299,  448,\n",
            "         222,  425,   17,  355,   92,  675,   48,   28,  363,   13,   33,  289,\n",
            "         245,  161,  102,  372,  529,  149,  253,   92], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  67,  370,  118,   62,   67,    9,   36,   49,   53,  229,   57,    9,\n",
            "         240,  138,   89,    9,  586,  118,  259,   77,  193,  139,   95,  330,\n",
            "          59,  110,   39,   92,  229,   88,   56,  351,  213,   87,  190,  163,\n",
            "         162,  101,   64,  332,  248,   57,  131,  256,  112,   37,   41,   63,\n",
            "          43,  126,   97,  153,  122,  304,   69,   67,  222,  615,  163,   70,\n",
            "         243,  235,  180,   22,  135, 1342,   48,  530,   48,  422,  218,  182,\n",
            "          63,  248,   81,   96,  164,  310,   34,  373,   14,  238,  375,  259,\n",
            "         140,  245,   30,   80,   52,   26,  100,   89,   78,  145,   32,  419,\n",
            "         308,   66,   40,   56,  131,  125,  120,  529,  212,   46,   31,  363,\n",
            "          76,  221,  130,  634,  100,   27,  159,  173,   37,   42,  603,   34,\n",
            "          84,  225,  281,  318,  698,   66,  163,  750], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 135,  197,  330,  276,   55,  161,   66,  167, 1219,   79,  517,   75,\n",
            "          42,   31,   39,  348,  930,  142,   76,  308,  430,  137,   59,   35,\n",
            "          21,  270,  362,  114,   19,   20,  179,  190, 1344,  350,  207,  153,\n",
            "         186,  295,   23,   91,  217,  256,  137,  118,   68,  123,   89,   97,\n",
            "          76,  350,   72,  148,   82,  162,  107,  257,   81,  645,  613,  137,\n",
            "         428,  685,  310,  363,   32,   50,  926,  460,  107,  327,   35,   40,\n",
            "          28,   82,  283,  231,  361,  221,  463,   49,   63,  496,  236,  194,\n",
            "          56,  635,  216,  279, 1404,  194,  421,  130,  127,  303,  728,   55,\n",
            "         111,  221,   66,  113,   35,  286,  293,   50,  177,  234,  453,  190,\n",
            "         151,  689,  359,  306,   97,  496,  120,  518,  559,  279,   94,   85,\n",
            "          95,   14,   87,  360,  168,  706,   85,  116], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  36,   54,  202,  210,  351,  237,  447,   18,   20,  336,  301,  326,\n",
            "         162,  135,   86,  157,   87, 1145,  236,   33,   86,   36,   48,  451,\n",
            "         304,  118,  143,   51,  205,  385,  147,   99,   45,  127,  201,  165,\n",
            "          62, 1041,  367,   54,  134,  184,  159,  488,  367,   40,  161,  123,\n",
            "         259,   66,   93,   83,   80,  189,  200,  141,  267,  161,  426,  257,\n",
            "         136,  348,  552,  227,   59,  293,  279,  133,  367,  282,  210,  452,\n",
            "          43,  331,   71,  232,   71,   71,  147,  492, 1501,   15,   96,  308,\n",
            "          45,  352,  287,   96,  157,   87,   63,  123,   59,  389,  350,  293,\n",
            "         423,   38,  229,  124,   98,  304,  131,  171,   47,  139,  387,   51,\n",
            "          62,   28,  133,  513,   27,  191,  225,  499,  323,  381,  106,  474,\n",
            "         409,  138,  510,   72,  786,  157,  371,  151], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 754,  263,  231,  354,  579,  815,  598,  528,   62,  641,  103,  397,\n",
            "         160,  747,  311,  113,   76,   64,  186,   37,   20,  407,  199,  154,\n",
            "          55,  419,   31,   56,  212,  107,   68,   16,   94, 1871,  584,   67,\n",
            "         419,  179,  969,  324,   70,  437,  663,  247,  109,  253,   69,  351,\n",
            "          15,  147,  137,   55,  453,   38,  144,   44,  107,  102,   49,  196,\n",
            "          35,   96,  182,  330,  241,  288,  463,   83,  137,   20,   72,  496,\n",
            "         311,  369,   43,  124,  212,   87,   56,   60,   80,  400,  170,  432,\n",
            "         346,  468,   46,  275,   54,   55,  268,   80,  101,  356,   86,   47,\n",
            "         214,  679,   51,  187,  287,  415,  155,   66,  115,  146,  416,  319,\n",
            "         675,   43,   68,  851,  104,  145,   30,  198,  457,   29,  178,  167,\n",
            "         270,   67,   87,   86,   47,   50,   56,   45], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 386,   33,   51,   16,  225,   98,  622,   71,   53,  179,   24,  181,\n",
            "         292,   96,  557,   59,   56,   73,  206,  433,   70,  219,  181,  309,\n",
            "          16,  286,  100,   28,   36,   23,   73,  285,  218,  230,   38,  101,\n",
            "          21,   65,   29,   56,   66,  420,   77,  154,   82,  193,  150,   93,\n",
            "         386,   65,  530,   35,  705,  455,   26,  147,   77,   24,  286,  344,\n",
            "         295,  770,  195,  400,  233,  280,  125,  596,   49,  116,   18,  232,\n",
            "         180,   34,  595,  245,   70,   48,  167,   83,  687,  933,   17,   60,\n",
            "         147,  446,  293,   63,   43,  306,  150,  269,  219,   87, 1697,   60,\n",
            "         112,   60,  435,  352, 1001,  791,   80,  504,   36,  362,  266,  109,\n",
            "          36,  310,  115,  441,  132,  205,   31,  551,  204,   69,  868,  202,\n",
            "          28,  215,   83,  218,  433,  173,  174,  554], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 249,  500,  322,  239,   41,   60,  455,  612,   24,  236,  266,  651,\n",
            "         155,   59,  119,  293,  133,  133,  138,  202,  943,   84,  324,  340,\n",
            "          90,   81,  731,  573,  241,  188,  164,   61,  167,  488,  206,  184,\n",
            "          38,  323,  144,  629,  535,  247,   34,   39, 1353,  319,   45,   74,\n",
            "         661,  285,   28,   78,   60,   34,  676,  747,  300,  185,  282,   14,\n",
            "          42,   48,  192,  183,   19,   16,  103,   90,   50,  440,   20,  168,\n",
            "          38,   85,   75,  144,  234,  192,  160,   65,  488,  159,  175,   40,\n",
            "         309,   90,  170,   64,   20,  141,  483,   33,   95,  706,  475,  142,\n",
            "          38,  107,   82,  128,   43,   54,   65,  230,  100,   97,   63,  424,\n",
            "         133,   38,   63,   48,  138,    8,  224,  106,  667,   49,   80,   95,\n",
            "         124,  213,  379, 1259,  106,  191,   44,   20], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([196, 237,  21,  47, 211, 250,  99,  39,  46, 102,  55,  23, 132, 139,\n",
            "        185, 627, 159,  64, 193,  55, 304, 240, 496, 107,  25,  72, 104, 149,\n",
            "        219,  11,  23, 440, 309, 667,  22,  29,  41,  66, 180,  73,  76, 302,\n",
            "        151, 235, 138, 543, 248, 511,  25,  56,  42,  77, 390, 431, 284, 129,\n",
            "         15, 118, 469, 465, 135,  20, 284,  50, 264, 371, 115, 162, 118, 771,\n",
            "        236,  32, 242, 348,  76,  36,  68, 785, 111,  83, 145, 141, 206, 180,\n",
            "         37, 424, 306, 506, 329,  56, 355,  15, 268, 189, 310, 187, 169,  90,\n",
            "         52, 117,  20, 964, 734, 238,  20, 102,  43, 176,  75, 125, 475,  74,\n",
            "        138, 372, 213, 620,  26, 402,  30, 229, 241,  56, 370,  37, 298, 290,\n",
            "         69,  20], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  46,  163,   74,  220,  498,   81,   52,  129,  188,  198,   47,  132,\n",
            "         320,  336,  101,  119,  100,   81,   31,  126,  266,  115,  368,  125,\n",
            "          43,  235,  210,   28,  177,  209,   23,   11,  135,  171,  169,  253,\n",
            "         176,  338,  207,  114,  208,  147,   84, 1371,   89,   47,  129,  231,\n",
            "          84,  125,   21,   17,   24,   60,   35,  363,   86,  117,   27,  127,\n",
            "         109,  215,  179,  339,   66,   91,  119,  365,  127,  196,  174,  115,\n",
            "          85,  221,  439,   43,  504,  279,  424,  123,   50,  153,   36,   38,\n",
            "         501,  182,  350,   80,   12,  221,    8,   28,   91,  191,   87,  407,\n",
            "         213,  405,   86,   78,   52,   31,   13,   29,  710,   76,   30,  126,\n",
            "          71,  108,  338,   52,   62,  369,  827,   78,   45,  173,   24,  463,\n",
            "          61,  105,  254,  141,  434,  273,   16,   86], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 42,  85, 504,  56,  86, 143, 845, 162, 133, 222,  24, 456, 100, 412,\n",
            "         17, 248, 330, 323, 277,  44, 101, 238, 774, 132,  88, 102, 165,  65,\n",
            "         46, 658,  68,  73,  41,  47, 787, 415,  75, 185, 214, 190, 386, 138,\n",
            "         30,  14,  21,  95, 310, 100, 316, 342,  23,  95, 370, 717,  35, 235,\n",
            "         69, 298, 117, 355,  27, 584,  43, 129,  33,  20, 391,  32, 112,  39,\n",
            "         99, 236, 620, 296,  62, 660,  22,  42, 218, 263,  42, 378,  69, 519,\n",
            "        190, 109,  34, 355, 135, 531,  39, 274, 226,  11, 446, 364, 429,  78,\n",
            "        275,  48,  36, 153,  20, 276, 183, 520,  69,  52, 197, 227, 231, 104,\n",
            "        476, 306, 500, 116, 155,  40, 422, 295, 518,  76,  57, 316,  39, 444,\n",
            "         94,  34], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([147,  89,  51,  42,  32,  79, 107, 219, 238,  29,  48,  47, 141, 156,\n",
            "        184, 110, 413, 184, 574,  64,  49, 140,  98, 375, 575,  84, 203, 564,\n",
            "         40,  69, 196,  43, 216, 237, 293, 160, 163,  68, 193,  38,  73, 152,\n",
            "        466, 184,  67,  79, 169,  62,  41,  37, 440, 174, 104, 377, 146,  13,\n",
            "        422, 355, 112,  15, 322,  39, 200, 145,  78, 427,  69,  71, 272,  62,\n",
            "        151,  10, 226,  68, 116, 104, 250,  37, 427,  99, 128, 144, 169, 139,\n",
            "         46, 142, 150,  36, 155, 526, 106, 540, 168, 217,  83, 280, 321,  40,\n",
            "        504, 268,  72, 200,  50, 233, 955,  38,  98, 215,  48, 918, 387, 109,\n",
            "        502, 171, 306,  34, 449, 458,  56, 250,  69,  70, 182,  52,  54,  34,\n",
            "        178, 422], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  79,   93,  230,   46,  232,   15,  143,  275,  116,   85,  571,  104,\n",
            "         466,   76,  405,   24,  189,  360,  284,  101,  196,  109,  312,  401,\n",
            "        2700,   47,   36,   41,   76,  126,   61,  409,   38,   57,  130,   85,\n",
            "         103,  382,  250,  241,   71,  347,  205,  439,  318,  367,  168,   64,\n",
            "          66,  652,  129,  163,  272,  402,  130,  253,  281,  101,  419,   55,\n",
            "          41,  142,  218,   49,  103,  406,  241,  120,  473,   11,   18,  196,\n",
            "         314,  117,  115,   25,  115,  725,  146,  395,  116,  163,   38,  388,\n",
            "          97,  408,  701,  205,  354,   91,  464,  569,  308,   85,  121,  254,\n",
            "          87,   32,  457,  190,   44,   86,  264,  196,  765,  204,  281,   86,\n",
            "          46,  540,  112,  309,   33,  113,  158,   72,   92,  201,   20,  513,\n",
            "         133,  763,  161,   56,   27,  144,  359,   97], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 130,  303,  333,  233,  169,   73,  366,   68,  447,   68,  230,  207,\n",
            "          68,   61,  715,   47,  114,   84,   12,  281,  277,   65,  521,   41,\n",
            "          45,  192,  495,   82,  497,  579,   45,  475,   32,   75,   61, 1287,\n",
            "          46,  349,   62,   78,   43,   69,  208,   25,  688,   46,  159,   77,\n",
            "         202,  763,  164,   18,  176,  275,  150,  158,  185,  477,  295,  121,\n",
            "         468,  367,  710,   30,  186,   47,  575,  383,  274,  227,  181,   93,\n",
            "          83,   91,   66,  518,  318,  227,   98,   20,   85,  158,   55,  146,\n",
            "         499,  111,  799,   40,  129,  145,  630,  225,  156,  105,  587,  458,\n",
            "          77,  126,   32,  289,  306,   95,   26,  194,  425,   97,  248,   19,\n",
            "         178,  209,  498,  110,  258,  165,   41,  257,  133,   75,  823,  196,\n",
            "         118,  115,   24,   92,   64,   75, 1406,  505], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  67,  120,  375,   61,  112,  642,  758,   76,   93,  106,   32,  135,\n",
            "         474,  202,  496,  212,  243,  407,   37,   68,  275,  128,   63,  786,\n",
            "         137,  160,   33,   43,  437,  365,   73,  104,  207,   62,  130,  266,\n",
            "         223,  339,   42,   45,   32,  313,  297,  130,  300,  237,   28,  412,\n",
            "          91,  139,   57,  314,  248,  456,  211,  211,  160,  644,  407,  107,\n",
            "          90,  397,  256,   77,   73,   93,  118,  318,  151,  236,  253,  385,\n",
            "         267,  349,  150,  344,   55,   93,   58,  286,  211,  225,  125,   85,\n",
            "          72,   43, 1115,  138,   39,  369,  408,  159,   68,  153,  123,  202,\n",
            "         129,  238,   57,  392,   67,  405,   66,  168,  285,   46,  212,  279,\n",
            "         201,   45,  491,  325,   95, 1047,  139,  236,   39,  314,  330,  147,\n",
            "         303,   46, 1432,  346,   16,  148,  217,   27], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 141,    9,  275,   20,  148,  136,  202,  112,  177,   54,  684,  135,\n",
            "         264,  101,   86,  367,   80,   51, 1303,   36,   63,   97,  277,  262,\n",
            "          63,  119,  213,   49,  530,  197,  157,   50,  331,   55,  140,  143,\n",
            "         139,   57,  321,  364,  100,  292,  164,   67,  121,  264,  152,  159,\n",
            "         262,  330,   69,  415,  273,  310,  408,  116,   70,  546, 1136,  336,\n",
            "         346,  101,  112,  459,  189,  734,   45,  222,  501,   25,   10,   41,\n",
            "          77,  110,   68,  138,   41,  285,   62,  233,  334,   16,  354,  223,\n",
            "         392,  447,  288,   28,   59,   33,   85,   74,   66,   73,  734,  198,\n",
            "         453,  224,   47,  175,  481,  506, 1816,  503,   42,  405,  383,  150,\n",
            "         154,   36,   78,  488,  293,  103,  225,   51,  429,  117,  573,  115,\n",
            "         146,  115,  486, 1243,  280,  238,  537,  128], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "Epoch: 001/002 | Batch 020/080 | Cost: 0.4017\n",
            "tensor([ 173,  731,   18,  589,  475,  121,   13,   74,  180,  261,  175,  225,\n",
            "        1007,  256,  198,   91,   91,   70, 1015,   41,  154,   14,  123,  387,\n",
            "          54,  108,  107,   67,  159,  350,  300,  128,  217,  255,   26,  357,\n",
            "         139,  147,  670,   46,  596,  103,  527,   34,  132,   37,  219,   49,\n",
            "         990,  107,   62,   39,  399,  206,  159,  266,  145,  149,   92,  362,\n",
            "         283,  233,  101,  256,  146,  265,  175,   73,  141,  303,   83,   69,\n",
            "         142,  207,  354,  100,   85,  118,   41,  116,  300,  235,  453,  288,\n",
            "          45,  399,   19,  124, 1329,   35,  158,  135,  441,   81,  118,   36,\n",
            "          67,  203,   79, 2461,   26,  353,  135,   84,  272,   86,  631,  129,\n",
            "         539,  117,  571,  134,   53,  274,   19,  579,   65,   45,   80,  367,\n",
            "          75,   47,   73,  493,  459,  233,  433,   10], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 112,  160,   63,  183,   25,   73,  833,  135,  206,  648,  280,  162,\n",
            "          52,  227,  114,  164,  119,  267,   26,  411,  137,   48,   88,  150,\n",
            "         168,  216,   66,   36,  378,  393,  588,  334,   82,   51,  130,   60,\n",
            "          15,   65,   87,  119,  357,  718,   35,   58,  347,   79,   21,   75,\n",
            "          84,  174,  129,   42,   63,   29,  303,  296,  564,  122,  189,  158,\n",
            "         184,   42,  138,   94,  157,   34,   65,   78,  206,  278,  144,  426,\n",
            "         119,  275,   21,  172,   31,  660,  331,   78,   44,  383,  804,  538,\n",
            "         370,   23,   52,   57,   25,  372,  615,   52,  235,   68,   60,  308,\n",
            "          80,  539,   57,  593,  554,   28,  580,   70,  426,  351,  335,   90,\n",
            "        1162,  187,  199,   98,  360,  172,  508,  319, 1952,  230,   71,  701,\n",
            "         571,  134, 1001,   44,   45,  116,  315,   87], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([166, 371, 928, 305,  34,  36, 404,  31,  55, 165, 103, 394,  77, 592,\n",
            "         47,  43,  46, 213, 625, 370, 229, 248, 101, 485,  40,  22, 181,  37,\n",
            "        487, 490, 143, 611, 248, 171, 197,  14, 293, 654,  64, 151,  11, 365,\n",
            "        251,  47, 135,  42, 329, 224,  42, 236, 429,  74, 210, 106,  58, 519,\n",
            "        705, 128, 744, 663, 249,  71,  20,  98, 244, 471,  24, 369, 169, 412,\n",
            "        222, 235,  42,  76, 177, 669,  53, 428, 416, 509,  82, 130,  33, 237,\n",
            "         32,  65, 174,  53, 107, 394, 110,  75, 185, 184,  15, 442,  75,  43,\n",
            "        440, 393, 397,  11, 217,  47,  45, 162, 197,  80,  83,  42,  27, 326,\n",
            "        488,  40,  99, 333,  73,  58,  13, 332, 236,  99,  20, 103,  45, 387,\n",
            "        407, 379], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  69,   31,  150,   33,   28,  106,   82,  484,  326,   91,  850,   82,\n",
            "         594,  103,  402,  209,   51,  111,  107,   91,  254,  112,   57,   82,\n",
            "          48,  281,  525,   79,  399,  145,  123,  644,  242,  200,   59,  327,\n",
            "         150,  302,   79,   41,   25,   75,  306,   51,  246,   58,   55, 1300,\n",
            "         322,   66,  339,  532,   20,   36,  118,   47,   84,  457,  178,  687,\n",
            "         201,  259,  852,  239,  381,  594,  584,   22,  199,  110,  226,  133,\n",
            "         120,   46,   64,   56,  181,  125,   67,  372,  472,  122,  113,   91,\n",
            "          63,   21,   44,   62,  250,   40,  134,  140,  294,  170,  150,  612,\n",
            "         278,  331,  134,  210,  269,   69,  656,   84,   19,  249,  192,  290,\n",
            "          97,  119,  375,  158,  702,   38,   57,  235,   36,  524,  122,  224,\n",
            "         615,   92,  131,  672,  251,  365,  216,   61], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  13,  197,  666,   95,  264,  212,   99, 1025,  181,  261,  830,   65,\n",
            "         110,   73,  884,  210,   51,   19,  192,  102,   36,  340,   18,  477,\n",
            "         170,  660,   81,   88,  112,   43,   94,   87,  491,  167,  111,  180,\n",
            "         194,   52,   25,   49,  321,  314,  377,  552,  322,   26,   42,  109,\n",
            "         247,   39,   21,  138,  119,   25,  131,  121,  654,   74,  236,   87,\n",
            "         120,   44,   28,  222,  669,  485,  123,  125,   50,  108, 1577,  623,\n",
            "         212,  157,   40,  295,  252,  163,  109,  104,  490,   59,  199,  479,\n",
            "         253,  539,   61, 1093,  238,   53,  243,  256,   74,   20,  101,  551,\n",
            "          78,   31,   80,  394,  110,  190,  479,   38,   25,   15,  101,   42,\n",
            "         280,  285,  219,  177,  165,   87,   44,   51,  115,  326,   50,  331,\n",
            "         265,  366,  204,  156,  246,   37,  163,   23], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 150,  303,  331,  157, 1399,  141,  258,  199,  142,   38,  306,  110,\n",
            "         106,  341,   91,   93,   14,  140,  671,  126,  400,  593,  647,   21,\n",
            "          41,  462, 1854,  573,   57,   92,  220,  331,   31,   43,  117,  687,\n",
            "         309,  391,  270,   20,    9,   25,  230,   76,  126,   99,  294,   35,\n",
            "          36,  234,  147,  441,  515,   28,  258,  273,  141,  288,   97,  128,\n",
            "         311,  420,  171,  308,  182,  373,  206,  266,   28,  257,  205,   34,\n",
            "         434,  416,   12,  142,  319,   48,  100,   24,  139,  728,  118,  216,\n",
            "         613,   84,  182, 1194,  519,  313,  491,   80,  154,   20,   16,   67,\n",
            "          56,  142,   53,  125,   87,  316,  185, 1318,   23,  306,  250,  189,\n",
            "         117,   28,   49,  201,  453,  717,   84,  375,  328, 1275,   26,   39,\n",
            "          38,  148,  939,  286,   57,   57,   71,  129], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 104,   96,  418,  403,   39,   21,   49, 1129,  102,  109,  103,  172,\n",
            "         111,  296,  100,  235,  487,  106,  542,  140,  238,   10,  410,   73,\n",
            "         392,  174,  102,  239,   36,  799,  196,  135,  159,  477,   31,   87,\n",
            "         662,   65,  367,  175,  696,  340,  574,  240,  359,  182,  336,  199,\n",
            "         204,   44,  435,   38,   67,  414,  573,   39,  112,   33,   48,   53,\n",
            "         192,  121,  396,  694,  443,  265,  540,  467,   70,  185,   43,  295,\n",
            "          99,  121,  118,  218,  187,  420,   46,  421,  237,  152,   13,  321,\n",
            "         170,  500,   60,  856,  127,   61,  412,  105,   40,   80,  499,   74,\n",
            "         328,   66,  195,  273,   72,  211,   45,  267,  242,   24,  108, 1018,\n",
            "          51,  175,  109,   25,  498,  132,  437,   42,   49,   74,  163,   97,\n",
            "          56,   23,  275,  272,  363,   16,  198,  127], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 318,   53,   20,   32,  162,   27,  138,  318,  179,  241,  222,  258,\n",
            "         246,  245,  169,   38,   39,  134,  396,  178,   12,  178,   25,  527,\n",
            "         183,  539,  151,  325, 1059,  111,   58,   57,   63,  556,  156,   82,\n",
            "         253,  116,   74,  197,  102,  338,  590,  763,   70,  129,   21,   93,\n",
            "          35,   16,  345,  156,  119,  470,   43,   39,  316,  108,  107,  212,\n",
            "         298,  479,  255,   45,  161,  228,  213,   50,  138,  189,   44,  278,\n",
            "          83,   42,   64,  270,  149,  201,   90,   95,  663,   64,  347,   57,\n",
            "         137,  404,   97,  176,   98,   87,   59,  407,  154,  766,  309,    9,\n",
            "          67,  127,   63,   14,  201,   31,  116,  273,  277,  332,  112, 1448,\n",
            "          83,   71,  140,   16,   80,  184,  425,  772,   89,  114,   55,  538,\n",
            "         183,  134,  140,   57,  375,  291,  239,  233], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 50, 374, 249,  45, 240,  21,  35, 376, 312,  64, 182, 419,  16, 110,\n",
            "        486, 111, 116, 167, 331, 152,  43, 519,  49,  61, 309, 237, 170, 399,\n",
            "        245,  33, 124, 340,  15, 154, 143,  51,  55,  88, 661, 135,  33,  35,\n",
            "         16, 219,  21, 265,  79,  89, 727, 160,  91, 306, 271, 339, 341,  93,\n",
            "        119,  69, 499, 411, 151,  97, 369, 242, 192, 718,  62,  63, 123,  23,\n",
            "         12, 295,  45, 131, 267,  45, 129,  59, 134,  55, 226,  61, 195, 278,\n",
            "         20, 245, 158, 196, 169, 144,  98,  32, 148, 335, 231, 162,  64, 434,\n",
            "        314, 243,  31, 526,  75, 462,  61, 343,  48, 129, 139, 936, 159,  80,\n",
            "        299,  74,  81,  73, 512,  59,  66, 532, 232, 171, 123,  56,  56, 111,\n",
            "         96,  97], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 196,  132,  162,  105,   17,   38,  638,  161,   21,  258,  130,  387,\n",
            "         300,   64,  104,  452,   78,   39,  198,  136, 1027,   44,   47,   72,\n",
            "          47,   25,  350,  308,   22,  294,   12,   68,  178,  178,  137,  172,\n",
            "         516,  136,  260,  161,  230,   29,   80,   56,  101,  443,  232,  147,\n",
            "        2141,  253,  308,  105,  455,  109, 1054,  274,   84,  487,  106,  215,\n",
            "          20,  417,   70,  152,  235,  225,  112,   60,   82,  299,   38,   96,\n",
            "         885,  228,  461,  546,  182,   95,  280,  266,   22,   37,   20,   73,\n",
            "         283,  284,   85,  297,  241,  630,   53,   47,  453,   74,  401,  303,\n",
            "         225,   49,   39,   92,  105,   44,   21,  899,  130,  140,  441,  399,\n",
            "         220,   61,  742,  219,  182,  228,  143,   83,  179,  170,   60,   32,\n",
            "          29,  139,   25,  325,   85,  512,   32, 1030], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  57,   27,  276,   48,  201,  568,   61,  775,  296,   92,   19,   29,\n",
            "         179,  350,  348,  462,  545,  228,   54,   89,  477,  714,   76,  467,\n",
            "         629,  117,  295,  468,   85,  176,  301,  543,   39,  804,  598,  103,\n",
            "          26,  599,  146,  200,  116,  155,  371,  201,  287,  136,  581,  185,\n",
            "          31,   73,   74,   32,  392,  167,  152,  198,   51,   65,  472,  405,\n",
            "         193,  278,  197,  374,  481,   91,  169,  155,   83,  161,   32,  173,\n",
            "          54,  254,  106,  202,   41,   68,  130,  180,   27,  319,   18,   49,\n",
            "         524,   11,  170,  475,   70,   38,   38,  237,  562,   18,  659,  118,\n",
            "         468,  758, 2022,   80,  335,   60,  607,  224,  211,  292,   81,   82,\n",
            "         122,   42,   62,  383,  291,  302,  153,  279,  122,  776,  223,  278,\n",
            "         401, 1208,  143,  117,  322,   90,  376,   89], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 161,  273,  111,   36,  146,  116,   50,   31,  360,  447,  394,  719,\n",
            "          73,  325,  217, 1036,  206,  111,   47,  280,  140,  431,   83,  665,\n",
            "         242,   98,   66,  524,  451,  368,  131,   13,   59,  824,   40,  157,\n",
            "         284,   39,  305,  124,  142,  705,   57,   91,  195,  511,  317,  525,\n",
            "         121,  319,   58,   53,  597,   29,  240,  301,   12,   75,  231,  145,\n",
            "         132,   40,   50,  137,  152,   20,  169,  121,  442,  649,   38,  640,\n",
            "         103,  244,  719,  110,  168,   56,   66,   75,   26,   64,  179,  217,\n",
            "          92,  143,  189,   89,   59,  191,   29,  571,   31,   99,  132, 1277,\n",
            "          59,  374,   25,  182,  192,  295,   89,  163,  160,  552,   70,  166,\n",
            "         290,  164,   61,  364,  544,  202,  130,  234,  186,  396,   55,  316,\n",
            "          75,  289,  262,  481,  245,   50,  141,  670], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 592,  713,  392,   19,   44,   14,  664,  255,   87,  277,  294,  646,\n",
            "          71,   30,  475,  166,  254,  227,  371,  281,   26,   15,  250,  171,\n",
            "         356,   22,  515,  102,   95,   88,  588,   39,  712,   38,  181,  297,\n",
            "          66,  255,   35,   66,  187,  187,  255,  139,   66,  155,  132, 1496,\n",
            "         147,  198,  135,   61,  600,  300,   73,  386,   43,   72,   12,   95,\n",
            "          17,  160,  237,  211,  433,  258,   65,   14,  300,  116,  354,  322,\n",
            "          99,  399,  270,  169,  195,  149,  364,  588,  118,  486,   60,   39,\n",
            "         201,  160,  744,   46,   51,  178, 1072,   39,   52,  118,   31,  825,\n",
            "         405,   69,   45,   53,   77,   66,  352,  396,  221,   97,  183,   55,\n",
            "         687,  155,   22,  120,   29,   57,  607,  575,   49,  179,  184,  235,\n",
            "         164,   57,  385,  293,  143,  306,   73,   13], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 475,  125,  153,  260,  284,  267,  458,   28,  121,  268,   87,   20,\n",
            "         189,   33,  571,  339,  281,  132,  311,  385,  134,   28,   69,  262,\n",
            "          50,   45,   86,  265,  460,   34,   42,   98,  195, 1008,  874,  344,\n",
            "         163,  689,   81,  202,  936,  617,  764,  236,  197,  366,  108,   81,\n",
            "        1108,   85,   54,  342,  266,   69,   92,  238,  208,  120,   40,  355,\n",
            "         591,  116,   78,  656,  305,   14,  340,  890,   86,  439,   49,  433,\n",
            "         196,   57,   50,   45,   16,  236,  217,  578,  513,   28,  189,  718,\n",
            "          52,  206,   84,   30,  132,  459,  391,  569,  220,   94,  137,   49,\n",
            "         128,  173,  177,  214,  208,  175,  235,   21,  111,  397,   65,  198,\n",
            "         175,   49,  203,  944,   13,  219,  280,   48,   31,  162,   47,  395,\n",
            "         335,  209,   65,  241,   42,  182,  217,  317], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 94, 183, 689, 180, 221, 146,  44, 349, 216, 696,  66, 255,  72, 633,\n",
            "        476,  17, 206, 166, 363,  41,  70, 365,  62, 104, 200,  29,  75,  21,\n",
            "        132, 361, 448,  60,  68, 100,  57, 215, 253, 198,  65, 141,  47, 434,\n",
            "        433, 169, 129, 124, 388, 185, 335, 183, 141, 534, 546,  26,  91, 342,\n",
            "        102,  46,  94, 330, 139, 637,  76,  34, 208, 134, 277,  83, 764,  32,\n",
            "         74,  61, 244,  37, 147,  57,  74, 637, 221, 427, 316,  72, 171,  50,\n",
            "        100, 226,  68, 149,  70, 208,  19,  39, 245, 197,  91, 508,  41, 769,\n",
            "        374, 213,  61, 422, 860, 103, 323,  63, 149,  39, 261, 335,  56, 210,\n",
            "         80, 192, 144,  46, 236,  70,  97, 252, 397, 135, 400, 450, 170, 150,\n",
            "         99, 302], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 182,   50,   64,  260,  303,  303,   30,  198,  208,  131, 1142,   74,\n",
            "         193,   31,   37,   84,  402,   68,  384,  258,   78,  133,  218,  340,\n",
            "         120,  532,  963,   70,  341,  251,  486,   56,  503,  422,  107,   52,\n",
            "          20,   70,  412,   52,   83,  214,   55,  198,   50,   76,   68,   57,\n",
            "         126,  222,  211,   37,  335,  126,  345,  351,  707,   74,   23,  254,\n",
            "         376,  243,   89,   92,  301,   81,  100,  169,  549,  149,  117,   79,\n",
            "          78,  253,  195,  504,   57,   71,  253,  202,   73,  185,   84,   91,\n",
            "         574,   61,   19,   86,   14,   66,   63,  219,  549,  102,  333, 1925,\n",
            "         104,   58,   25,  389,  407,   70,   80,   70,  112,   42,  103,  212,\n",
            "         523,  424,   16,   89,   69,  157,  150,  361,   50,   26,  146,   28,\n",
            "          39,  937,   99,  199,  120,   88,   90,   15], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  77,   58,  529,   31,  105,   64,  188,  776,  258,  521,   60,   36,\n",
            "         241,   98,   38,  419,   40,  167,  500,   74,  136,  174,  281,  120,\n",
            "         430,  435,   72,  206,   28,  265,   20,  154,  509,  336,  261,  144,\n",
            "         384,   67,   12,   21,  408,   25,  247,   54,  295,  131,  302,  678,\n",
            "          34,  249,   54,  261,  111,   52,   60,  192,  243,   65,   92,  536,\n",
            "         164,  381,   20,  591,   60,  313,  251,   69,  326,   34,  107,  566,\n",
            "          61,  263,   96,  186,  409,   41,  215,  141,  277,  124,  117,  814,\n",
            "         253,  238,  126,   94,  343,  145,  128,   37,   72,   41,  138,  153,\n",
            "         117,  383,   37,   48,   54,   36,  631,  732,   15,   63,  120,  456,\n",
            "         106,  144,  152,   17,  255,  402,  165,  189,   42,  108,  343,  206,\n",
            "         191,   48,   25, 1120,  188,  271,  166,  126], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 312,  276,   85,   76,  116,  819,  122,  205,  462,  161,  304,   19,\n",
            "         105,   46,   27,   39,  539,  216,  452,   91,  350,  445,   82,  384,\n",
            "         348,  253,   88,   76,   37,  203,   95,   44,   12,  133,  409,   70,\n",
            "         724,  179,  321,  120,  632,   41,   15,  289,   45,  211,   98,  355,\n",
            "         106,   54,  185,  109,   85,  279,  200,  291,   67,   45,  281,  218,\n",
            "         130,  161,  191,  643,  158,   32,   61,  312,   11,   80,  165,  476,\n",
            "         147, 1068,  106,   44,  346,  105,  145,  135,   81,  169,   20,   91,\n",
            "         233,   84,  289,  143,  112,   49,   47,  273,  110,   58,  123,  236,\n",
            "          44,  261,   34,  451,  684,   22,  871,  252,   31,   26,   23,  162,\n",
            "         106,   57,   26,  107,  216,  154,   15,  102,  209,   61,  454,  118,\n",
            "         475,   68,  485,   11,  426,   10,  250,  247], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 314,  271,   95,   80,  197,  167,  286,  152,   32,  447,  741,  229,\n",
            "           7,  282,  292,   62,  401,  284,  254,  405,  541,  357,   68,  364,\n",
            "         585,   50,   52,   70,  120,  192,  474,  259,  644,  334,  124,   44,\n",
            "         264,   44,   13,  104,   33,   67,  655,  187,  318,  205,  350,   10,\n",
            "         252,  241,  178,  874,   74,  358,  144,   66,   22, 1166,   38,  291,\n",
            "          34,  180,   22,  107,  294,   54,  238,  477,  405,   57,   41,   50,\n",
            "          38,  257,  542,  148,  168,   39,   23,   91,   25,  174,   86,  124,\n",
            "          21,  519,   34,   94,  514,  111,  139,   25,   73,  285,  241,  254,\n",
            "          79,  110,  240,  194,  108,   80,  422,   89,  109, 1261,   40,   43,\n",
            "          30,  160,  194,   57,  171,  212,   89,   83,  167,  219,  213,  277,\n",
            "          52,  236,  266,  102,  130,   35,  159,   45], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 127,  116,   86,  205,  111,  324,    8,   47,  151,   50,   98,   80,\n",
            "          39,  250,  118,  103,  968,  157,  507,   84,   75,  138,  387,  190,\n",
            "          65,   62,  390,   51,  191,  162,  115,  427,  815,  370,   96,   89,\n",
            "          95,  100,  480,   65,  336,  345,   50,   25,  282,  463,  199,  361,\n",
            "         553,  138,   51,   68,   48,  140,  151,   44,  135,  187,   61,  253,\n",
            "          74,  335,   51,   73,  773,   52,   63,  582,   92,   15,  360,  320,\n",
            "         372,  136,   32,  440,  132,   21,   40,  139,  330,  456,  254,   83,\n",
            "          68,   22,  196,  252,   93,  123,  454,   62,   80,  180,  411,  148,\n",
            "        1137,   68,  280,  308,  157,  709,   54,  616,  141,  295,  295,  151,\n",
            "         478,   53,  122,   22,  343,   52,  127,  610,  143,   27,   85,  163,\n",
            "          60,  386,  333,  231,  211,  539,  181,   56], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "Epoch: 001/002 | Batch 040/080 | Cost: 0.4023\n",
            "tensor([ 256,   84,  423,   71,  396,  302,   50,   74,  111,  353,  309,   35,\n",
            "         260,   32,   33,  221,  173,  959,   84,   31,   49,   31,  168,  166,\n",
            "         409,  311,  145,   82,  582,  116,  159,  649,  123,   72,   63,  456,\n",
            "         218,    8,   85,  116,  181,  113,   68,   13,  122,  148,   91,   21,\n",
            "          41, 1759,  181,  327,  241,   53,  151,   23,  112,   52,  321,  339,\n",
            "          71,  360,  407,  211,  191,   77,  374,  793,   88,  250,  179,  190,\n",
            "          81,  294,   69,  949,  297,   64,   42,   65,  423,   46,   72,  167,\n",
            "         215,   93,   24,  120,  996,  702,   65,   66,  308,   63,  157,   62,\n",
            "         408,  477,   51,  354,  303,  276,  257,   40,  158,  198,   27,  336,\n",
            "         488,  319,  226,  536,   90,  330,  344,   56,  594,  352,   88,  245,\n",
            "         779,  265,  208,   41,  332,  179,  238,  678], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 332,  449, 1293,   59,   78,  197,   47,   45,   82,   70, 1467,  107,\n",
            "         949,   50,   27,  163,   43,  234,  161,  575,  292, 1511,   52,  201,\n",
            "         415,  254,   48,   78,   81,   87,   85,  132,  264,  255,   56,  283,\n",
            "         266,  120,  334,   19,   68,  165,  401,  738,  129,  249,  312,  128,\n",
            "         263, 1243,  222, 1083,   41,  376,   92,  203,  136,  358,   66,   96,\n",
            "         192,  394,  146,  515,   25,  512,  849,  690,   87, 1493,   73,  387,\n",
            "         429,   87,  622,   77,   48,   42,  125,  546,  251,   41,   89,   48,\n",
            "          63,   30,  674,  627,  342,  104,  221,  172,   64,  166,  251,  162,\n",
            "          71,  471,  240,  365,   36,  189,  151,  369,  112,   70,   63,  189,\n",
            "         280,  231,  186,   82,   44,  323,  144,  212,  192,  348,   49,  430,\n",
            "          44,  636,  149,  395,  107,   24,  157,   40], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 328,   30,  460,  276,   80,  212,   72,   96,   68,   53,   25,  205,\n",
            "         391,   67,   75,   54,  313,   75,  307,   63,  189,  312,   62,   43,\n",
            "           9,  128,  324,  286,  158, 1022,   85,  742,  182,  219,  108,  166,\n",
            "          78,  215,  230,   56,  327,  109,  500,  178,  311,   44,  123,  141,\n",
            "         137,   90,  621,   45,  267,   64,   23,  165,  111,  504,  198,   63,\n",
            "         192,   37,   82,  148,   23,  545,  181,  405,  194,   49,  116,   53,\n",
            "         274,  361,  556,  112,   38,  445,   68,  617,  354,   26,  246,  187,\n",
            "          81,  249,  351,  539,  164,  153,  579,   72,  260,   59,  291,  124,\n",
            "          77,  694,  212, 1466,  344,  155,  115,  297,  261,  247,   25,  109,\n",
            "          62,  757,  106,   71,  129,  245,   66,  246,   71,   94,  102,   80,\n",
            "         418,   27,  156,   65,  158,  193,  944,  153], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  36,   86,   51,  616,   69,  126,  370,  132,   40,  447,   84,   58,\n",
            "         102, 2856,  236,  249,   54,  310,  110,   31,   61,  320,   67,  325,\n",
            "          66,   61,  226,  111,   99,   83,   98,  333,   61,  218,  406,  166,\n",
            "          92,  151,  214,   60,  108,  166,  215,  432,  775,  376,  145,  392,\n",
            "          34,  153,   62,   13,   34,  123,  188,   40,   86,   47,  447,  152,\n",
            "         139,   41,   83, 1111,  127,   55,  262,  191,   76,   75,  214,  540,\n",
            "         234,   21,  254,  244,  123,   33,   39,  229,   25,   56,  217,  315,\n",
            "          74,   70,  200,   88,  389,   63,   90,  325,   88,   90,  433,   97,\n",
            "          84,   41,   93,   29,  167,   31,  245,   62,  262,   58,  130,   48,\n",
            "          71,   92,   30,  113,  224,  522,    9,   16,  203,  447,   75,   10,\n",
            "         218,  186,  350,  167,   82,  248,  262,  221], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 303,  101,  380,  300,   58,  567,   83,   19,  128,  221,   53, 1714,\n",
            "          38,   95,  409,  363,   54,   84,  712,  798,  303,  389,  101,   28,\n",
            "          41,  148,   50,   41,  105,   51,  526,  282,  355,  125,  268,  145,\n",
            "          91,  363,  281,  417,  492,   40,  593,   63,   36,  231,  331,  144,\n",
            "          75,  502,  118,   98,  172,   51,  430,  565,   78,  298, 1444,  433,\n",
            "         157,  319,  368,  308,   54,  167,  315,    8,  738,  286,   44,  175,\n",
            "        1261,  145,  402,  235,  174,  223,  607,   51,  354,   37,  225,   39,\n",
            "         161,   29,   32,  168,  274,  169,  245,  167,  102,   91,  161,  234,\n",
            "         231,   41,   78,  198,   21,  190,  166, 2670,  424,   63,   73,  330,\n",
            "         305,   79,  572,   39,  788,   43,   38,  318,  357,  278,  116,   43,\n",
            "         401,  308,  379,  260,  194,  161,  420,  863], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 278,  637,  286,  328,   44,   46,  218,   55,  596,   23,   26,   47,\n",
            "         108,  209,  239,   41,   43,  404,  131,  679,  108,  758,  206,   55,\n",
            "          91,   49,  528,  226,  182,  447,   50,  356,   38,   52,  114,   46,\n",
            "          78,   37,   36,  655,  235,  159,  217,  362,  155,  105,   38,  274,\n",
            "        1123,  229,  181,  189,  108, 1072,   92,   53,  289,  422,   29,  109,\n",
            "         302,   42,   82,  202,   10,  430,  543,  206,   83,  144,  131,  279,\n",
            "          15,  513,   56,  156,  352,  187,   76,  369,   61,  290,  166,  364,\n",
            "         594,  119,  226,   65,   12,   44,  855,   95,  933,   58,  160,   83,\n",
            "         252,   44,   64,  303,  171,  250,   76,  208,   57,  228,  102,  120,\n",
            "         105,   44,   60,  133,  161,  533,  265,  282,   32,  119, 1514,   46,\n",
            "         360,   77,  299,   49,  245,   77,   99,   84], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  27,  110,   41,  441,   75, 1741,   31,  138,  164,  124,   32,  388,\n",
            "          20,  150,  319,  128,  766,  495,  954,  195,   68,  145,  520,   72,\n",
            "          90,   53,  166, 1033,  745,  756,  292,   80,  153,  422,   20,   94,\n",
            "          44,  545,  112,   77,   32,  350,  527,  255,  161,  394,  646,  262,\n",
            "         403,   90,   93,  213,  362, 4250,  296,  253,  219,  206,  192,  615,\n",
            "          79,  676,  289,  152,  367,   69,  585,   38,  472,  141,   57,  262,\n",
            "          51,  955,   31,  195,   68,  181,  108,  199,   89,  183,  670,   68,\n",
            "         156,   91,   41,   87, 1086,  115,   73,   48,  399,   78,  297,   42,\n",
            "         204,  102,  349,   15,   93,   81,  123,  338,  245,  320,   91,  117,\n",
            "         334,   26,   27,  556,   44,  143,   94,  115,  978,   36,  127,   34,\n",
            "         325,    9,   35,   28,   37,  131,  146,   16], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 238,  145,  293,  542,  372,  224,  380,  192,   69,   74,  519,   36,\n",
            "         409,  252,  283,   19,   65,   85,   80,  286,  427,  198,  193,   17,\n",
            "         130,  370,   64,   21,  261, 1200,  101,  184,   31,   17,   74,  144,\n",
            "          89,   31,   64,  431,   41,  274,   98,   22,  235,  207,  118,  199,\n",
            "         301,   35,   97,  246,  114,  215,   24,   75,  250,  178,  288,  404,\n",
            "         179,  152,  385,  516,   56,   67,   59,  320,  112,   68,   55,  361,\n",
            "         317,  234,  114,  568,  572,  188,  279,  314,  357,  576,  191,  453,\n",
            "         177,  224,  146,   51,   72,  137,  156,  182,   70,  208,   95,  399,\n",
            "          20,  208,   73,   52,   11,   69,   87,   58,   76,  346,   59,  448,\n",
            "         165,  158,  140,  115,  250,   75,   83,  112,  203,  128,  400, 1649,\n",
            "          55,   78,  120,   35,  237,  126,   87,   27], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  87,  406,   19, 1304,  147,  172,  146,  134,  231,  344,   58,  107,\n",
            "         178,  169,  244,  487,  259,  445,  136,  568,  646,   97,   47,   52,\n",
            "          81,  249,   56,   37,  207,  350,  106,  100,  138,  357,  101,  370,\n",
            "         668,   76,   37,   89,   59,  133,  354,  754,  138,   43,  494,  119,\n",
            "          59,   47,   39,   27,  335,  370,  202,   48,   83,   17,   89,  100,\n",
            "          23,  182,  145,   14,   37,   58,   57,   45,  219,  111,  558,  595,\n",
            "          42,   92,   79,   49,   50,  148,  258,   87,   91,  374,  409,   72,\n",
            "         442,  277,   39,   39,   90,  168,   62,  320,  353,  540,  311,  267,\n",
            "         123,  746,  884,   64,  678,  186,  188,  211,   40,   37,  121,  437,\n",
            "         268,  182,  132,  269,  175,   40,  743,   33,  407,   95,  304,   53,\n",
            "         432,  395,   63,  492,   81,   56,   73,   25], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  91,  256,  487,   63,  164,  120,   79,  198,  265,   71,  462,   56,\n",
            "         249,  391,   99,   60,  145,   26,   95,  222,  142,    9,   48,   48,\n",
            "          49,   91,  508,  281,   59,   31,  370,  168,  209,  127,   79,   54,\n",
            "          97,  356,  301,   90,  207,  201,  565,  390,   73,  234,  559,   25,\n",
            "         219,  106,   77,  168,  435,  488,   52,  127,  876,   69,   49,   26,\n",
            "          48,  855,   98,  206,  372,  579,   88,   69,   12,  878,   79,   30,\n",
            "         122,  111,  224,   66,  294,  566,  360,  119,  360,   35,   58,   95,\n",
            "          22,  537,  108,  288,  352,  131,   12,  300,   29,   99,   50,   36,\n",
            "         200,  137,  250,  300,  217,  414,   83,  285,  157,  317,  198,   40,\n",
            "         196, 1036,  112,   44,   75,  127,  429,  197,  159,   67,   74,  182,\n",
            "          17,   83,  987,  254,   22,  527,   48,  445], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([173,  44, 224,  88, 584, 171, 832,  73, 537, 214, 131,  79, 288,  11,\n",
            "         26, 131,  99,  37, 240,  70,  66,  31, 138, 175,  67, 152, 110,  26,\n",
            "         61, 286, 390,  54, 324, 304, 178,  51, 127, 135,  23, 116, 390,  68,\n",
            "        332, 109,  38, 200,  19,  73, 144, 121, 174,  95, 299, 333, 234,  45,\n",
            "        179, 219, 138, 183, 128, 403, 184,  85, 189, 282, 405, 298, 147, 366,\n",
            "        172, 392,  28,  94, 546, 123, 396, 180,  97, 259, 165, 211, 220, 111,\n",
            "        402, 111, 205,  18, 267, 281, 132, 165, 275, 373,  67, 233, 386, 176,\n",
            "         14, 671,  31, 111,  72, 461, 499,  21,   8,  83,  72, 198, 711, 953,\n",
            "         82,  45, 180,  83, 174,  36, 110, 227, 115, 106,  72, 239, 318, 254,\n",
            "         50,  70], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  18,  285,  358,   44,  289,  355,  118, 1253,  214,   15,   19,  206,\n",
            "          28,   71,   31,  359,  205,  239,  255,   47,  542,  319,  287,  250,\n",
            "          31,  167,  602,  605,  411,   85,  303,  177,  129,   31,  257,  103,\n",
            "          92,   63,   69,  422,  305,   40,   46,   99,   19,   62,  321,   85,\n",
            "         512,  158,   94,  472,  318,  140,  439,   70,  152,   55,  112,   54,\n",
            "        1337,   32,  823,  217,   72,   79,  161,  278,   42,  128,  643,  347,\n",
            "          13,   66,  615,  339,  343,  446,  183,   85,   49,  183,   61,   32,\n",
            "          53,   66,   78,  206,  213,  320,  434,  427,  157,  253,   65,  772,\n",
            "         478,   54,  206,  261,   22,   62,  775,  137,   73,   26,  281,  220,\n",
            "         517,  235,  203,  235,  389,  590,  102,  526,  123,  254,  145,  231,\n",
            "         230,  239, 4065,   60,  201,  361,  210,   74], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([173, 155, 382, 165,  66, 132,  61, 395, 189, 114, 335, 125, 681, 403,\n",
            "        311,  12,  96,  86, 486,  91,  76,  99,  64, 127, 270, 102, 515, 103,\n",
            "         52, 303,  66, 248,  28, 119, 104,  20, 615, 100, 910,  29, 111, 348,\n",
            "        103,  62, 202,  31, 103,  93,  54,  92, 603, 348,  63, 152,  77,  41,\n",
            "        201,  75, 430,  79,  30,  78,  43,  15, 776, 444, 218, 138, 297, 103,\n",
            "         77, 137,  40,  60, 488, 133, 246, 535, 238, 248,  11,  51, 556, 464,\n",
            "         56, 392, 112, 525,  36, 173,  39,  48, 134, 223,  94, 445, 159, 174,\n",
            "        356,  73,  42,  70,  38, 150,  99,  71, 210,  19, 321, 326, 204,  54,\n",
            "        206, 498, 311,  20,  51, 102, 231,  62, 166, 245,  49,  85,  33, 683,\n",
            "         55, 586], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([   9,  728,   31,   44,   98,   63,  221,   79,  386,  159,   67,  123,\n",
            "         237,  358,  192,   42,   87,   83,   42,  171,  549,   49,   38,  452,\n",
            "          96,   38,  160,  119,  143,  121,  147,  117,   76, 1241,   18,  434,\n",
            "          62,  147,  127,   42,  250,  134,   92,  324,  101,  294,  262,  274,\n",
            "        1111,   39,  141,   71,  277,   63,   57,   53,  238,  297,  444,   24,\n",
            "          28, 1015,  407,  441,  394,  142,   82,  522,   31,  179,   49,  331,\n",
            "         117,  103,   12,   86, 1169,  153,   81,  144,  160,   28,   38,  293,\n",
            "         130,  142,  367,  133,  206,  550,  559,  528,   56,  161,  160,   30,\n",
            "         438,  322,  405,   50,  303,  142,  188,  240,  173,   48,  125,   32,\n",
            "         510,  401,  424,   37,   55,   89,  156,  273,   73,  742,  144,   48,\n",
            "         539,  150,  113,   23,  584,  109,   28,   38], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 135,  326,  115,  179,  527,  226,   58,  141, 1240,  152,   95,   87,\n",
            "         832,  221,   62,  138,   32,   19,   65,  301,   25,   33,   74,  329,\n",
            "         175,  311,  118,  292,   33,   18,  285,   28,   33,   13,   90,  129,\n",
            "         197,   45,   34,   30,  936,  105,  212,  386,  109,   48,  473,   68,\n",
            "          56,  304,   77,  167,  494, 1206,   37,  419,  274,  180,  277,  552,\n",
            "         191,  214,   62,   83,   51,   33,  149,   78,  367,  312, 1265,  186,\n",
            "        2306,   58,  532,   34,  442,  495,  249,   64,   29,  220,   16,  152,\n",
            "         261,  113,  349,  865,   25,  423,  292,  139,   29,   35,  257,   12,\n",
            "         142,  357,  262,   71,  117,  536,  211,  525,  215,  718,  106,   38,\n",
            "          56,   30,   12,   64,  756,  184,   96,  253,  152,  198,  410,  285,\n",
            "         109,  217,   86,  107,   66,  677,   78,  736], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  24,  233,   63,  197,  231,   54,  420,   96,   71,   77,  246,  635,\n",
            "         118,  385,  172,   44,  518,  117,  238,  346,  297,   66,  282, 1127,\n",
            "          42,   26,  218,  120,  151,  155,   46,   99,  252,  198,  121,  121,\n",
            "         251,  260,   75,   55,  220,  230,  159,  245,   62,   55,  125,  230,\n",
            "          26,  187,  112,  387,   12,  459,  188,  317,  140,  376,  255,  144,\n",
            "          49,   15,   53,  102,  483,  371,  122,  197,  127,   20,  422,   56,\n",
            "         145,  151,   93,  289,  297,   58,   35,  104,  420,   33,   98,   73,\n",
            "          41,   11,  250,  123,  860,  123,   48,  412,   27,   91,  722,  879,\n",
            "          67,  115,  309,  151,   69,  197,  137,  285,  138,  224,  130,   67,\n",
            "          26,  122,  132,  382,  308,  118,   35,   58,  236,  302,   62,   80,\n",
            "         298,   55,  359,  678,  125,  331,   99,   93], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  46,  396,  393,   26,   33,   65,  341,  301,  177,   77,   69,   84,\n",
            "         279,  106,  212,   45,  236,  157,  233,   74,  116,  187,   77,  246,\n",
            "          63,  109,   93,  299,  158,   25,   74,  127,  218,  100,  708,  446,\n",
            "         169,  269,  169,   35,  144,   26,  322,  216,  153,   95,  444,  202,\n",
            "          92,  227,   17,  359,  163,  119,  467,   90,   70,   15,   77,   12,\n",
            "         190, 1283,  115,  296,  922,  485,  438,  129,  289,  269,   70,  182,\n",
            "         793,  124,   78,  435,   57,  449,  130,   54,  501,   58,  436,   54,\n",
            "         140,  172,  280,  232,   42,   25,  188,  481,  273,   57,   59,  876,\n",
            "         368,  346,  165,   92,   96,  376,   39,  131,   57,  397,  109,  268,\n",
            "           9,   27,  173,   74,  133,   70,   22,  161,  230,  463,  265,   57,\n",
            "         137,  714,   26,  105,  106,  173,  192,   56], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  99,  392,   39,  130,  194,  240,   96,   92,  258,   82,   48,  437,\n",
            "         327,  569,  391,  279,  107,   40,   11,  316,  322,  444,  142,   59,\n",
            "          67,  106,  271,  435,  236,   77,  140,  134,  165,  621,   48,   68,\n",
            "         191,  289,   30,  192,  110,   90,  622,   72,   72,  853,  116,   89,\n",
            "         432,  451,  282,   30,  428,   56,  191,  314,   36,   55,  443,   41,\n",
            "          24,  312,  138,  129,  283,  243,   55,  493,  249,  152,   41, 2018,\n",
            "          23,  354,   52,  158,   50,   37,  181,   62,   58,  186,  904,  227,\n",
            "          54,   59,   13,  123,  205,   79,  347,   98,  145,  126,   88,  108,\n",
            "         152,  206,  105,  796,   27,  137,   71,   43,   22,  338,  119,  350,\n",
            "         122,   74,  158,   78,   11,  161,   22,  333,   97,   49,   41,  371,\n",
            "          21,   34,  258,   39,   66,  158,   51,  138], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  21,   22,  236,   70,  341,  435,  343,  142,  107,   88,  106,  241,\n",
            "          20,   47,   66,   76,   72,   74,   30,  786,  230,  114,   88,  467,\n",
            "          95,  377,  176,  124,  219,  141,   53,  335,  196,   53,  253,  242,\n",
            "          96,  532,  470,   16,  340,  535,  176,  364,  432,   21,   47,  975,\n",
            "          27,  433,   57,  576,  267,  146,  377,   57,  203,  469,  331,  197,\n",
            "         184,   87,   35,  422,  139,   12,  133,   68,   69,  185,  346,  201,\n",
            "          29,  140,  168,   17,  207,  353,  186,  320,  145,  103,   34,   44,\n",
            "         537,   62,  303,  106,   63,   85,  669,   49,   21,   24,  128,  217,\n",
            "         182,  163,  323,   97,  306,  107,  170,   69,  121,   72,  103,  247,\n",
            "         511,  188,  413,  139,  296,   91,   14,  333,   12,  102,   18,  310,\n",
            "          79,  737,  452,   62, 1009,   89,   83,  146], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 467,  258, 1271,   82,  219,  380,  114,  282,   25,  155,  208,  209,\n",
            "          31,  957,  122,  345,  128,   31,  201,  100,  416,   83,  468,   63,\n",
            "         106,  273,   32,  229,   30,   49,  222,  591,  251,  151,  129,  829,\n",
            "         543,  234,   86,   71,  249,  140,  285,  256,  439,  203,   21,   73,\n",
            "         290,   62,  171,   53,  276,   63,  298,  231,  180,  218,   74,  273,\n",
            "          68,  507,  467,   52,   19,  190,  117,  172,  317,  181,  119,   24,\n",
            "         337,   92,  129,   88,  138,  185,   66,  330,   60,   83,   66,  141,\n",
            "          79,  236,  129,  206,  112,  115,   48,  888,  122,   61,  365,   62,\n",
            "          78,   39,   38,   71,  224,  263,  121,   90,  639,  285,  152,   18,\n",
            "          25,   81,  603,  140,   58,   48,  307,  240,   72,   46,  431, 1044,\n",
            "          33,  458,   41,  339,  198,  406,  248,  216], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "Epoch: 001/002 | Batch 060/080 | Cost: 0.3977\n",
            "tensor([ 284,   78,   77,  148,  159,   23,  298,  255,  132,  352,  639,  186,\n",
            "        1299,   42,  297,  299,  197,  131,  180,  310,   90,  110,   31,  195,\n",
            "         163,   64,   61,  163,   24,   70,  419,  548,  353,  381,   42,  152,\n",
            "          51,  388,  355,   45,  453,  676,  492,   33,  288,  160,  269,   94,\n",
            "         247,  740,  109,  350,   66,   95,  328,  404,   39,   48,  225,  662,\n",
            "         200,  450,  242,  146,  146,  569,  415,  344,   63,   77,  266,  468,\n",
            "         164,  204,   18,  282,   49,   11,  131,  876,   65,   29,   65,   51,\n",
            "         407,   76,   83,   87,   51,  623,  522,   98,  185,  639,  132,  217,\n",
            "          40,   44, 1104,   78,   90,   44,   80,  318,  338,   78,   91,   65,\n",
            "          51,   34,   69,   29,   38,   58,  151,   48,  141,   66,  277,  260,\n",
            "         376,  182,  141,  875,  320,   51,   66,   93], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  57,  149,   57,  169,   45,  137,  408,  119,  117,  257,  329,  150,\n",
            "         302,  178,   89,  254,  151,   25,   72,   79,   56,  805,  411,  140,\n",
            "          94,  844,   35,   84,  580,   36,  118,   79,   88,   68,   52, 1035,\n",
            "         218,   89,  238,   98,   84,  343,  230,  169,  132,  633,  389,  111,\n",
            "          83,  622,   24,  376,  568,  269,   58,  213,  138,  259,  308,  840,\n",
            "          98,   66,  317,  127,   59,  230,  196,  183, 1382,  101,   71,  291,\n",
            "         854,  136,   76,   94,  226,   19,  478,  238,  811,  159,  157,  147,\n",
            "         148,   23,  145,  293,  169,  157,  180,  110,  236,  134,   44,  167,\n",
            "         208,  272,   11,   52,  211,   40,  203,  101,  281,  132,   55,   44,\n",
            "         405,   95,  151,   71,  115,  151,  501,  286,   90,  522,   27,   50,\n",
            "         307,   56,  161,  499,   80,  574,  183,  239], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([166,  33, 724, 366,  47, 157, 193,  61, 306,  54, 195, 345, 104, 200,\n",
            "         74,  31,  29, 106, 402, 122,  91, 116,  95, 609,  54, 280,  55, 227,\n",
            "        375, 319,  85, 748, 597, 284,  18, 431, 115,  31, 138, 103,  40, 536,\n",
            "         39,  79,  59,  39,  28, 778, 258,  24,  23, 159,  12, 197, 393, 123,\n",
            "         36, 278, 494, 241, 493,   9, 576, 476, 254, 495, 230, 250, 106,  14,\n",
            "         12, 400,  66, 152,  50, 301, 194,  87, 218, 202,  77, 212,  90, 487,\n",
            "        791, 312, 105, 177, 107, 337, 179,  86, 137,  62,  96,  64, 101, 228,\n",
            "        208, 177, 377,  11,  59, 145,  74, 194, 550, 451, 147,  62,  75, 193,\n",
            "         33, 468, 190, 145, 107,  94, 198, 154, 268,  29,  10,  27, 115, 559,\n",
            "        153, 383], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  53,  143,  175,   86,   46,   38,   45,  612,  462,   63,   27,  243,\n",
            "        1517,   18,  853,   33,   11,  575,   89,  401,  403,   25,  237,  204,\n",
            "          75,  106,   69,  159,  162,   79,  122,   81,   30,  113,   21,  419,\n",
            "          12,  149,   55,  220,   89,  126,   42,  131,  198,  254,  539,   44,\n",
            "         365,  108,  203,   52,   10,  216,  124,  240,  287,  121,  629,  544,\n",
            "         397,  117,  113,  140,  290,   94,  231,   21,  961,   51,  158,   89,\n",
            "         293,  118,   20,  243,  686,  273,   99,   20,  282,   15,   41,  412,\n",
            "         211,  711,  146,   50,  670,  302,  696,   38,  154,  317,  152,  284,\n",
            "          51,  530,   67,  264,   81,  772,   82,   70,  497,   10,  290,  157,\n",
            "         110,  324,  314,  107,   70,   74,   77,   12,  121,   72,  112,  186,\n",
            "         143,   40,   37,  320,  145,  145,  233,   84], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 445,  141,  192,  263,  210,  417,   38,  872,   75,   46,  267,  907,\n",
            "          74,   66,  198,   76,  114,  209,  386,  160,   63,  620,  319,  173,\n",
            "          41,   29,   50,  686,  104,   75,   80,  183,  581,  119,  194,  392,\n",
            "         581,  740, 1024,   56,  762,  199,  154,  389,   87,  426,   16,  193,\n",
            "         673,  344,  520,  299,   75,   37,  109,  213,   62,  264,   92,   59,\n",
            "         739,  510,  293,  246,  394,  269,  137,  292,  291,  103,  154,   46,\n",
            "          38,  334,  187,  458,  938,   41,  337,   98,  131,  484,  111,  121,\n",
            "         121,  425,   54, 1035,  105,  223,  594,  429,  314,   29,   24,  491,\n",
            "         198, 1886,  364,  245,  185,  265,  161,  193,  152,   30,   25,  259,\n",
            "           9,  394,  242,  230,  324,  253,  228,   96,  101,  486,   91,   24,\n",
            "         287,  164,  510,  280,  875,  247,  347,  860], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 581,   80,  623,  917,   92,  184,   70,  942,  233,  137,  211, 1421,\n",
            "          70,  255,  589,   39,   37,  273,  157,  614,   43,  727,   33,   62,\n",
            "         182,   91,   59,  136,  444,  339,  229,  284,   48,   43,   83,   19,\n",
            "         538,   50,  178,  203,  204,  160,  170,   48,  132,  114,   85,   59,\n",
            "         410,   22,   32,  153,   63,  229,   24,   95,   32, 1072,   83,  134,\n",
            "         177,  336,  391,  269,   55,   27,  632,   46,  115,  531,  265,  286,\n",
            "         123,  147,  221,  224,   43,   20,  530,   69,  303,   83,  194,  110,\n",
            "         311,  659,   46,   64,   36,  245,  466,   29,   27,   94,  340,  359,\n",
            "         292,  295,   30,  118,  470,  463,  232,   71,   63,  158,   92,   48,\n",
            "           9,  122,  538,  187,  262,   43,  245,  118,  185,  108,   77,   37,\n",
            "          47,   23,  515,  110,  290,   53,  225,   93], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 885,   38,   70,  503,  118,   87,  132,   20,  368,  196,  477,  306,\n",
            "          40,  338,  567,   18,   66,  749,  118,   51,  512,   10,  123,   32,\n",
            "         352,   56,   80,  172, 1285,   68,  465,  330,  199,  338,   46,  551,\n",
            "         239,  542,   39,  298,  519,   40,  412,  627,   89,  268,  763, 1898,\n",
            "          44,  123,   48,   12,   51,   18,   32,  101,   75,  344,   84,  114,\n",
            "         361,  576,  296,   70,  720,   52,   40,  287,   85,   72,   37,  113,\n",
            "         300,   29,   97,   85,  594,  194,   72,  118,   79,  137,  120,   32,\n",
            "         166,   57,  101,  665,   30,   62,   48,  105,  176,  117,   10,  266,\n",
            "         237,  686,  175,  229, 1014,   52,   66,  255,  204,   83,  519,  209,\n",
            "         535,  641,   68,  330,  216,   66,  463,  763,  120,  535,    9,   86,\n",
            "          54,  252,  683,  131,  481,  181,   29,   40], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  43,  759,   91,  165,   86,  728,  144,   47,   80,   40,   46,   61,\n",
            "          68,   83,   99,  309,   33,  493,   67,  491,   94,  465,   53,  293,\n",
            "          63,  470,  171,  315,   88,   20,  123,   58,  360,  300,   19,   75,\n",
            "          70,  228,   72,  504,  432,  210,  153,  270,  221,  279,   97,  161,\n",
            "          34,  169,   47,  427,  219,  393,   72,  289,   50,  316,   41,   48,\n",
            "         108,  137,  448,   99,   63,  642,  525,  128,   42,   83,  311,  692,\n",
            "         187,   93,  186,   44,  134,  352,  484,  215,  441,  263,   28,  655,\n",
            "         997,   90,  155,  114,   38,  399,   80, 1370,  104,  258,   54,  139,\n",
            "         548,   63,  156,  190,  252,  130,  407,   34,   76,  142,  248,  196,\n",
            "         393,  353,  137,  177,   24,  461,  260,   58,  332,  149,  266,  171,\n",
            "         167,  108,   25,  113,  490,  168,  245,  535], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 451,  220,  123,   40,   23,  146,  162,   63,   55,   56,   48,   88,\n",
            "          59,  559,  525,  105,   81,   88,  540,  155,  555,   31,   37,   49,\n",
            "         809,   31,  482,  248,  420,  261,   67,  199,   26, 2135,  219,   52,\n",
            "          86,   65,  353,   45,  332,  102,   70,  543,  338,   69,   56,   42,\n",
            "          28,  125,  150,  145,  290,  613,  242,   60,   62,  361,   58,  130,\n",
            "         437,  218,  332,   62,  792,  293,  615,  593,  257,  164,   58,   61,\n",
            "         750,  355,  194,  143,   50,  112,   73,   39,   40,   78,  344,   41,\n",
            "         295,  109,  269,  406,  110,  214,  207,   19,   64,  303,  333,  269,\n",
            "         339,  387,  121,  471,  471,  133,  105,   51,   37,   88,  595,   17,\n",
            "          35,  128,   65,  411,  300,  547,  932,  757,  147,   87,  167,   98,\n",
            "          15,   50,  263,  110,   70,  251,  252,  102], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([142,  80, 361, 515,  56, 460,  71, 240, 724,  60,  62, 226,  81,  70,\n",
            "        150, 327, 101,  83, 469, 443,  10, 336,  91, 295, 320,  42,  68,  71,\n",
            "         70,  76,  93, 121, 241,  79, 608,  71,  62,  33,  43, 284, 193, 790,\n",
            "        354, 562, 137, 336, 268,  94, 435,  95, 199, 111,  15, 129,  16, 239,\n",
            "        432, 457, 169, 342,  37, 191, 385,  40,  59,   8,  50, 202,  60,  95,\n",
            "        338, 173,  47,  53, 307,  21,  61, 457,  28,  10,  52, 124, 551,  14,\n",
            "        197, 132, 130,  23, 367, 240,  46,  94, 137, 263, 293, 248, 689, 542,\n",
            "        105,  41,  79, 121, 580,  94,  68,  17,  51, 124, 346, 422, 255, 129,\n",
            "         97, 195, 277, 432, 204, 178, 327, 106,  27, 516, 217, 167, 291,  10,\n",
            "         86, 152], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  86,  178,  481,   77,  455,  129,  159,   48,  232,  779,   85,  331,\n",
            "         421,   36,   41,  258,   75,  552,  319,   73,   93,  174,  145,  429,\n",
            "           9,  301,  209,  244,  120,  223,  375,  344,  484,  646,   73,   12,\n",
            "          57,   64,   71,  357, 1108,  335,   54,  271,  494,  350,  117,  642,\n",
            "          19,  470,   27,   89,  356,  292,  164,   65,   42,  238,  131,   36,\n",
            "         109,   28,  192,  272,  489, 1233,  148,  231,   88,  132,   98,  148,\n",
            "         164, 1048,  110,  591,  107,   43, 1012,  110,  117,   65,   94,  270,\n",
            "          85,  241,  148,  432,  800,  249,   40,   81,  357,   66,  380,   65,\n",
            "          59,   19,  833,  357,  408,  262,  115,  165,   67,   63,  119,  446,\n",
            "         294,  117,   58,  106,  348,   40,  112,  114,  205,   63,  305,  141,\n",
            "          77,  104,  293,   79,  220,  225,   33,  336], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([324,  38, 322, 256, 152, 133, 236,  88, 596, 732,  71, 330, 520,  86,\n",
            "        370, 196, 234, 101, 249, 229, 273,  59,  17,  91, 143, 121,  54, 164,\n",
            "         37, 425, 207, 229,  92,  36, 148, 415,  69, 161,  30, 350, 223, 298,\n",
            "        667,  96, 269,  98, 137, 321,  76, 359,  43,  38,  62, 165,  43, 127,\n",
            "         42, 428, 464, 376,  38, 689, 116, 365,  47,  46,  20, 768, 294, 126,\n",
            "         49, 276, 434,  77, 531, 145, 205,  98, 148,  95, 100, 173,  30, 319,\n",
            "        674, 260, 143, 431,  21,  50,  67,  81,  73, 157,  44, 284,  63, 150,\n",
            "        249,  49, 915, 318,  70, 236,  46, 360, 297,  30, 469, 737,  73,  39,\n",
            "        104,  45,  21, 381,  87,  76, 300, 239, 110,  23,  92, 254, 317, 142,\n",
            "         39,  38], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 296,   65, 1243,  116,  309,  156,  202,   90,   23,  115,  376, 1685,\n",
            "          28,  229,  525,  159,   55,  743,  327,  424,   56,  122,  236,   85,\n",
            "         645,  826,  724,   31,  539,  160,  314,   29,  316,   93,  429,  145,\n",
            "         148,  243,  145,   96,  166,  516,   74,  166,   78,  227,  216,  155,\n",
            "           8,   79,  391,  224,   12,   46,   51,   91,   48,  254,  123,  114,\n",
            "         134,   99,  499,  359,  117,  946,  836,   58,   26,  289,  170,  287,\n",
            "         110,   31,   21,  489,   66,   57,  213,   58,  436,   36,  194,  831,\n",
            "         533,  250,   52,  193,   44,  190,  496,   85,  136,  270,   57,  516,\n",
            "          67,  222,  378,  577,   30,   83,  129,  397,  376,   13,   61,   28,\n",
            "         320,   70,  199,  104,   49,  131,  112,  404,  234,   12,    8,  384,\n",
            "          62,   80,  263,   46,  191,  193,   48,  399], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  36,  983,  377,  189,  275,  156,  127,  303,  333,  220,   30,  241,\n",
            "         412,   23,   28,  118,   42,   73,   87,  129,   85,  147,   37,  438,\n",
            "         222,  625,   53,  329,  156,  171,  157,  231,  120,  114,   96,   26,\n",
            "        1105,   96,    9,  390,  315,  327,  455,   57,    9,  604,   73,   88,\n",
            "         113,  142,  107,  227,  155,  133,  285,  201,  100,  106,  234,   10,\n",
            "          64,  282,   56,  179,   60,  147,   52,  237,  134,   69,   76,   26,\n",
            "          97,  151,   77,  231,  148,  348,  527,  553,  228,   48,   99,   37,\n",
            "          37,   54,  104,  175,  225,   71,  139,  137,   26,  193,  403,  434,\n",
            "         152,  245,  801,  160,   55,   48,  470,  137,  326,  252,  137,   66,\n",
            "         313,   17,  421,   14,  161,  367,   63,   98,  353,  215,  125,   68,\n",
            "          30,  269,   97, 1029,  271,   58,   30,   54], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 96,  31, 157,  94, 151, 156, 104,  49,  64,  57, 235,  22,  35,  77,\n",
            "        294, 104,  60, 923, 181,  84, 109, 105,  54,  92,  20,  78,  34, 126,\n",
            "        144,  76, 231, 829,  45, 125, 238,  72, 421,  98,  22, 174, 425, 138,\n",
            "        494, 662,  65,  50,  19, 410, 346,  96, 197,  68,  75, 177, 469,  94,\n",
            "        105, 353,  44, 352, 469,  15,  57, 684, 262,  50, 539,  46, 342, 173,\n",
            "        138, 143, 257, 123, 156,  76, 813, 111, 419,  85, 644, 128, 217, 249,\n",
            "        207, 267,  60, 207, 232,  58,  30,  46, 158, 330,  93, 319,  35,  31,\n",
            "        160, 929, 447, 202, 373, 525,  50,  35, 350, 276,  63,  92,  45, 552,\n",
            "         45,  90, 506,  13,  17,  67,  98, 488, 669, 209, 242, 139, 237, 219,\n",
            "        122, 301], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 576,  138,  129,   20,  422,  200,   85,  362,  327,   32,  141,  111,\n",
            "         165,   32,  427,  504,  308,   36,   37,   24,   53,  340,  107,   41,\n",
            "         340,   29,   65,  422,   87,   19,  336,  376,  118,  208,  260,   91,\n",
            "          40,  159,  150,  295,   93,  179,   96,  107,  165,   91,  600,  157,\n",
            "          90,   59,   61, 1209,  110,   31,  511,  443,  293,  194,   58,  124,\n",
            "          51,  246,   86,   21,   40,  181,  193,  343,  565,  294,   77,  345,\n",
            "          86,  112,   76,  129,    9,   23,   67,  101,  616,  293,   94,   63,\n",
            "         125,   89,  115,   42,   68,  381,  162,  193,  402,  475,   69,  328,\n",
            "         298,  203,   89,  153,   45,   72,   51,   39,  222,   67,  124,  332,\n",
            "           9,  717,   34,  106,   66,  278,   96,  508,  442,  207,  185,  263,\n",
            "         154,  199,   35,   29,  522,  119,   83,  285], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 139,  114,  223,  238,   69,  128,  229,  143,  212,  204,  331,   23,\n",
            "         296,  292,  472,  504,  225,  359,   59,  547,  133,   80,  328,  247,\n",
            "         767,  178,  138,  177,  429,   53,  288,  726,  132,   92,   67,  296,\n",
            "          91,   22,   99,   91,  474,  187,   25,  607,   85,   93,  164,   48,\n",
            "         457,   54,   51,  107,  108,  100,  232,  668,  189,  510,   20,  103,\n",
            "         113,   85,  158,  313,  186,  128,  102,  236,  451,  195,  105,   37,\n",
            "         229,  586,  499,   37,   70,  418,  370,   64,  150,   33,  127,  748,\n",
            "         761,  546,   17,  188,   12,  535,   57,  425,   58,   39,   45,  142,\n",
            "          33, 1463,  319,  285,   97,  159,  427, 1707,  215,  135, 2742,  138,\n",
            "          69,  386,   23,  117,  369,   41,  134,  100,   58,  177,  194,  816,\n",
            "          56,  352,   43,  139,   14,  153, 1002,  150], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([203, 501, 170, 245, 209, 496, 338,  24, 388, 193,  56, 770,  96,  77,\n",
            "        190, 341, 142, 257, 219, 546, 221,  67, 843, 279, 143,  25, 485,  37,\n",
            "         82, 167, 570, 334, 210, 209, 204, 370, 632, 181,  77, 352,  99, 165,\n",
            "        286, 145, 200, 251, 116,  71,  43,  87, 515, 191, 227, 105, 487, 513,\n",
            "         41,  49, 163, 162,  32, 629, 315, 308, 892, 144, 226, 177, 193, 220,\n",
            "        111, 812,  63,  38,  62, 232, 150, 143, 101, 529, 161, 123, 519, 129,\n",
            "        231,  77, 500, 566,  15,  91, 149, 589,  28, 120,  47, 342,  38, 757,\n",
            "        116,  67, 212, 700, 111, 315, 565, 225,  32,  64, 439, 622,  28,  18,\n",
            "        218, 328, 410,  36, 263, 339, 310,  58,  98, 256, 153,  51,  77, 123,\n",
            "        469, 358], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 174,   37,  586,   69,  287, 2453,   34,  143,   50,  600,  712,   72,\n",
            "         583,   40,  116,  112,   74,  319,  106,   58,   44,   53,  359,   77,\n",
            "         293,  349,  193,  169,  296,  171, 1013,  494,   10,  103,  232,  184,\n",
            "          38,  200,   36,   36,   62,  502,   31,   82,   95,  103,  212,   27,\n",
            "         348,   48,  277,  118,   34,  217,   71,  453,   16,  274,  269,  115,\n",
            "         165,   49,  367,   68,  276,   63,   24,  167,  592,   67,  550,   99,\n",
            "         348,  412,  104,  463], device='cuda:0')\n",
            "torch.Size([76, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([76, 4250, 300])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([122, 4250, 300])\n",
            "Epoch: 001/002 Train Acc.: 0.00% | Validation Acc.: 0.00%\n",
            "Time elapsed: 0.62 min\n",
            "the precision_recall_fscore_support of F-score_train:  (0.23748079877112135, 0.04516129032258064, 0.07534125636672326, None)\n",
            "the precision_recall_fscore_support of F-score_val:  (0.08375350140056022, 0.023529411764705882, 0.02480859010270775, None)\n",
            "tensor([ 246,   55,  504,  293,  324,   58,  192,  193,   77,  113,   16,   62,\n",
            "         163,  264,  917,   43,   85,  127,  168,  530,  468,  386,  102,  627,\n",
            "          36,   39,  235,  115,   86,  217,   97,   37,  394,  214,  942,  416,\n",
            "         201,  178,  201,  108,   37,  402,  763,  149,   68,  474,   99,   48,\n",
            "          47,   42,   34,   93,  433,  161,  447,   91,  101,  206,   47,   20,\n",
            "         483,  121,   45,  260,  162,   51,  109,  365,   62, 1261,   22,  142,\n",
            "          83,  405,  199,   98,   97,  597,   24,   83,  170, 1697,  359,  316,\n",
            "         100,  527,   40,   46,  133,  103,  205,  434,  116,  400,   43,   16,\n",
            "        1048,  211,  218,  152,  456,  248,   97,  100,   83,  617,   91,  130,\n",
            "          26,  146,   73,  321,   24,  107,  167,   82,  122,  216,  101,  106,\n",
            "          33,  170,   84,   75,   43,  158,  390,  236], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "Epoch: 002/002 | Batch 000/080 | Cost: 0.3973\n",
            "tensor([ 212,  185,   19,  272,  181,  321,   89,  180,  196,   46,  161,  136,\n",
            "          19,   87,  392,  422,  135,  405,  304,  198,  513,  148,   37,  418,\n",
            "         137,  225,   36,  229,  143,   28,  734,   95,   69,   49, 1035,  139,\n",
            "         706,  101,  232,  300,   71,   12,  108,  187,  253,  130,  234,  115,\n",
            "          17,  286,  218,  139,   53,   28,  182,   12,  153,  300,   66,  844,\n",
            "         536,  170,   68,   12,  233,   64,  238,   21,  254,  394,  257,   94,\n",
            "          49,   31,   14,   72,   61,  270,  337,  333,  404,   37,   63, 1243,\n",
            "          27,  272,  607,   62,  328,  200,   60,   83,  367,   91,  738,  355,\n",
            "         205,  115,  446,  167,  665,  139,  292,  196,  209,  407,  132,   66,\n",
            "         349,   43,  157,  294,   86,   11,  141,  184,   83,  622,   30,  158,\n",
            "          70,  245,   80,  499,   51,   74,   15,  145], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 571,  291,  122,   58,  120,  196,   75,  216,   11,  155,  634,  266,\n",
            "          34,  145,  226,  246,  458,  199,   62,  363,  103, 2306,  539,  154,\n",
            "         112,  744,   39,   80,   14,   77,   41,   19,  118,  515,  504,  250,\n",
            "          31,   20,  283,   81,   51,   21,   30,   79,  339,  669,   55, 1072,\n",
            "         183,   68,  133,   36,  314,   34,   88,  356,   31,  227,   59,   65,\n",
            "          85,   21,  156,  252,  129,  237,   89,   47,  379,  491,  342,  543,\n",
            "         424,   25,   45,  499,  673,  339,  330,   34,   17,  270,   26,   47,\n",
            "         230,  345,  363,  161,  107,   38,  135,   98,   64,  112,  104,  415,\n",
            "         199,   46,   96,   27,   50,  196,  222,   85,  149,  242,   90,   65,\n",
            "         262,  160,   17,  524,  123,  160,  983,  144,  102,   59,   41,   46,\n",
            "          49,   28,   89,  242,  778,  175,  815,  101], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  18,   38,  322,  210,   20,   12,  343,  142,  178,   43,  248,  145,\n",
            "         167,  140,   30,  291,  351,  405,   66,   87,   88,  342,  190,  154,\n",
            "         487,   35,   81,  147,  964,  463,   96,   51,   50,   58,  319,  676,\n",
            "          89,  159,  535,  639,  498,   56,   84,  315,  143,   52,  453,  642,\n",
            "         331,  118,   42,  404,   64,  266,   16,   94,  177,  141,  409,  335,\n",
            "         742,   67, 1253,   69,  410,  172,   84,  331,  155,  674,  159,  104,\n",
            "          74,   88,  219,  128,   57,  441,  239,   95,  139,  303,   57,   20,\n",
            "         256,   69,  124,  110,  468,  712,  201,   76,   40,  133,  369,   32,\n",
            "         183,  423,  155,  370,  279,   22,  402,  225,  233,  706,  573,  286,\n",
            "          76,  498,   43,  296,   35,  202,   81,   91,   40,  233,   37,  179,\n",
            "         249,   23,  251,  407,  456,  370,   60,  529], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 225,  826,   71,  453,  318,   41,  397,  110,  160,  229,  394,   42,\n",
            "         200,   72,  193,   23,  165,   37,  144,  379,  318,  413,  161,   26,\n",
            "         144,    9,  286,  194,  130,  538,  211,  255,  127,   91,  121, 1035,\n",
            "          41,   11,  109,  491,  107,  773,   79,   23,  103,  331,   56,  415,\n",
            "         101,   14,  468,   94,   97,   76,   74,  196,   23,  218,  154,  506,\n",
            "          38,  116,  308,  200,  441,  174,   32,   50,  119,  111,  173,  269,\n",
            "          48,  389,   29,   50,  165,  126,  299,   80,  202,  234,  236,   57,\n",
            "         742,  363,  227,  349,  106,  313,  167,  132,  176,  357,  285,  194,\n",
            "         463,  279,  144,   27,  359,  409,   61,  776,  235,  115,   80,  151,\n",
            "          23,  325,  263,  238,  242,   29,  126,  169,   29,   44,  132,  101,\n",
            "         169,  330,  407,  194,  318,  124,  130,  184], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 393,  144,  208,   58,  481,  303,  328,  128,   38,   42,  374,   30,\n",
            "         369,  361,  383,   46,   30,  254,  538,   69,   67,   40,  620,  186,\n",
            "          92,  182,   68,  123,   56,   63,  119,   33,  229,  607,   51,   78,\n",
            "         116,   67,  145,  210,  285,  255,   29,  354,  104,  100,  300,   88,\n",
            "         550,   92,  113,  330,  212,  221,   25,  195,  238,  103,  227,  397,\n",
            "          14,  235,  367,  289,   69,  180,   61,   67,  129,  317,  453,   96,\n",
            "          44,   71,  129,   65,   90,  191,  214,  133,  514, 1241,   53,   26,\n",
            "         111,   87,  209,  151,  287,   64,   41,  219,   70,  545,   14,   48,\n",
            "         314,  150,  151,   66,  155,  148,  150,  247,   55,  359,   83,  244,\n",
            "         495,   72,  137,  116,  474,   77,  167,  186,  391,  615,  125, 1054,\n",
            "         644,   56,  400,  294,  285,  213,   25,  287], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  91,   30,  390,  221,  344,  355,  270,  233,  290,  179,  158,   77,\n",
            "         424,  159,  124,  310,   71,  243,  324,  467,   61,  101,  384,  275,\n",
            "         348,  259,   55,  121,   11,  254,   62,  385,   28,   72,   77,  728,\n",
            "         354,   31, 2700,  231,  270,  138,   60,  241,  493,  397,  529,   27,\n",
            "         312,  120,   83,  144,   19,  108,  127,  319,  166,  383,   34,  113,\n",
            "         245,   73,   15,   75,  129,  262,  290,   84,   64,  319,   33,  213,\n",
            "         353,  370,  786,  350,  180,   42,  537, 1371,   73,  151,   44,  252,\n",
            "          63,   93,  206,  425,   89,  659,  115,   17,  237,   24,  595,   49,\n",
            "          40,  203,  342,   19,  219,  100,   77,  112,  392,  247,  368,   21,\n",
            "         264,   60,  109,   25,  358,   31,   38,  303,   73,   43,  254,   64,\n",
            "         476,   61,  336,  435,  255,  205,  108,  569], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  44,   27,  260,   19,   93,  277,   26,  171,  165,  159,  131,  208,\n",
            "         542,  508,  106,  340,  159,  422,   44,   60,  309,  339,   51,  246,\n",
            "         282,   51,   65,   74,  225, 1283,   36,  123,  220,  451,  233,   51,\n",
            "          53,  149,   82,  113,  836,  281,  387,  391,   71,  182,  370,   67,\n",
            "         107,   69,  116,  179,  125,   20,   99,  324,  210,   92,  949,  243,\n",
            "          48,   49,  169,   34,  112,   73,   78,  418,  213,  432,   75,   71,\n",
            "          27,   32,  445,  118,  128,  306,  132,  234,  253,   52,   64,  218,\n",
            "         164,  248,  233,   60,  101,   38,  392,   61,  501,  138,  343,   37,\n",
            "          50,   71,  105,  174,   70,  361,  263,   76,  128,   45,   69,  698,\n",
            "         119,  205,   80,  333,   11,   90,  243,   56,    9,   77,  615,   83,\n",
            "          52,   68,  299,   65,  134,  152,   21,  289], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  18,   85,   30,   83,  338,  237,   19,  336,  160,   39,   91,  143,\n",
            "         111,  475,   39,  109,  402,  180,   60,   51,   13,  169,  292,  397,\n",
            "          51,  159,  119,  134,  442,  366,  125,  112,  584,   51,  170,   46,\n",
            "          97,  369,  106,   67,   47,  318,  536,  996,   83,  157,  374,   42,\n",
            "          85,   87,  483,  280,   96,   76,   34,  180,  263,  216,  961,   41,\n",
            "          28,   41,   62,  228,   79,  179,   32,  189,   42,   52,  122,  112,\n",
            "         128,  167,  151,   28,  408,  192,  136,   70,  435,  468,  186,  125,\n",
            "           9,  242,  175,  128,  485, 1287,  476,  141,  505,  655,   31,  421,\n",
            "         410,  338,  229,  899,  413,  288, 1209,   99,  221,  179,   36,  193,\n",
            "          54,  167,   56,  141,   22,  361,  137,  253,  955,  182,   72,   23,\n",
            "         241,  237,   49,  142,   50,  168,  190,  603], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([303,  98, 456,  83,  30, 997,  50, 323, 282, 345, 146, 532, 174,  22,\n",
            "         60, 119,  15,  36, 270, 509, 247, 133,  43,  37,  55,  76,  62,  75,\n",
            "         18, 118, 183, 165, 183, 121,  76, 330, 327, 423, 590, 142, 102, 168,\n",
            "        612, 487,  83, 326, 432,  51, 356,  57,  69, 147,  80, 188, 401, 173,\n",
            "         70, 245,  28,  20, 332,  24, 414,  50, 120, 184, 367, 186, 219,  58,\n",
            "        314, 724, 531,  33, 163, 364,  31, 262,  55,  52,   9, 359, 273, 764,\n",
            "         92, 485,  23, 134,  76, 200,  25,  47,  39, 110,  66,  20, 221,  61,\n",
            "         99,  78,  26, 539, 180,  69, 215,  63, 104, 426,  19, 216, 438, 137,\n",
            "        474,  12, 286,  42, 542, 298,  67, 360, 119, 734, 101,  86, 281,  42,\n",
            "        147, 207], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 143,   38,  309,  678,   59,  105,  615,   12,   41,  287,   46,   24,\n",
            "         155,  365,   89,  340,  241, 1093,  107,  429,   78,  623,  485,  178,\n",
            "         153,   38,  421, 1009,  125, 1136,  308,  130,  405, 1044,  260,    9,\n",
            "          25,   48,  164,  178,   12,  263,  748,   10,   67,  670,   54,  135,\n",
            "          90,  275,   78,  120,  274,  392,  109,   25,  641,  302,  193,  153,\n",
            "         163,   97,   49,  490,   63,   78,   52,  775,   42,   20,   12,  154,\n",
            "         234,  331,   85,  227,  164,   91,   99,  271,   78,   65,  111,   62,\n",
            "         248,  158,  132,   35,  740,  150,  317,   76,  276,  210,  714,  132,\n",
            "         405,  284,   11,   44,   39,  145,  581,  510,   96,   38,  245,  123,\n",
            "         212,  201,  207,   37,  353,  292,  321,  195,  157,  144,   48,  404,\n",
            "         492,   81,   37,  137,  107,   57,  251,   33], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 926,   72,  276,   40,   86,   44,  264,  444,  260,  157,   37,  590,\n",
            "         110,  257,  175,   35,  132,   26,   45,  262,  158,  430,   76,  314,\n",
            "         663,   84,   14,   50,   56,   37,  127,  116,  477,   21,  213,  636,\n",
            "          85,  229,  564,   43,   54,  247,   95,  169,  565,  317,  149,   94,\n",
            "         145,   66,  129,  108,   46,   45,   46,  154,   32,  273,  176,   75,\n",
            "         200,   72,   48,  353,   95,  252,   69,   39,   58,  418,   36,   31,\n",
            "         169,  295,  339,  114,  134,   70,  702,  114,  284,  475,  165,  105,\n",
            "         425,  143,  240,   64,   19,   41,  670,  165,  709,   40,  212,  481,\n",
            "          59,   85,   47,  265,  107,   93,  215,  532,  535,  266,  262,  421,\n",
            "          75, 1886,  346,  434,  194,   47,  654,  230,   47,  874,   52,  589,\n",
            "         923,   54,  936,   22,   47,  151,  109,  401], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  79,  484,  376,  195,  216,   61,   31, 1120,  346,   31,  233,  448,\n",
            "         594,  500,  321,  566,  554,  230,  126,  187,  111,  768,  280,   33,\n",
            "         386,  443,  294,   56,  325,  324,  315,  258,  116,   48,  407,   36,\n",
            "         255,  320,   70,  517,   32,   23,   49,  187,   72,   76,   99,  143,\n",
            "          38,  238,  305,  206,  396,  437,  252,   94,  104,  199,  110,  300,\n",
            "          76,   52,   56,  236,  689,  748,  254,  131,  112,    8,  411,  552,\n",
            "          53,  615,  959,   36,   48,  466,  148,  466,   71,   22,  118,   84,\n",
            "         439,  489,   93,   37,  146,  575,   49,  222,  192,  584,  101,  915,\n",
            "           8,  159,   60,   71,   27,   62,  617,  256,   56,  121,  240,  219,\n",
            "         184,  311, 1053,  151,  150,   54,   85,  136,  374,  344,   52,  110,\n",
            "          76,  535,  158,  197,  401,  105,  669,  126], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 426,   79,   45,  393,  235,   32,  152,   70,  519,  169,   61,  231,\n",
            "         538,  728,  297,  195,   24,  175,  516, 1261,  223,   29,   52,  710,\n",
            "          93,  629,  212,   50,  133,   88,  499,  761,  186,  132,   48,  252,\n",
            "          92,  750,  130,   26,  245,  212,    9,  494,   62,   63,  361,  107,\n",
            "          83,  521,  139,  407,  458,  212,  364,   50,   33,  202,  453,   63,\n",
            "         573,  202,   10,   98,  179,  401,  556,   58,  488,  439,  138,   21,\n",
            "         463,  315,  154,  189,  314,   81,  121,  157,  376,  117,   41,  665,\n",
            "          28,   26,   26,  306,  321,  182,  286,   40,  460,  473,  150,   60,\n",
            "          75,  111,   57,   71,  222,   30,   56,   81,   11,  168,   85,   42,\n",
            "         144,   21,   16,   66,  100,  129,   60,  334,   63,  198,  134,  876,\n",
            "         373,  251,  156,  399,   91,  248,  493,  117], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 258,  153,   91,  238,  173,  539,  147,   58,  110,  240,  317,  110,\n",
            "         927,  603,  143,  246,   91,  147,   75,   33,  211,  386,  207,   97,\n",
            "         220,  406,  770,  155,  115, 1014,  504,  250,  155,  107,  936,  280,\n",
            "         280,   44,   68,  191,  131,  496,  371,  163,  177,   91,  341,   72,\n",
            "         367,  547,   85,   28,  312,   24,  434,   32,   20,   77,  133,  150,\n",
            "          55,  942,  236,  367,  449,   97,   66,  137,  564,  370,   21,  380,\n",
            "          60,   33,  123,  465,  144,  734,   63,  189,   63,   34,  118,  250,\n",
            "          17,   70,  172,  258,  151,   48,  623,  118,  216,  221,  484,   42,\n",
            "         215,   94,  200,   94,  161,  131,  157,   89,  150,  182,  429,   62,\n",
            "         103,   77,  306,  437,   49,  477,  256,  169,  195,   34,  472,  344,\n",
            "         254,  131,  278,  236,  502,  194, 2856,   41], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  48,   89,  286,  182,  148,  340,  853,  289,  308,  225,   45,  277,\n",
            "         286,  281,  106,  229,   56,  267,   64,   33,   68,   63,  275,   66,\n",
            "         451,  106,   45,  306,  311,  651,  263,  330,  150,  123,  357,   55,\n",
            "         139,  240,   37,  170,  280,   97,  352,  801,  117,  169,  247,  539,\n",
            "        1175,  129,   30,   94,   40,   31,  116,  116,   43,   83,  250,  162,\n",
            "         237,  152,  259,  248,   87,  328,  112,   33,   55,  301, 1285,  323,\n",
            "         253,   36,  401,   91,  255,   84,  497,   57,  855,   25,   74,  431,\n",
            "          36,   39,  472,   63,   87,  376,  304,   67, 1344,  126,   62,  147,\n",
            "         469,   87,  183,  274,  129,   29,   15,  202,  228,   75,   43,   37,\n",
            "          60,  235,   80,  149,  607,   41,   39,  101,   46,  169,  152,  267,\n",
            "          95,  106,  165,   44,   13,  338,   72,   91], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  20,  304,  266,  527,  311,  362,  116,   20,   55,  189,  120,   63,\n",
            "         122, 1707,   39,  234,   90,   56,   41,   43,   37,   16,   72,  173,\n",
            "         127,  193,   83,  696,  249,   71,  121,   91,   84,  543,  139,   55,\n",
            "         479,  830,   79,  197,  341,  231,  210,  206,  422,   10,   45,  207,\n",
            "         143,   96,  342,   49,   95,  188,   42,  496, 1514,   24,   30,   72,\n",
            "         361,   78,  488,  200,   96,  190,   39,  303,   72,  316,   71,  316,\n",
            "         308,   62,   39,  284,  207,  170,  131,  220,  128,  176,  521,  462,\n",
            "          95,   37,  121,  258,  167,   17,   83,   46,  399,  122,  324,  349,\n",
            "         134,  157,   62,  111,   39,   57,   32,   42,  495,  214,   31,   45,\n",
            "          51,   42,   67,  236,   81,   98,   29,  122,  137,  144,  274,  221,\n",
            "         165,  221,  131,   62,  314,  116,   40,  249], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([101,  41, 266,  65, 461, 576, 270,  66, 224,  89, 120, 140, 230,  48,\n",
            "         52, 400, 559, 438,  40, 107,  63,  73, 140, 339, 169, 241,  31, 365,\n",
            "        144, 391, 405, 218, 245, 338,  43,  23, 416, 336,  66,  54,  91, 120,\n",
            "         23,  56, 137, 151,  27,  74,  25,  88, 241,  45, 238, 428, 206,  58,\n",
            "        213, 122, 412,  18, 164,  73, 167,  64,  50, 338,  96,  44,  21, 322,\n",
            "         46,  51,  39, 432, 422, 228, 166, 277,  48, 298, 307, 106,  26, 575,\n",
            "         59, 254, 236, 383, 213, 104, 119, 128,  97, 266, 190,  81, 362, 221,\n",
            "        245, 160, 510,  32, 600, 511, 119, 267, 198,  30, 576, 430, 171, 116,\n",
            "        160,  84,  27,  56,  39,  77,  93, 123,  64,  27,  76,  84, 165,  28,\n",
            "         64, 230], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  50,  447,   50,  833,   44,   49,  128,  387, 1090,   85,  354,  552,\n",
            "         144,   39,  143,  440,   18,  182,   46,   44,  110,   70,   32,  501,\n",
            "         264,  662,  250,  128,   54,   44,   69,   78,  288,  151,  787,   69,\n",
            "         256,   25,  182,  210,   27,  888,  233,   60,  539,  590,  184,  335,\n",
            "          86,  111,   93,  265,   54,  407,  116,  177,  366,  160,  133,   74,\n",
            "         231,  145,   19,   14, 1466,  199,  357,   95,  878,  360,   31,  219,\n",
            "          28,  592,   67,   28,  222,  546,   75,  268,  621,   66,   90,  243,\n",
            "         101,  339,   99,   77,  586,  645,   10,  203,  300,   64,  143,   23,\n",
            "          90,   95,  282,  247,  403,  102, 2018,   50,  108,  151,  120,   31,\n",
            "         329,  130,  236,   43,   65,   62,  182,  550,   76,    8,   65,   33,\n",
            "         298,   81,   45,  489,   88,   70,  116,  212], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 409,  167,   60,   58,  180,  181,  269,  348,  419,   90,  281,  112,\n",
            "         101,   54,  332,   54,  290,   99,  512,  178,  375,   53,  660,  108,\n",
            "         199,   48,  164, 1382,   58,  140,  218,  884,  135,  127,   98,   20,\n",
            "          62,  637,  282,  248,   44,   40,  419,  289,   50,   69,   38,  249,\n",
            "         485,  205,  686,  187,  689,  167,  332,  359,   36,  220,   50,  885,\n",
            "         127,   36,  276,  171,  179,  218,  162,  458,  131,   51,  430,   96,\n",
            "         210,  146,  385,  423,  137,  103,   11,   40,  249,  428,  302,  588,\n",
            "         198,  363,  109,  116,   92,  181,  161,  174,  552,  574,   99,  292,\n",
            "         525,  112,  143,  205,   47,  161,  186,   40,   91,   43,   20,  158,\n",
            "          23,  103, 1685,  160,  213,   63,   32,   72,  486,  435,   70,  397,\n",
            "         120,  355,  270,   40,   54,  291,   46,  137], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  24,  578,  486,  378,   87,   34,   70,  569,  129,  161,   45,  119,\n",
            "         476,  149,   73,  422,  118,  907,  229,  594,   48,  111,  252,  118,\n",
            "         103,  285,   80,  496,  165,   31,   66,  159,   75,  406, 1759,  339,\n",
            "          57,  199,  470,   50,  248,  295,  119,   62,   59,  285,  231,   77,\n",
            "         763,  100,  429,   36,  531,  343,  247,   62,  120,  166,  130,  738,\n",
            "         504,  237,  512,   37,  132,  579,   63,  435,  309,   91,  109,  180,\n",
            "         393,   42,  189,   98,    8,  871,  439,   80,   39,  127,   26,  199,\n",
            "         102,  485, 1206,  546,  117,  155,   72,   26,  348,  140,   45,   73,\n",
            "          89,   47,   24,  244,  234,   84,  350,  208,  151,  434,  249,   23,\n",
            "          43,  436,   70,   69,  107,   91,   31,   49,  104,  164,  253,  236,\n",
            "         201,  129,  197,   55,  308,   21,  122,  425], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "Epoch: 002/002 | Batch 020/080 | Cost: 0.4009\n",
            "tensor([106, 218, 125,  25, 228, 118, 201,  85, 133,  59,  34, 622,  12, 122,\n",
            "        241, 261, 232,  35, 230, 183, 211,  77,  65,  83,  26, 103, 364,  91,\n",
            "         49,  56, 182, 173, 484,  40,  77, 261, 492, 100, 385, 289, 583,  44,\n",
            "         27,  75, 236,  62,  48, 293, 133, 101, 798,  16,  41,  20, 453,  98,\n",
            "         62,  31, 106,  11, 107,  50, 291,  70,  85,  54, 207, 804,  75, 157,\n",
            "         25,  53,  34,  73,  38, 301,  20, 229,  23, 287, 111, 200,  94,  75,\n",
            "        620,  77,  73, 169,  21, 659,  44, 229, 160, 148,  53, 118,  29,  37,\n",
            "        233, 279,  27, 241, 443, 308,  41, 128,  80, 581,  80, 222, 172,  76,\n",
            "        309,  81,  15,  36, 667, 652, 365,  21, 772,  20, 221, 250,  34, 325,\n",
            "         36, 122], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 163,   30,  295,   80,   63,   86,  804,  178,  118,   12,  300,  233,\n",
            "          29,   43,   57,   59,  119,  178,   89,   43,   19,  201,   68,  640,\n",
            "         186,   36,   73,  295,   81,  499,  331,   90,  740,  179,   69,  147,\n",
            "          46,  385,  247,   65,  178,  166,  172,  371,  151,   68,  519,  226,\n",
            "          85,   98,   91,  152,  501,   62,  384,  142,   34,  385,  330,  261,\n",
            "         137,  116,  206,   63,  282,  109,  231,   22,  120,  178,   85,   71,\n",
            "         290,   93,   70,  114,   95,   48,  103,   72,  327,  124,  407,   18,\n",
            "          37,   91,  671, 1015,   70,   53,   20,   32,   12,  129,  381,   37,\n",
            "          77,   62,   98,   65,   61,  110,  705,  957,  256,  487,   34,  190,\n",
            "         427, 1013,  639,  320,   80,   92,  258,   92,  217,  145,  153,   95,\n",
            "         280,  559,  247,   56, 1741,   59,   68,   62], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 212,   50,   82,  153,   66,   62,   63,   33,   62,  228,  406,  208,\n",
            "          72,  372,   10,  173,  500,   25,  390,   39,  224,  213,  297,   58,\n",
            "         705,  113,  266,   76,  406,   82,  110,   80,   85,  310,   30,   67,\n",
            "          48,  386,  600,  134,   50,   61,   67,  222,  366,   45,  324,  134,\n",
            "         266,  264,  166, 1162,  537,   39,   80,   56,   79,  193,   99,  643,\n",
            "          22,  119,   23,  355,  333,  118,   28,   66,   70,  109,  204,   13,\n",
            "          92,  412,   50,   79,  238,   80,   20,  127,    8,  983,   94,  234,\n",
            "         267,  300,   41,   75,  146,   11,  470,  265,   41,  241,   64,  189,\n",
            "         203,  206,  175,  291,  407,  348,  868,  167,  234,   49,  223,   36,\n",
            "         424,  156,  255,   74,  117,  228,  191,   59, 1952,  990,  223,   51,\n",
            "          78,  120,  193,  267,  103,   35,  445,  155], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 180,  928,  814,   58,   68,  297,  109,  295,  123,  763,  131,  222,\n",
            "         291,  118,  143,  156,  204,  176,   59,  285,   28,  243,  121,  159,\n",
            "         147,   45,  204,  294,  872,  260,  116,  208,  198,  277,   63,   70,\n",
            "          45,  127,   60,   93,  206,  192,  279,  225,  119,  348,  190,   56,\n",
            "         522,   73,  356,   23,  156,   80,  663,  465,   90,   17,  103,   52,\n",
            "          58,   80,  236,  187,  173,  206,  148,  231,  471,  103,   89,   43,\n",
            "         142,  323,   44,   50,  236,  112,  161,   66,  344,   62,   78,  674,\n",
            "         129,  185,  217,  124,  141,   51,   77,  420,  117,   24,   35,   51,\n",
            "          42,  281,  375,   26,  138,   90,  291,  319,  145,   95,  201,  128,\n",
            "         571,  655,   56,  185,   70,  121,  194,   94,  208,   75,  301,   83,\n",
            "         118,   93, 1299,  145,  259,   40,  649,  402], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([193, 158,  81, 236, 222, 238,  19, 298,  47, 224,  22,  14,  44, 416,\n",
            "        141,  20,  71,  26, 277,  66,  62,  61, 228,  28, 118,  71,  25,   9,\n",
            "        145, 512,  28, 399, 180,  84, 402, 886,  47, 159,  28, 296, 232,  38,\n",
            "        355, 286, 190, 254, 631, 182, 779, 212, 544, 174, 520, 457, 278,  51,\n",
            "         49, 294, 250,  61,  43, 159, 468, 117, 678, 296,  57, 770, 153, 116,\n",
            "         79,  85,  41, 120, 402, 230,  88, 158,  37, 103,  20,  12,  53, 274,\n",
            "        322, 530, 277, 174, 193, 250,  30, 350,  47, 500, 131,  52, 240, 110,\n",
            "        146, 128,  40, 486, 153, 481,  36, 105, 196, 149,  41, 254,  54,  91,\n",
            "        189,  39, 246, 276, 107, 354,  71, 133, 164, 100, 220, 542,  96,  15,\n",
            "         85, 117], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 281,  515,  140,  177,  724,  352,   25,   18,  232,  232,  158,  121,\n",
            "         290,  184,   53,   50, 1025,  629,   84,   88,  239,   75,    9,  149,\n",
            "         316,   46,   78,  467,  172, 1137,  234,  219,  334,   35,  336,   83,\n",
            "         245,  172,   71,  242,  535,  211,  218,   85,   21,  463,  102,   46,\n",
            "          76,  350,   44,  447,   58,   55,  477,  390,   25,   73,  369,   25,\n",
            "          40,  160,  228,  408,  119,   85,  405,  358,   76,   58,    8,  346,\n",
            "          80,   18,   34,   69,  224,  419,  201,  627,  637,   48,   76,   52,\n",
            "          42,  353,  139,   66,  388,  158,  268,  113,   10,  255,   82,  140,\n",
            "          43,   38,  504,  161,  213,  210,   96,   51,  367,   80,   21,   65,\n",
            "         399,   71,   13,   28,  111,  162,   78,  127,  420,  309,   50,   10,\n",
            "         539,  148,   38,  275,  365,  269,  531,   58], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 206,  259,  580,   73,  321,   85,  220,  191,   71,  236,   82,   42,\n",
            "         537,   28,  198,  876,  427,   68,  366,  451,  112,  132,  112,   71,\n",
            "         122,  303,  809,  123,  200,  593,  332,  154,   78,  152,   16,  148,\n",
            "          69,   25,  298,  134,  168,  141,  660,  123,  355,  661,  433,  823,\n",
            "         312,  152,  219,  496,  610,   12,  117,  414,  289,   52,  127,  300,\n",
            "          46,   80,   56,   71,  463,   49, 1517,  301,   17,   42,   71,  157,\n",
            "         269,  232,   93,   17,  372,  215,   66,  161,  274,   91,  123,  110,\n",
            "         492,   92,  118,  125,  310,  401,   30,   54,  754,   58,  298,   79,\n",
            "         418,  160, 2141,  240,   38,  466,  204,  297,  273,  436,  535,  187,\n",
            "         216,   51,  245,  451,   51,  124,  314,  168,   78,   39,  380,  142,\n",
            "         264,   49,  253,   83, 1448,   63,  427,  127], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 281,   11,  936,  229,   58,  488,   57,  150,  148,  331,  214,   81,\n",
            "         542,   54,  243,   65,  126,  124,   29,  147,  293,  389,  228,  687,\n",
            "          85,  422,   22,  146,   73,   49,   64,   10,  156,   88,   75, 1432,\n",
            "          55,   66,   26,  129,  549,  569, 1259,  408,   93,  728,  144,  240,\n",
            "         203,   70,   65,  139,  410,  289,  372,  244,  117,  107,  231,  253,\n",
            "          90,  213,  278,  109,   67,   45,  256,  267,  527,  100,  196,   12,\n",
            "         100,   39,  167,  141,   45,  166,   90,  197,  241,   47,  210,  197,\n",
            "         513,  175,  124,   33,  295,   39,   83,  130,   61,  322,  633,  388,\n",
            "         131,  319,  138,  121,  165,   35,   22,  347,  279,   58,  138,  185,\n",
            "         221,  367,   21,  262,   69,   97,  112,  127,   67,   29,   66,  314,\n",
            "          51,  105, 1169,   86,  494,   57,  511,  150], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 105,   20,  544,  165,  123,  259,  339,  476,  304,   66,  112,   52,\n",
            "         235,   42,  303,   38,  452,  622,  197,  492,  243,  372,  551,  190,\n",
            "         687,  169, 2453,   23,   38,   86,   34,   38,   28,   33, 2005,  387,\n",
            "          89,  135,  169,  106,   35,   48,  236,  129,   49,  173,   53,    8,\n",
            "         615,  124,   86,  867,   29,  333,  318,  111,   57,  594,  110,   62,\n",
            "          61,   38,  509,    9,  115,   67,   48,  447,  137,  357,  463,  232,\n",
            "         125,  265,   28,   58,   78,  146,   70,  724,  185,  157,   98,  340,\n",
            "         311,  113,   15,  203,  281,   97,  293,   41,  336,   19,  162,  254,\n",
            "         148,  387,   50,   67,   57,   48,   53,  407,  234,  161,  320,   35,\n",
            "         511,  344,  109,   83,  225,   56,   61,  401,  140,  286,  271,   64,\n",
            "          18,  351,   37,   42,   84,   82,  354,  402], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 57, 250, 219,  34, 205,  25, 422, 285, 275, 128,  14, 215, 324,  24,\n",
            "        370,  25,  97,  73, 446, 212, 392,  30, 294, 188, 116, 128, 206,  36,\n",
            "        218,  16, 306,  36,  69, 607, 490, 183,  77, 556, 622,  95, 581, 297,\n",
            "        131,  80, 265,  32, 632, 157, 437, 171, 246, 519, 331, 133,  14, 208,\n",
            "        273,  83,  57, 322,  64,  44, 283,  52, 127,  26,  61, 345, 211,  47,\n",
            "        363,  88, 302, 236, 459,  39, 137, 316, 419, 130,  30,  86,  26, 224,\n",
            "        596,  77, 747, 127, 212, 153, 194,  51,  21,  70,  42, 366, 534, 171,\n",
            "        170, 630,  10, 151, 671, 122, 143,  37,  98, 108,  85, 155,  26,  87,\n",
            "        283,  65,  83, 815, 230, 383,  67, 320,  69, 177, 154, 162,  34, 424,\n",
            "         16,  56], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 278,  433,  144,   54,  108,  500,   56,   74,   94,   46,   35, 1029,\n",
            "         179,  186,  312,   45,   75,   72,  359,   90,  292,   48,   58,  264,\n",
            "         202,  285,   51,  117,   98,  131,  411,  192,  138,  344,  442,  390,\n",
            "          92,  686,  217,   57,  328,  543,  425,  565,  101,   92,  107,  364,\n",
            "         147,  281,  114,  300,  700,  143,  192,  462,   14,   72,   87,  679,\n",
            "          62,  294,  126,  274,  121,   20,  123,  343,   74,  580,   34,   23,\n",
            "         100,  348,  904,  203,  184,  146,  347,  148,  106,   87,   50,  239,\n",
            "         661,  171,   66,   62,   44,  290,  125,   80,  170,  399,   45,  718,\n",
            "         678,   48,  168,   60,  180,  540,   40,  314,  106,   49,  580,  860,\n",
            "         527,   32,  123,  215,  132,  854,   74,   42,  146,   67,   52,  372,\n",
            "          75,  332,  196,   55,   88,  288,  189,   63], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  27,  457,  135,   40,  322,  157,   31,   19,   47,   62,  367,  280,\n",
            "         511,   77,   22,   93,  198,  292,  331,  215,  260,  298,   39,  117,\n",
            "         522,  199,   45,  299,  412,  219,   64,  746,  123,   41,   52,  403,\n",
            "         117,  132,  528,  164,  216,  462,  170,  400,  350,   24,  319,  503,\n",
            "         235,  274,  202,  103,  123,  216,   58,   63,   90,  726,  456,  100,\n",
            "          91,  152,  179,   78,  374,   29,   74,  152,   84,   21,  549,  327,\n",
            "          85,  206,   87, 1115,   64,  257,   82,   60,   46,   35,  333,  335,\n",
            "        1271,  696,   37,  551,  156, 1030,  213,  538, 1196,  308,   86,  633,\n",
            "          72,  468,  409, 1068,  786,   60,  192,  433,  196,  199,   37,   91,\n",
            "         338,  717,   59,  124,  829,   29,  238, 1318,  165,   61,  161,  316,\n",
            "         236,   54,   76,  145,  591,   87,   48,   55], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  35,  524,   86,   84,   33,   65,   22,  238,   38,  194,  474,  576,\n",
            "         168,  177,  388,   74,  296,  203,  494,  327,   32,  510,  399,   23,\n",
            "          19,  330,  647,  219,  519,  207, 1105,  957,  296,   51,   63,  377,\n",
            "          77,  591,  145,   28,   59,  396,  145,  110,   33,  178,  159,   22,\n",
            "         222, 1007,  157,  129,   90,   49,  525,   54,   43,  792,   27,  335,\n",
            "         408,  156,  270,  237,  120,   15,  269,  269,   76,   22,  391,   14,\n",
            "         104,  351,   67,  420,   92,   77,  117,  434,  119,  164,  463,   36,\n",
            "          74,  433,  303, 1406,  452,  435,  370,  368,   45,   73,  192,  231,\n",
            "         330,  111,   38,  340,   69,   33,  137,  235,  200,  116,  507,  186,\n",
            "         118,   56,  275,  553,   95,  396,   91,  150,  245,  145,   83,  146,\n",
            "         346,  196,  218,   83,   88,  198,   88,  114], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 71, 240,  28,  52,  56,  39, 159, 248, 116, 225,  54, 142,  52, 260,\n",
            "        387,  32,  27, 145, 370, 214, 140, 250,  12,  86, 103, 229, 346,  61,\n",
            "        291,  15, 277, 239, 509,  15, 137, 427, 109, 285, 475, 157,  58,  58,\n",
            "         54,  85,  94, 153, 257, 115,  31,  24, 192,  79, 115,  99,  15,  71,\n",
            "        139,  92,  40, 477,  78, 268, 334,  42,  74, 182, 203,  86,  12, 852,\n",
            "         50, 471,  18, 455, 270, 469, 124, 192, 370, 536, 164, 182,  11, 166,\n",
            "         75, 379, 320, 558, 179,  21, 472, 346, 204, 214, 376, 293, 301,  45,\n",
            "         96,  19, 130, 137, 113, 374, 115, 302, 140, 532, 112,  66,  65,  32,\n",
            "        830,  37,  76,  20, 350,  66, 242,   9, 178, 136, 575, 383,  31, 229,\n",
            "         38,  86], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 137,   53,   44,   70,  193,  243,  405,   16,   51,  105,  171,   73,\n",
            "         192,   40,  154,  587,  194,   71,   64,  592,  267,  139,  151,  137,\n",
            "          21,  357,  811,   73,   33,   38,  252,   54,  779,  432,  233,  104,\n",
            "         410,  296,  139,   77,  148,  588,   57,  202,   33,  327,   75,   18,\n",
            "         219,   56,   59,  129,  143,   67,   82,   87,   44,  269,  293,   32,\n",
            "         303,   57,   39,   97,  222,  515,  101,   24,  110,   11,   57,  772,\n",
            "          70,  142,  582,  131,  358,   36,  338,  130,   10,   10,  884,  370,\n",
            "         330,  281,  111,  223,  170,   24,  395,   28,  330,  101,   31,  227,\n",
            "         248,  418,  129,   44,  138,  216,  122,   50,   15,  840,  224,   22,\n",
            "          23,  127,  578,   96,  419, 1898,  324,  240,   24,  141,  277,  132,\n",
            "         197,  231,  810,  437,   15,  441,  214,  269], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([127, 273, 832, 288, 105, 173, 146,  94,  35, 592,  29,  75, 955,  56,\n",
            "        168, 282, 207, 171, 147, 122, 230, 405, 155, 655,  31, 208,  12,  32,\n",
            "        127, 229, 195,  33, 110, 159, 329, 159, 254,  46,  82,  30, 167, 758,\n",
            "        487, 175, 186, 203, 233,  40, 129, 241,  52,  76, 702, 147,  32,  37,\n",
            "        441, 442, 395,  79,  13, 553, 106, 106,  77, 286, 369, 222, 462, 252,\n",
            "         47,  45, 100,  38,  24, 389, 254, 297,  83,  54, 181, 137, 273,  57,\n",
            "        164, 142,  37,  25, 241, 212,  76, 141, 278, 315,  87, 258, 340, 154,\n",
            "        101, 318, 327, 242, 635, 254, 303, 361,  92, 257,  85,  42, 304, 362,\n",
            "         47, 197,  44,  98,  78,  69,  45,  41, 596,  30, 140,  48, 101, 149,\n",
            "        208,  60], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([689, 349,  56, 354,  55, 117, 641, 120,  84,  85, 263, 195, 178, 302,\n",
            "         65, 103, 138, 185, 312, 367,  33,  32,  45, 229,  21, 342,  88, 145,\n",
            "        214, 110,  34, 164, 219, 669,   9, 586, 377,  90, 380, 283,  67, 282,\n",
            "        226, 662,  36, 352,  46, 271, 212, 360, 179, 235,  40, 455, 217,  27,\n",
            "        254, 169, 138,  28,  12, 184, 116, 219,  46, 129,  64,  15, 115,  87,\n",
            "         39, 110, 156, 297, 248,  31, 204, 296, 467, 207, 203,  28,  93, 194,\n",
            "         32, 108, 178, 354,   9, 166, 186, 153, 166, 211, 494, 108, 313,  29,\n",
            "         37, 122, 196,  66,  78, 190, 460, 376, 153,  39,  64, 166, 101, 335,\n",
            "         87, 466, 397, 116,  67, 259, 171,   9,  27,  13, 139,  73,  40,  30,\n",
            "        206,  59], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  19,   26,   80,  267,  381,  193,   84,  764,   71,  193,  481,  428,\n",
            "         308,   74,  447,  125,   76,  501,  357,   25,   42,  360,   89,  313,\n",
            "         266,   37,  209,  181,  374,  148,  190,  132,  126,   61,   78,  230,\n",
            "         138,  118,   84,   28,  443,  100,  261,  409,  201,  295,  217,  169,\n",
            "         235,   33,  334,  175,  126,  123,   46,   59,   85,  140,   68,  691,\n",
            "         269,   17,  542,  117,  719,  279,  206,  225,   36,  472,  100,   63,\n",
            "         468,  570,   78,   68,  437,  825,   78,  252,   75,  471,   50,  135,\n",
            "         220,  194,  540,  500,   34,  173,  197,   78,  156,  106,  420,   95,\n",
            "         242,  129,   57,  415,  138,  245, 1511,  263,   74,   22,  341,    9,\n",
            "         248,   53,  350,  465,  303,  107,  102,  431,  460,  130,  829,  644,\n",
            "         118,   47,  187,  181,   57,  112,   67,  599], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 174,  130,  201,   52,  284,  364,  119,   38,  766,  763,   61,  302,\n",
            "         132,  199,  134,   75,  412,  325,  384,   37,  673,  282,  381,  180,\n",
            "          99,   60,   44,  159,  532,  427,  177,  816,   33,  360,  132,   42,\n",
            "         589,  238,  340,  516,  340,  429,   31,   22,   93,  259,   51,   88,\n",
            "         916,   72,  258,  181,  264,  147, 1265,  144,   34,  109,  675,  637,\n",
            "          40,  156,  295,   72,   70,  217,  383,  679,  345,   32,  125,  154,\n",
            "         351,  291,  312,  613,  196,  388,   81,  188,  349,  206,   91,  111,\n",
            "          70,   21,  776,  160,  263,  313,  216,   53,  219,  104,  335,  272,\n",
            "         130,  319,   48,  204,   47,   80,  207,   44,  137,  148,  722,  114,\n",
            "          78,  128,  791,   77,   15,   50,   58,  221,  133,  860,  261,   65,\n",
            "          33,  115,   83,   46,   41,  179,   37,  409], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([161, 112, 294, 495, 179, 288,  53,  99, 204, 530,  51, 153, 386,  36,\n",
            "         35, 240,  35, 275, 237, 459, 497, 350,  68,  40, 158,  36, 471, 186,\n",
            "        258, 297, 229, 622, 225, 349, 102, 747, 175,  56, 107, 291,  33, 105,\n",
            "        153, 279, 227, 125,  51,  91, 441, 143,  47,  40, 155, 197,  63, 122,\n",
            "         59, 110, 131,  31, 319,  30, 539, 720, 302, 274,  82,  57, 181, 739,\n",
            "        129,  82, 126, 202, 370, 743, 358, 134, 133, 187,  42,  86, 104,  86,\n",
            "        175, 458, 784, 110, 169, 305, 403, 182, 103,  22, 788, 497, 554, 110,\n",
            "         74,  30, 116,  81,  42, 261, 294, 434, 525,  64, 519, 145, 396,  70,\n",
            "        138,  33,  35,  43, 308, 434,  27, 503,  79,  63,  77,  96, 114, 355,\n",
            "        530,  51], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "Epoch: 002/002 | Batch 040/080 | Cost: 0.4064\n",
            "tensor([ 276,   88,   40,   53,   77,  216,  251,   95,   92,  317,   74,  213,\n",
            "         509,  251,  245,  189,  237,  283,   74,  191,  181,   51,  184,  598,\n",
            "         337,  211,  124,   65,  160,  429,  118,  144,  217,  164,  192,  348,\n",
            "         458,  308,   87,  377,  133,   20,  224,  478,   68,   63,   97,   55,\n",
            "          83,  249,   89,  119,  362,  139,  515,  295,   53,   70,  418,   15,\n",
            "         525,   81,   28,   13,  145,  218,  102,  188,   80,  300,  298,  191,\n",
            "          13,  248,  365,  129,  283,  369,   43,   40,   94,   29,   77,  179,\n",
            "          90,  101,  137,  112,  123,  524,  198,   23,  314,  120,   59,   89,\n",
            "         193,  303,   83,  103,  176,   41,  417,  131,  104,  192,  274,   22,\n",
            "         190,  306,  121,  172,  138,  445,  171, 1108,  146,  224,   25,  394,\n",
            "         323,  108,   73,   66,   74,  492,   13,  179], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([306, 106,  33, 320, 141,  64, 162, 287, 102,  63, 156, 182, 268,  52,\n",
            "         68, 502, 109,  38, 156,  23, 319, 546, 416,  74,  44,  78, 160,  13,\n",
            "        879, 175, 299, 112, 206,  65, 488, 110, 535,  50, 434, 359, 238,  74,\n",
            "         33, 126,  31,  69,  44, 171, 185,  55,  45, 419, 744, 281, 564, 823,\n",
            "         35,  55, 584,  63, 160,  56, 932, 260, 397,  84, 171, 790,  84, 339,\n",
            "        145,  50, 469,  29, 331, 124, 290,  52,  73, 450, 117,  43,  82,   8,\n",
            "        109, 447, 129, 351, 108, 425, 106, 252,  18,  71,  68, 161, 127,  48,\n",
            "        339, 240, 100, 496,  89,  28, 360,  58, 193, 392,  12, 130, 486, 131,\n",
            "        228, 143, 271, 269,  98, 320, 223,  25, 137, 288,  40, 170,  80, 340,\n",
            "         57, 143], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  15,  519,  147,  219,   13,   39,  182,   73,  933,  411,  114,  556,\n",
            "         479,  199,  255,  200,  749,  123,  269,  345,  106,  384,  171,  250,\n",
            "          72,  263,   86,   77,  494,   50,  194,  286,   69,   15,  165,   58,\n",
            "          84,  199,   38,  241,  229,   41,  938,  112,   92,  153,  375,  437,\n",
            "         110,   78,  163,   41,  717,   93,  257,   26,   99,  381,   56,  237,\n",
            "         157,  235,  162,  139,  377,  302,   18,  208,  252,  559,   43,   14,\n",
            "         490,  360,  162,   55,  422,  158,  286,   76,  255,   67,  375,  123,\n",
            "          59,  246,   29,   20,  470,   41,   39,   57,  136,  100,  529,   70,\n",
            "         107,   70,  110,  164,  131,   87,  574,  596,  115,  299,   25,  112,\n",
            "          73,  226,   25,  111, 1816,  525,  146,   75,  234,  186,  323,   80,\n",
            "          15,  280,  623,   50,  341,   17,   50,  292], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 24,  97,  54,  53, 332,  52,  81,  81, 421, 137, 860,  36,  66,  40,\n",
            "         88, 455, 123, 163, 227,  34,  27, 133, 350, 444,  77,  66, 167,  19,\n",
            "         54,  33, 459, 679, 732, 549,  98,  80, 136, 105, 352, 255,  41, 443,\n",
            "         52,  81, 587,  92,  45,  51,  48,  73, 646, 132, 422, 201, 255, 173,\n",
            "         50, 579, 231, 658,  97, 114,   9,  67,  52, 182, 364,  95,  70,  31,\n",
            "         21,  77, 299, 354, 685,  91, 107, 202, 132,  48, 195, 197, 563, 413,\n",
            "         86, 126, 152,  63, 239, 321,  47, 162, 147,  87, 224, 132,  73, 394,\n",
            "        218,  25,  91, 865, 184, 240, 588, 319, 145, 113, 575, 106, 532, 291,\n",
            "        280, 536, 167, 236, 181,  31, 104,  60, 102,  43, 946,  50, 287,  55,\n",
            "        145, 644], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 677,   27,  481,  197,   94,   36,   51,   70,  314,   14,  185,  260,\n",
            "         600,  326,  160,   87,   65,  530,  207,   45,  371,   73,  231,  137,\n",
            "         227,  125,   35,  162,  118,  793,  229,   87,   48,  123,   69,   65,\n",
            "         235,   89,  192,   68,  579,  763,  130, 1072,   94,  485,   38,  118,\n",
            "         217,  103,   70,  330,  222,  115,  329,   49,  135,  205,  240,  484,\n",
            "         747,   19,  156,  525,   73,   31,  117,  108,  579,  120,  343,  161,\n",
            "         176,  727,   72,   48,  133,   71,  169,  306,  197,   72,  285,  213,\n",
            "          27,  182,  165,  187,  157,  679, 1127,  159,   34,  144,  361,  136,\n",
            "         273,  138,   44,   83,  353,   98,  417,   66,  502,   38,  591,  827,\n",
            "         342,   62,   77,   74,  137,   99,   26,  546,  515,  182,   89, 1208,\n",
            "          71,   20,   73,   19,   11,  157,  112,  522], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  42, 1404,  504,  179,   99,   50,   48,  135,  106,  274,   48,   89,\n",
            "        1233,   81,   99,   18,  118,  156,  107,  574,   32,   70,  116,  138,\n",
            "          16,   67,   57,  114,   18,  805,  559,  512,   63,   82,  334,  137,\n",
            "         247,   20,  114,  218,   89,  506,  250,  269,  135,  508,  167,  442,\n",
            "         303,  163,  781,  728,  446,  164, 2461,   50,  113,  253,  109,  386,\n",
            "          63,  155,   33, 1577,  144,   48,   80,  222,  285,  318,  558,  116,\n",
            "          68,  361,  110,  105,  370,  104,  194,  266,   77,   68,  447,   79,\n",
            "         282,  719,   29,  165, 1072,  943,  540,  292,   94,  142,   57,  159,\n",
            "          63,   98,   63, 1166,  142,   83,  161,  217,  196,  310,  163,   70,\n",
            "         262,  776,  106,  276,  656,  129,  616,  345,   18,   40,  295,  160,\n",
            "         609,   13,  642,  539,   59,  117,  221,  333], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 208,  168,  123,  376,   34,  493,    9,  651,  252,  167,  669,  122,\n",
            "          96,  613,  309,  152,  147,   66,  141,  235,   96,   45,   42,  265,\n",
            "         234,   38,   55,    9,  196,   21,  467,   67,   71,   61,  519,  142,\n",
            "         346,  516,  119,   87,  119,  242,   54,   41,  286,   36,  374,  193,\n",
            "        1277,  683,  390,   44,   60,  168,  140,   47,   38,  222,  478,  301,\n",
            "         119,  427,  694,   58,  435,   19,  429,  122,  162,  355,   76,  800,\n",
            "          53,  133,   92,   91,  158, 1022,   49,  189,  186,  134,  313,  177,\n",
            "          24,   36,  191,  168,   77,  525,   28,  233,   30,   26,  541,  226,\n",
            "          27,   29,  312, 1104,   57,   32,   75,  522,   95,  226,  138,   26,\n",
            "         287,  410,   97,  221,   13,   62,   40,  257,   62,   69,  202,  372,\n",
            "          48, 1012,   54,  262,  359,  258,  117,  528], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 107,   61,  104,  586,   17,  190,   97,  260,   59,  462,   35,   31,\n",
            "          98,   81,  159,   81,   50,   42,  165,   52,  387,  148,  183,   28,\n",
            "          95,   29,  105,   99,  121,   68,  174,  194,   87,   31,  667,   53,\n",
            "          31,   22,  410,  255,   12,  102,   59,   52,  368,  159,  469,   77,\n",
            "          57,  309,  190,  341,   69,   39,  166,   73,  419,  162,  209,  119,\n",
            "          27,  715,  515,  890,   46,  170,   62,   11,   53,  743,  141,   54,\n",
            "         252,  128,  470,  203,  275,  167,  287,  109,   88,  944,   74,   77,\n",
            "         169,   60,  235,  694,  461,  265,  161,   48,   79,   64,  460,  163,\n",
            "         163,   88,   30,  688,   60,   68,   54,  121,  200,   55,  197,   34,\n",
            "          67, 1123,  344,   75,  219,  109,   31,  352,   50,   45,  287,  300,\n",
            "         767,  108,   21,   80,   61,  646,  177,  123], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  98,   40,  191,   73,  350,  161,  566,   55,   76,  265,  557,  447,\n",
            "          14,  173,  191,   66,   87,   48,   87,   43,   96,  320,   74,   66,\n",
            "         496,   36,   51,  163,   95,  149,  111,  689,  286,   80,   75,  113,\n",
            "         209,  225,   25,   46,  185,   65,   94,  487,  378,  407,   31,   85,\n",
            "         121,   46,  276,   39,  114,   69,   56,   38,  429,  166,  311,  175,\n",
            "         366, 1086,  475,  327,  130,  304,   39,   95,   35,  765,  132,  109,\n",
            "         385,  335,  147,  353,  242,  273,   72,   58,   52,  112,  255,  161,\n",
            "          59,  139,   37,  103,  233,   54,  127,  318,   93,  132,   31,  470,\n",
            "         161,  211,   32,  312,  183,  522,   55,   97,   15,   78,   34,  162,\n",
            "         107,  193,   92,   49,  231,   38,  327,  180,   86,  131,  684,   10,\n",
            "         507,   63,   68,  227,  273,  314,   83,  299], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  74,   32,  261,  254,  211,  124,   69,  282,   91,  189,   85,   19,\n",
            "          35,  515,  110,   86,  101,   13,  207,  159,   69,   66,  451,  187,\n",
            "          40,   16,  138,   22,  279,   87,  115,   80,   47,   57,   65,   10,\n",
            "          91,  149,   55,   63,   33,   74,   20,   54,  225,  120,  243,   38,\n",
            "         662,  249,  403,  666,  167,  457,  212,  470,  171,  496,  819,  316,\n",
            "          39,  491,   48,   36,  649,   98,  404,   37,  453,   51,  110,  281,\n",
            "          56,   66,  106,  220,   78,  251,   53,  178,  240,  220,  116,  245,\n",
            "         304,  140, 1036,  311,  415,   98,   93,  119,  217,   65,  184,   47,\n",
            "          31,  278,  218,  223,   94,   47,   76,  149,   50,  208,  206,   75,\n",
            "          57,  138,  247,   70,  187,   41,  216,  540,   93,  361,   32,  177,\n",
            "         211,  364,  309,   19,  187,  170,   63,  419], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  22,   59,  204,   40,   48,  153,  277,   96,  133,   56,  253,   51,\n",
            "          26,  286,  139,   16,   73,  433,  364,  342, 1421,  445,  319,  348,\n",
            "         141,  103,  106,  105,   67,  367,  718,  417,  197,  159,  283,   58,\n",
            "          83,  303,   44,   45,  340,   20,  298,   87,  360,  488,  253,  107,\n",
            "         165,  270,  101,  170,  184,  192,  108,  371,  758,   22,   83,  196,\n",
            "         981, 1200,   84, 1194,  435,   12,  115,  733,  175,   83,  262,  126,\n",
            "          94,  365,  256,   57,  179,  185,   57,   32,   48,  306,  348,  191,\n",
            "          36,    9,  268,  192,  171,  377,  144,  124,  191,  125,  510,  513,\n",
            "         173,  194,  104,  293,  141,  232,  322,  278,  163,  510,   10,  444,\n",
            "          28,  203,  347,  238,   97,  569,  392,   71,   87,  550,  440,  106,\n",
            "         285,   42,   60,  567,  386,  180,   50,  426], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 545,   66,   37,  475,   77,   80,  391,  206,  303,  267,   68,   32,\n",
            "          91,  234,  231,  510,  279,   70,   94,  415,   78,  629,   23,   62,\n",
            "          55,  100,  137,   12,   87,  352,   50,  201,  103,  146,  184,  855,\n",
            "          92,  148,  183,   59,   99,  135,  400,   45,   79,  488,  115,  262,\n",
            "          71,   41,  361,   68,  227,  750,   98,  386,   61,   25,  310,  149,\n",
            "         329,   23,  574,   63,  261,   92,  136,  149,   32,  143,  220,   53,\n",
            "         242,   99,  297,  648,  173,  125,  301,  212,   53,   44,  153,  253,\n",
            "         125,   72,  331,  335,   11,  287,  458,  464,  573,  101, 1243,  148,\n",
            "         333,  111,  114,  495,  433,   32,  151,  632,  159,   39,  287,  103,\n",
            "         223,  126,  168,  311,  111,   93,  330,  568,  548, 1230,  163,   60,\n",
            "         172,  491,  771,   50,   24,  319,   85,  180], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 190,  232,   82,  112,   48,   15,  106,  426,   78,  111,   95,  156,\n",
            "         411,   73,   68,   54,   89,  596,   90,   43,  253,   46,  205,   66,\n",
            "          97,  111,   64,  367,  270,  451,  121,   59,  774,  265,  236,  285,\n",
            "         415,   20,  365,  284,  380,  145,   65,  201,  286,  277,   75,  177,\n",
            "         102,  163,  377,   55,   86,  123,  109,  387,   28,  445,  152,  578,\n",
            "         388,  632,  104,  320,   39,   78,   89,   89,   31,   79,  203,   65,\n",
            "        1036,  220,   66,   42,  168,   37,  332,   94,  422,   40,  420,   59,\n",
            "         165,   16,  263,  933,  105,  313,  930,  219,  696,   36,  812,  355,\n",
            "         565,  539,  352,  322,  678,   20,  166,   19,   97,  271,   62,  212,\n",
            "          36,   93,  260,  442,  126,   17,  147,  822,   90,  382,  183,  577,\n",
            "          45,   17,  319,  151,  677,  160,  969,  460], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 290,   74,  312,  224,  148,   75,   21,  185, 1041, 1925,  525,   72,\n",
            "         424,  257,  244,  672,  269,  280, 1192,   68,  168,   35,  600,   59,\n",
            "         356,   85,  175,   49,  635,  167,   51,   11,   29,  273,   69,  824,\n",
            "          51,  796,  264,   52,  150,  279,  246,  312,  391,   73,  276,   13,\n",
            "         357,  350,  131,  407,  160,  174,  266,    9,  138,   67,  504,   12,\n",
            "          49,   66,  114,  213,   26,   27,  689,  267,  551,  215,  135,  164,\n",
            "         396,  107,   80,  100,   97,  426,   21,  139,  163,   98,  176,   32,\n",
            "         142,   83,  322,   66,  394,  293,   56,   69,  151,  217,  205,   65,\n",
            "          96,  450,   26,   90,  245,   80,  222,  130,   31,  160,   21,   11,\n",
            "         112,  230,  377,  198,   27,   37,  233,   48,   27,  453,  422,  135,\n",
            "         319,   58,  745,   21,  545,   46,   57,  401], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  12,  104,  408,   21,  293,   42,   55,  114,  400,   12,  709,   55,\n",
            "         310,  105,  360,  336,   69,  317,  256, 1142,   50,  102,  124,  166,\n",
            "         162,   91,   29,  165,  202,  333,   98,   27,  138,   85,  362,   53,\n",
            "          60,   47, 1370,  227,   62,   60,   64,  197,   25,   28,  617,  104,\n",
            "          96,  406,   42,   98,  284,  663,  987,  954,  111,  117,  145,  405,\n",
            "         630,  453,  189,  516,   12,   36,  134,  253,   21,  586, 1399,   60,\n",
            "          54,  212,  111,   57,  319,  335,  528,   52,  211,   38,  978,  645,\n",
            "         146,  181,  744,  102,   54,   57,  133,   28,   98,  237,   50,  211,\n",
            "          68,   17,  125,   43,  194,   61,   87,  106,  202,   63,  331,  439,\n",
            "         717,  365,   48,  300,  141,  151,  180,  575,  431,  210,  226,   49,\n",
            "         192,  119,  306,   41, 1871,   30,  249,  121], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 107,   92,  249,  163,  276,   44,  194,  387,   63,   57,  429,  464,\n",
            "         299, 1353,  568,  179,   46,  305,   59,   24,  284,  250,  368,  392,\n",
            "         100,  102,   44,  690,  333,   66,  162,   44,   62,   41,   53,   57,\n",
            "          59,  415,   97,  347,   64,   13,  203,  221,  427,  163,   45,   41,\n",
            "         541,   10,   57,   54,  110,  182,  272,   62,  487,  214,  106,  198,\n",
            "         236,  245,   49,   70,   39,  589,   22,  540,  139,  440,   55,   77,\n",
            "         422,  178,  469, 4250,  196,  885,  101,   79,   16,   43,  220,  369,\n",
            "         123,   12,  129,   81,   61,  258,  198,   69,   50,   46,  134,   39,\n",
            "         149,  568,   57,   93,  397,   44,   93,  463,  102,  165,   40,  223,\n",
            "          94,  508,   41,  645,  203,  150,  318,  183,  402,  305,  181,  102,\n",
            "         195, 1293,  258,   63,   81,   66,   38,   79], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 111,   71,  140,  146,  197,  256,  411,  684,   28,  419,   90,   90,\n",
            "         143,   63,  164,  392,  432,  189,  128,  351,  526,   83,   77,  110,\n",
            "         352, 1300,   38,  407,  164,  125,  177,  493,   52,  116,   90,   57,\n",
            "          49,   33,   42,   26,  762,  131,  151,   56,   78,  118,  616,  159,\n",
            "         288,  144,   84,  701,   38,   67,   68,   64,  107, 1024,   52,   74,\n",
            "         307,  430,   77,   75,  142,  212,  266,   76,  219,  230,   51,  127,\n",
            "         322,   91,   19,  418,  670,   50,   94,   38,  187,   65,  213,  304,\n",
            "         238,   86,   24,  109,  547,   61,  687,   49,  250,  253,  304,  174,\n",
            "         643,  210,  238,  615,  647,   25,  529,   19,   67,   71,   24,   75,\n",
            "         130,  473,  856,  115,  169,   25,  376,   65,  579,  289,  112,   98,\n",
            "         338,   35,   56,   38,   38,  184,  280,  328], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([109, 132,  66, 843, 389,  29, 155, 145,  12, 187, 889,  95,  37,  73,\n",
            "         29, 484, 237,  72,  66, 111, 188,  45, 302, 356, 134, 769,  73, 294,\n",
            "         52, 499, 262, 113, 243,  76,  87, 183,  74,  65, 605,  88,  35, 366,\n",
            "         95, 305,  39, 284,  26,  29, 292, 362, 446, 218, 133, 152,  48,  63,\n",
            "        631,  90,  37,  20, 157, 174, 151,  61,  35, 262, 207,  55, 118,  34,\n",
            "        929,  79, 403, 216, 334, 154, 123, 284, 272,  35, 147,  72,  71, 438,\n",
            "        318,  41, 386, 474, 106, 227, 150, 238,  60, 264, 317, 118,  69,  55,\n",
            "        363,  92, 581,  63, 133, 791, 236, 225,  62, 781, 310, 191,  81, 316,\n",
            "         87,  49, 224, 254,  62, 389, 158, 516, 159,  26, 310, 147, 568, 204,\n",
            "        338,  89], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  51,  220,   58,   27,  150,  164,  282,   74,  166,   92,  892,  158,\n",
            "         302,   92,  255,  100,   54,  711,  552,  115,   93,  172,   86,   52,\n",
            "         217,    8,  117,  193,   91,  118,   24,  944,   93,   62,  157,  121,\n",
            "          80,   81,   12,   62,   95,   87,   21,   31,   92,   31,  152,  102,\n",
            "         537,  584,  198,   47,   17,  501,   29,  311,   43,   56,  757,   31,\n",
            "          58,  335,  115,  533,   74, 2670,  111,  102,   99,  193,  343,  139,\n",
            "         120,  209,   39,  281,   91,   76,   82,  190,  215,  106,  200,   15,\n",
            "          79,  211,  316,  145,   39,  231,  118,  174,   20,  289,  177,  152,\n",
            "          53,  142,  546,   80,   36,  441,  452,   48,  134,   97,   36,  198,\n",
            "          47,   68,   26,  336,  725,  456,    9,  248,  409,  261,   70,  833,\n",
            "         132,  285,   41,  137,  234,  737,  103,   94], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 117,  120,  135,  115,  206,  209,   18, 1243,  337,    9,  101,   72,\n",
            "          44,   44,  331,  756,  217,  478,   48,   74,  355,   87,   55,   21,\n",
            "         910, 1275,   84,   95,  148,   76,  150,  605,   82,   37,  124,  255,\n",
            "         107,  115,  754,   91,  363,  163,  294,  231,  212,   98,   64,  412,\n",
            "         189,   86,  427,   58,  134,   23,   20,  217,  202,   89,   61,  171,\n",
            "         301,  481,  181,  631,  272,   68,  259,  111,  526,  150,   67,   42,\n",
            "         224,  138,   56,  107,   23,  300,  117,  325,  527,   55,  263,  470,\n",
            "          48,  104,  245,   61,  668,   78,   44,   31,  200,   69,  390,   57,\n",
            "         191,  173,  447,  381,  102,  120,  542,  521,  208,  375,  280,   34,\n",
            "          68,  172,  284,   78,  198,  301,   55,  209,   76,   22,   54,  602,\n",
            "         642,   69,  137,  353,   71,  257,  382,  120], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "Epoch: 002/002 | Batch 060/080 | Cost: 0.3999\n",
            "tensor([ 412,  740,  433,   46,  344,  184,  402,   57,  785,  331,   72,   91,\n",
            "          18,  343,  850,  469,  231,  444,  431,  208,  219,  358,  376,  937,\n",
            "         335,  267,  627,   72,   47,   59,   39,   91,   61,  201,  161,  644,\n",
            "         284,   84, 1342,   85,  198,  546,   21,  293,   45,  335,  140,  161,\n",
            "         359,  534,  353,  132,  461,  499,   84,  177,   92,   80,   41, 2022,\n",
            "          90,  293,   25,  250,  223,  148,   99,  141,   49,   37,   77,   85,\n",
            "         123,   28,   99,  757,  664,   42,   15,  259,  258,  423,  248,   57,\n",
            "          22,   26,  138,   55,  153,  319,  258,  519,   46,   54,   43,  202,\n",
            "         429,  557,  326,   38,  248,  318,  136,   51,   97,  251,  162,   95,\n",
            "         101,  139,   48,   61,  114,   47,  170,  130,  325, 1240,   69,   25,\n",
            "         105,   30,  488,  731,  127,  320,  336,  153], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  20,  586,   42,  104,  289,  922,  135,   48,   42,  239,   99, 1018,\n",
            "          94,  120, 1493,  347,   41,   45,   81,  202,  298,   30,   44,  213,\n",
            "          25,   93,  232,  832,  793,  198,  724,   58,  376,   50,  191,  548,\n",
            "         147,  195,   68,  111,  593,  332,  197,  152,   47,   43,  293,   14,\n",
            "          20,  436,   48,   94,  309,  520,  197,   40,  133,  352,  277,   69,\n",
            "         104,  936,  146,   23,  173,  137,  425,  357,   92,   35,   59,   45,\n",
            "          33,  448,  409,   23,  459,   59,   42,   65,  593,   50,   88,   81,\n",
            "          63,  105,  371,   53,  567,  181,   30,  141,   54,  296,  152,  620,\n",
            "         295,   38,  350,  472,  122,  284,   99,  453,  140,  453,  164,  499,\n",
            "         186,  395,   46,  167,   45,  225,   31,   88,  132,   52,  350,  132,\n",
            "          41,   91,   61,  386,  292,   41,  203,   35], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  15,  459,  254,  134,  390,  181,  562,   80,  392,   71,   94,   36,\n",
            "         196,  134,   87,   83,  245,   63,  411,  347,   66,  202,   48,  234,\n",
            "         217,  430,   14,   79,   48,   89,   47,   21,   67,  275,   74,  253,\n",
            "         278,   23,  571,  668,  150,    9,  235,  195,   41,  422,   24,  113,\n",
            "          73,  452,   68,  455,   76,   70,   66,  373,   33,  169,  338,  255,\n",
            "         177,  112,  258,   77,  399,   23,  275,  372,  175,  112,  113,  268,\n",
            "         317,  179,  511,  121,  394,   50,  308,  178,  222,  736, 1001,  671,\n",
            "         176,   41,   31,  708,   81,   90,  116,   35,   43,   56,  556,   32,\n",
            "          23,  462,  533,   15,  205,  235,  205,  138,   78,   41,   48,  100,\n",
            "          56, 1001,  109,   23,  345,  153,  568,  417,  399,  130,   44,   30,\n",
            "         377,   68,   56, 1295,   61,  199,  182,  918], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 215,   99,  152,  141,   82,   32,  218,  293,  152,  105,   55,  432,\n",
            "         263,  112,  194,  759,  371,   50,  241,  707,  130,   32,  166,  147,\n",
            "          30,  330,  333,   49,   54,  109,  120,  237,   40,   59,  263,  399,\n",
            "         346,  275,  288,   42,   51,  301, 1444,  151,  166,   32,  412,  591,\n",
            "         518,   82,   98,  156,  161,  126,  475,   84,  625,  191,  362,   98,\n",
            "         187,   47,  130,  141,  193,  741,  215,   89,  140,   83,  876,  249,\n",
            "          62,  131,  143,  343,  420,    8, 1467,  106,  576,   67,  249,  585,\n",
            "          23,  177,  158,  138,  221,  207,  140,   86,   73,  446,  452,  572,\n",
            "          73,   36,   65,   82,   79,  683,   55,  370,  416,   49,  107,  176,\n",
            "          25,   99,  135,  238,  444,   67,   79,   35,   38,   53,  183,  120,\n",
            "          85,  161,   51,   66,    7,    8,  265,  656], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 152,   44,  110,  173,   66,  611,  424,   28,  571,  249,  198,  109,\n",
            "         219,   58,  292,  153,   56,  123,   47,   20,   94,   54,  103,  114,\n",
            "          20,   19,   18,   62,   20,   27,   79,  204,  131,  448,  145,   93,\n",
            "         124,  185,   96,  540,  111,   90,  297,   69,   30,  162,   32,   43,\n",
            "          24,   53,  173,  673,  249,  244,   64,  367,   14,   64,  102,  225,\n",
            "         105,   26,   19,   72,   79,  809,   63,  344,   31,  539,  240,   66,\n",
            "         197,  269,   65,   56,  201,  238,   34,   43,   48,   51,  350,  595,\n",
            "         107,   31,  244,   66,   16,  681,  539,  477,   36,  737,   42,  863,\n",
            "          74,   94,  393,   63,  124,  485,  302,  118,   66,  132,  499,  513,\n",
            "         308,  231,  158,  122,   60,  190,  277,  359, 2742,  297,   71,   90,\n",
            "          71,  701,  375,   66,  295,  517,  139,   97], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([756, 308,  12,  95, 300, 328, 217, 253, 311, 445, 348, 102, 106, 209,\n",
            "        198, 177,  71, 477, 500, 241,  52,  34, 293,  47, 161, 391,  20, 206,\n",
            "        123, 151,  86, 118,  54,  36, 240,  91, 135, 136,  95,  70,  70, 369,\n",
            "        562,  60, 447,  68,  96, 318,  36, 279, 175, 875, 239,  44, 116, 428,\n",
            "        296, 187, 475, 101, 197, 279,  95, 231, 211, 274, 303, 117, 155,  20,\n",
            "        234, 115,  40,  13,  25, 250,  38, 189,  22, 621, 389,  40,  85, 676,\n",
            "        399, 262, 111,  46, 150, 332, 111,  57, 135, 237, 713,  37, 192,  10,\n",
            "        542,  46, 175, 134, 118, 320,  22,  70, 757, 273,  54,  78, 101, 154,\n",
            "         58,  47, 223, 231, 121, 131,  85,  32, 354, 127, 430, 305,  40, 184,\n",
            "        132,  43], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 480,  278,  197,   39,  198,   45,   29,   20,  199,  273,  714,  103,\n",
            "         140,  366,  375,  292,   16,   16,  448,  185,   20,  115,   70,   86,\n",
            "         160,  566,  138,  482,  256,  488,  230,   57,  188,   99,  120,   52,\n",
            "         142,   20,  350,  343,  262,  108,   30,  457,   96,   47,  245,  239,\n",
            "         241,  145,  289,  192,  963,  527,   79,  327,  402,  216,   83,  139,\n",
            "         178,   23,   41,  667,  127,  249,  949,  152,   15,  103,  348,  200,\n",
            "         115,  271,   49,  138,   59,  660,   41,   65,  220,  118,  381,  111,\n",
            "         139,  202,   73,   77,  217,   40,  353,  126, 1501,   10,  237,  520,\n",
            "          66,  114,   57,   31,  310,  411,  245,   34,   55,  646,   38,  140,\n",
            "          91,  102,   38,   19,  163,   39,  432,  206,   76,  275,   20, 1111,\n",
            "         336,  143,  152,  215,   69,  157,  319,   53], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  59,  336,  350,  106,   43,  129,  102,  515,  138,  182,  355,  276,\n",
            "         102,  129,  597,  245,  354,   42,   53,  513,   29,   52,  289,  594,\n",
            "         119,   94,  158,  415,   86,   63,   29,  232,   69,   15,  115,  766,\n",
            "         114,  247,   33,   92,   79,   42,  330,  204,  198,  150,  140,   37,\n",
            "          57,  612,  225,   37,  208,  253,   53,  167,   67,  267, 1007,  170,\n",
            "         654,  214,   65,   83,  715,  431,  775,   20,   23,   46,   54,  125,\n",
            "         154,   94,   69,   51,  449,  369,  140,   28,  142,  142,  427,   82,\n",
            "         269,  226,  135,  246,  430,   40,   44,  188,  392,  405,  245,  111,\n",
            "          37,  271,   75,   91,   70,   82,  595,   21,  174,  152,  849,   48,\n",
            "          68,  124,  520,  619,  324,  775,   25,  277,  939,  121,  241,  167,\n",
            "          79,  585,  166,  115,  174,   49,  435,  323], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 200,   44,   41,   46,  118,  103,   42,  142,  512,  415,  338,  245,\n",
            "         162,  209,  372,   92,  256,  138,  162,   63,   59,  150,  211,  191,\n",
            "          26,  317,  362,   58,  615,   50,   85,  123,   52,  250,   87,   86,\n",
            "         108,  403,  102,    8,   87,   70,   89,  454,  205,  268,   38,   24,\n",
            "         251,  370,  277,   57,   73,   92,  282,  126,  102,  705,  202,  441,\n",
            "         198,   36,   39,  484,  192,  121,   24,  146,  462,  412,   93,  183,\n",
            "         204,  218,  494,  361,   53,  526,  108,  245,   32,  231,  308,   41,\n",
            "          47,   37,  434,  209,  185,  189,  391,   31,  394,   36,   87,   58,\n",
            "         198,  190, 1329,  360,  229,   66,   47,  150,  110,  395,  344,  250,\n",
            "          43,  265,  174,   51,  236,   55,  112,  444,  125,  692,  359,  504,\n",
            "          83,  311,   52,  246,   47,   48,  144,   61], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 115,  352,   71,   44,  706,  161,  337,   94,  295,  975,   10,   22,\n",
            "         169,   36,   93,  385,  154,   98,   58,   65,  222,   74,   93,  112,\n",
            "          56,  244,  197,  161,  718,   97,  352,  295,  152,  874,  724,   15,\n",
            "         374,  196,   44,  226,  321,  306,  146,  311,  284,   12,  426,   59,\n",
            "         169,   80,  320,  572,  286,   35,  467,  262,   41,  675,  410,  612,\n",
            "          53,  332,  148,   54,  220,  686,  194,   26,   92,  535,  141,   65,\n",
            "          70,   41,  227,  495,  157,   85,   28,  786,   78,   37,   39,  225,\n",
            "          89,   81,  319,  249,  132,   48,  147,  147,  273,  180,  396,  239,\n",
            "          46,  309,  425,   34,   58,  129,   89,  307,  181,  280,   65,  353,\n",
            "         596,  181,  138,   33,   46,   18,  254,   76,  175,   13,   56,  174,\n",
            "         476,   99,  376,  488,  181,   83,   64, 1047], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 439,  266,  257,   17,   47,  165,  135,  739,   10,   66,  144,  221,\n",
            "         308,   45,  555,  565,   70,  304,  479,  289,  218,  457,  629,  144,\n",
            "         138,   55,  241,  111,  319,  341,  479,  368,  138,  324,  281,   17,\n",
            "         467,  181,  245,  201,   14,  288,   95,  327,  238,  115,   61,  168,\n",
            "          87,  315,   17,  221,   92,  409,  240,  253,   74,  498,  372,  163,\n",
            "         331,  401,   30, 1854,  142,  687,  123,   72,  261,  143,  192,   81,\n",
            "         249,   57,  178,  336,   61,  225,  128,   16,  247,   38,  209,   46,\n",
            "         114,  727,  306,   26,   83,  139,   82,   37, 1337,  278,  178,  298,\n",
            "          25,  250,  165,  152,  357,  105,   64,   79,  198,  598,  416,  405,\n",
            "         376,   13,  159,  134,  236,  117,  375,  205,  222,  143,  332,   80,\n",
            "         183,   78,  188,   84,  164,  195,  582,  348], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 104,  319,   10,  329,   99,   90,   16,   92,   22,   69,  152,   90,\n",
            "         274,  518,   25,   64,   34,  264,  712,   88,  326,   84,  123,   64,\n",
            "          49,   66,  193,  129,   27,  156,   71,   98,  267,  254,   46,   66,\n",
            "          15,  223,  315,   86,  293,   63,  297,  118,   38,  101,   99,   31,\n",
            "         202,   88,   23,  422,  432,  226,  137,  463,  487,  115,   74,  155,\n",
            "          97,  393,  718, 1304,  358,   49,   28,  154,   58,   80,  366,  552,\n",
            "         408,  290,  198,  277,  194,  232,   38,  251,   42,   95,   72,  379,\n",
            "         853,  217,  191,  394,   67,  236,   82,   21,  448,  266,  199,   78,\n",
            "         504,  591,  195,  133,  236,  457,  339,  108,   63,  241,  105,  229,\n",
            "         239,  195,   24,   87,  181,  454,   75,   10,  149,  399,  117,  367,\n",
            "          19,  361,  111,   17,  280,  475,   45,   33], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  72,  110, 1129,   38,  127,   57,   82,  375,  353,  246,  328,   59,\n",
            "         410,  387,  332,  388,   96,   24,  169,  478,  295,   95,  105,  128,\n",
            "         432,   20,  336,  238,   38,   18,  132,  831,   77,  204,  207,   56,\n",
            "          85,   46,  296,  230,   29,  533,  449,   17,   74,  172,  111,  295,\n",
            "         162, 1002,   55,   41,  464,   84,   91,  500,  273,   59,   86,  597,\n",
            "          34,  559,  308,  115,   21,  370,   79,   68, 1059,  403,   27,   67,\n",
            "         652,   24,  378,   36,  102,  305,  594,  250,  226,   54,  185,  334,\n",
            "          82,   80,  133,   28,  212,  296,   50,  446,  146,  206,  123,   79,\n",
            "         101,  188,  111,  390,   33,  118,  273,  285,   49,   43,  347,  248,\n",
            "         243,  326,  189,   38,   82,   62,  243,   79,  113,  260,  153,  173,\n",
            "         443,   51,  304,  479,  968,  243,  448,  237], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 613,   96,  114,   96,  145,  194,   31,   89,  330,   68,   15,  419,\n",
            "          44,   81,  107,   67,  189,  258,  218,   62,  151,  154,  309,   95,\n",
            "          61,  389,   89,  340,   57,   98,  157,  319,   32,   68,  126,  205,\n",
            "         269,   49,  251,  260,  373,   64,   25,   28,  119,  115,  179,  953,\n",
            "          88,  161,   34,  294,  145,  230,  118,  584,  128,   51,   20,  373,\n",
            "         506,  387, 1649,  129,  197,  103,  258,  129,   64,  100,  205,  209,\n",
            "         286,  486,  226,   21,  316,  523,  227,  224,  105,  143,   77,  293,\n",
            "         163,  219,   48,  165,  459,  298,   41,   39,   52,  137,  269,   26,\n",
            "         200,  594,   56,  293,  138,   74,  261,   58,  772,   70,   49,  118,\n",
            "          58,   26, 2135,   40,  124,   41, 1178,  193,   89,   81,  336,  293,\n",
            "         165,  189,  219,  422,   40,  183,  326,  321], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  87,   99,  134,   96,  422,   79,  128,  257,   98,   37,  198,  104,\n",
            "          68,   84,   30,   78,   66,  357, 1033,   12,  268,   38,  226,  179,\n",
            "         243,  221,  265,   17,  145,  293,   99,   21, 1207,  142,  231,   68,\n",
            "         293,  297,  307,   45,  323,  354,  363,  193,   53,   43,   47,   84,\n",
            "         491,  346,   39,  303,  182,  451,  218,   58,   25,   19,  135,  188,\n",
            "         124,  303,  551,   29,  131,  202,   73,  112,  204,   56,  712,  206,\n",
            "          98,  215,  222,  543, 1496,  359,  422,  360,  312,  150,   33,   22,\n",
            "         123,  508,   96,  684,   68,  105,  710,   41,  404,  185,  526,  205,\n",
            "         145,  135,  235,   41,   52,  136,  546,  112,  102,   73,  171,  121,\n",
            "         147,  408,  303,   93,  593,   92,   90,  130,  137,   20,  100,  346,\n",
            "         100,   35,  369,  603,  116,  552,   66,  116], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([  57,   49,   24,  217,  224,   96,  731,   68,   16,  111,  543,  327,\n",
            "          99,  230,  137,   87,  742,  152,   29,   56,  128,   78,  439,   67,\n",
            "          26,  799, 1219,   38,  447,  326,  297,  117,  109,   63,   55,  394,\n",
            "         119,  625,   36,   72,  155,  330,  245,  128, 1008,   62,  239,  129,\n",
            "         238,   63,  503,   33,  185,  168,   65,  348, 1083,  112,  282,   82,\n",
            "         116,   25,   64,   15,   83,   95,  103,   11,  245,  646,   31,  231,\n",
            "         158,  356,  157,  396,  105,   61,   38,  325,   32,  184,   83,  183,\n",
            "         106,  412,   51, 1145,  249,   75,   34,  475,   49,  293,  215,  382,\n",
            "         321,   39,  469,   23,   76,  210,  141,  272,  242,   63,  119,   63,\n",
            "         427,  236,  389,  139,  261,  136,   69,  171,  431,   91,  279,  670,\n",
            "         104,  202,   59,  192,   72,   88,   62,  440], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 259,   50,  239,   23,  193,  217,  477,  315,  569,  229,  123,  346,\n",
            "          73, 1714,  115,   75,  320,  165,  151,  219,   49,   85,  159,  352,\n",
            "         186,   74,  427,  311,  433,  352,   65,  333,  420,  338, 1111,  344,\n",
            "         370,  268,  196,  333,   90,   63,  684,  301,   55,   72,  101,   50,\n",
            "         346,   72,   49,  147,  434,  231,  123,   68,  209,  141,  144,  157,\n",
            "          67,  108,   45,  437,  306,  367,   47,   67,  296,  193,  365,  198,\n",
            "         325,   26,  369,  224,  135,  266,  178,  123,   39,  152,  124,   86,\n",
            "         519,  571,  335,  293,  172,  103,  104,   74,   40,  230,  518,   36,\n",
            "          89,   90,  116,  123,  234,   92,  308,   32,   61,   71,  875,   69,\n",
            "          25, 1303,  327,  169,   85,   31,  150,   72,  204,   60,  292,   77,\n",
            "          83,  299,  278,  328,   10,  488,  218,  196], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([ 376,  145,  257,   38,   48,  191,  200,  608,   99,   19,   59,  268,\n",
            "         249,   51,  158,   71,  518,   16,  367,  283,  455,  130,   19,   62,\n",
            "          70,  235,   59,  115,  123,  230,  197,  802,   36,  263,   71,  486,\n",
            "          28,  303,  141,  111,  799,  222,   39,  248,   92,  166,   53,   30,\n",
            "         227,  137,  121,  614,   42,  732,  446,   95,   21,  183,   16, 1108,\n",
            "         251,  137,   43,  318,   96,  112,  211,   79, 4065,  141,  310,   14,\n",
            "         376,   54,   98,   88,  180,  107,  107, 1015,   56,   71,   42,  845,\n",
            "          54, 1463,   88,  498,   78,   86,  108,  150,  370,   72,  152,   36,\n",
            "         157,  149,  556,   27,  412,   49,   82,   39,  253,   18,   90,  445,\n",
            "          99,  115,   61,  171,  144,  257,   68,  424,  233,   75,  851,  371,\n",
            "         394, 1027,  148,  413,   60,  157,   91,   33], device='cuda:0')\n",
            "torch.Size([128, 4250, 300])\n",
            "tensor([326,  92, 167, 224,  72, 421,  75,  63,  20, 293, 813, 148, 160, 106,\n",
            "        171,  56, 262, 298, 132,  76,  68,  83, 404, 127, 676, 214,  29, 420,\n",
            "        148, 104, 177, 138, 393, 376, 341, 132,  46,  27, 396,  96, 207,  26,\n",
            "        188,  62, 125, 576,  66,  52, 350,  42,  29,  61, 321, 472, 105,  73,\n",
            "         32, 148,  30, 440, 616, 306,  47, 285, 639, 150,  75, 638, 573, 758,\n",
            "        711, 604, 232, 187,  49, 395], device='cuda:0')\n",
            "torch.Size([76, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([76, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([122, 4250, 300])\n",
            "Epoch: 002/002 Train Acc.: 0.00% | Validation Acc.: 0.00%\n",
            "Time elapsed: 1.23 min\n",
            "the precision_recall_fscore_support of F-score_train:  (0.14932983682983683, 0.07692307692307693, 0.0968095734053181, None)\n",
            "the precision_recall_fscore_support of F-score_val:  (0.045787545787545784, 0.027472527472527472, 0.022483909951940644, None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VqIyq4Mkp7Ed",
        "outputId": "58adabbb-97b1-4b12-dcd7-e6920e106cc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "plt.plot(range(1, NUM_EPOCHS+1), train_loss_lst, label='Training loss')\n",
        "plt.plot(range(1, NUM_EPOCHS+1), valid_loss_lst, label='Validation loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Binary cross entropy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEGCAYAAACzYDhlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de3xdZZn3/88357TpAdqUlhZslQKCUpBQHDsiB/0B6tCRwaGMvwEExXFAUB4fB2ZQwcPvNSgz4zACz6+CchidygAyFUFUoIKc2hRKpUCllAKBtpQCTdM2aQ7X88daCTvJ3slum5U06ff9eq1X9l7rXjvX6iFX7nVf674VEZiZmWWpZKgDMDOzkc/JxszMMudkY2ZmmXOyMTOzzDnZmJlZ5sqGOoDd0cSJE2P69OlDHYaZ2bCydOnSNyKiNt8xJ5s8pk+fTn19/VCHYWY2rEh6qdAx30YzM7PMOdmYmVnmnGzMzCxzHrMxs91Ca2srDQ0NNDc3D3Uo1o+qqiqmTZtGeXl50ec42ZjZbqGhoYExY8Ywffp0JA11OFZARLBx40YaGhqYMWNG0ef5NpqZ7Raam5uZMGGCE81uThITJkzY4R6ok42Z7TacaIaHnfl78m20AfT8+s38cvlaJo+tYvK4SvYZW8WUcdXsNarc/4nMbI/mZDOAVq7fzH/c/zw9lwiqKCthn7GVTBlbzT7jqpg89p1E1JmUJo2poqLMHU2zobJx40ZOOOEEANatW0dpaSm1tcnD8IsXL6aioqLgufX19dx8881cffXVfX6PD33oQzzyyCO7HOuiRYu46qqruOuuu3b5swaLk80A+uRh+3LioZPZsLmFdY3NrN/UzNpNzaxvbGZdY/J6ecPb/GZTMy1tHd3OlWDC6Eomj6tMe0ZVTB5bxT7p6ynjktdjqoqv/jCz4k2YMIFly5YBcPnll1NTU8NXv/rVruNtbW2UleX/kVlXV0ddXV2/32MgEs1w5WQzwMpLS9h3fDX7jq8u2CYi2LStlbWbmruS0rrGZtalXxve2sbSl97ira2tvc4dXVHKPjnJJzcxdX6dUFNJaYlv25ntqrPPPpuqqiqefPJJ5syZw7x587joootobm6murqan/zkJxx00EHdehqXX345L7/8MqtXr+bll1/my1/+MhdeeCEANTU1NDU1sWjRIi6//HImTpzI008/zZFHHsl//ud/Iom7776biy++mNGjRzNnzhxWr17dZw/mzTff5JxzzmH16tWMGjWK+fPnc9hhh/H73/+eiy66CEjGWB588EGampo4/fTTaWxspK2tjeuuu44Pf/jDg/Jn6WQzBCQxflQF40dV8N4pYwu2a25tT3pFPZLR+rSX9NgLG3l9cwttHd3v25WViEljKtNbdt2TUXL7LvlaVV6a9aWa7ZQrfrmCZ15rHNDPPGTfsXzzLw7d4fMaGhp45JFHKC0tpbGxkYceeoiysjJ+97vf8Y//+I/cfvvtvc557rnneOCBB9i8eTMHHXQQX/ziF3s9k/Lkk0+yYsUK9t13X+bMmcPDDz9MXV0dX/jCF3jwwQeZMWMGZ5xxRr/xffOb3+SII47gzjvv5P777+fMM89k2bJlXHXVVVxzzTXMmTOHpqYmqqqqmD9/PieeeCL/9E//RHt7O1u3bt3hP4+d5WSzG6sqL+VdE0bzrgmjC7Zp7wg2bmlJEtGmdxJRZ1L60/rNPPinDWzZ3t7r3PGjynvdspsyruqdJDW2ivEubrA93Kc//WlKS5NfzDZt2sRZZ53F888/jyRaW3vffQD4xCc+QWVlJZWVlUyaNIn169czbdq0bm1mz57dte/www9nzZo11NTU8O53v7vr+ZUzzjiD+fPn9xnfH/7wh66Ed/zxx7Nx40YaGxuZM2cOF198MZ/5zGc49dRTmTZtGkcddRTnnHMOra2t/OVf/iWHH374Lv3Z7Agnm2GutERMGpMUGBw2rXC7zc2taS+phbWbtnWNI3X2lp5+tZGNW1p6FTdUlpV06xHljiN1JqnaMZWUl7q4wQbOzvRAsjJ69Du/7H3961/nuOOO4xe/+AVr1qzh2GOPzXtOZWVl1+vS0lLa2tp2qs2uuOSSS/jEJz7B3XffzZw5c7j33ns55phjePDBB/nVr37F2WefzcUXX8yZZ545oN+3ECebPcSYqnLGVJVzwKQxBdtsb+vg9c3NXUkpSUbbWNfYwvpNzTzx8lus39TC9vbexQ0TayoLjiN1JqrRlf7nZsPbpk2bmDp1KgA33njjgH/+QQcdxOrVq1mzZg3Tp0/n5z//eb/nfPjDH+anP/0pX//611m0aBETJ05k7NixvPDCC7z//e/n/e9/P0uWLOG5556jurqaadOm8fnPf56WlhaeeOIJJxsbfBVlJUzbaxTT9hpVsE1E8NbW1rRHtK0rKa3f1MzaxmZe3riVxS++yaZtvW8vjKks6zWOtM+4KqbkJKUJoysocXGD7aa+9rWvcdZZZ/Gd73yHT3ziEwP++dXV1Vx77bWcdNJJjB49mqOOOqrfcy6//HLOOeccDjvsMEaNGsVNN90EwA9+8AMeeOABSkpKOPTQQzn55JNZsGAB3//+9ykvL6empoabb755wK+hEEXP+yZGXV1dePG0XbNte3vXbbrOcaSexQ6vb26mR20D5aXJbcFC40hTxlUxaWwllWUubhhpnn32Wd773vcOdRhDrqmpiZqaGiKC888/n5kzZ/KVr3xlqMPqJd/fl6SlEZG3Btw9G8tEdUUpMyaOZsbEvosb3mhqSQoacp5H6ix2eHZtI/c/9zrbWnsXN+w9uqJbZV3nrA2Tx1V3FTeMrS5zcYMNOz/60Y+46aab2L59O0cccQRf+MIXhjqkAeFkY0OmtETsk/Ze2C9/m4igsbktfwl4+tDsU6+8zcYt23udW1VewpRx1ewztvNB2Womj63MGUeqZmJNBWUubrDdyFe+8pXdsiezq5xsbLcmiXHV5YyrLufAfQoXN7S0tfN6Y0u3W3fr0nGk9ZuaqX/pLdY3rqW1vft9uxJB7ZjK3uNIPYodRlX4v4rZrvD/IBsRKstK2W/vUey3d+Hiho6O4M2t2wuOI63esIVHXtjI5ubeJahjq8q6ekSTez6PlH7de3SFb9uZFeBkY3uMkhIxsaaSiTWVvG/quILttrS0dZ9GKGccqfNB2Q2bW3oVN1SUljBpbJ4S8Jxih33GesJV2zNlmmwknQT8O1AKXB8R/9zjeCVwM3AksBE4PSLWpMcuBc4F2oELI+JeSVXAg0BlGvttEfHNtP0FwJeB9wC1EfFGzvc5FvgBUA68EREfyeqabfgbXVnGe2preE9tTcE2be0dbGh6Z+aGzqTUOY709Kub+N2z62lu7eh17sSail7PI+VW2+0zrooxlS5usJEls2QjqRS4BvgY0AAskbQwIp7JaXYu8FZEHCBpHnAlcLqkQ4B5wKHAvsDvJB0ItADHR0STpHLgD5LuiYjHgIeBu4BFPeIYD1wLnBQRL0ualNU1256jrDQpPpgyru8JVxu3tbG2cVveW3evpQ/K5ptwdVRF6TsPxhYYR5roCVcH1HHHHccll1zCiSee2LXvBz/4AStXruS6667Le86xxx7LVVddRV1dHR//+Mf52c9+xvjx47u1yTeDdE933nknBx54IIcccggA3/jGNzjmmGP46Ec/ukvXtDstRZBlz2Y2sCoiVgNIWgDMBXKTzVzg8vT1bcAPlfw6NxdYEBEtwIuSVgGzI+JRoCltX55uARART6bfp2ccfwPcEREvp+1eH8BrNCtIEuNGlTNuVDkHT+57wtXO4oauqYQ2taTJaRuPv/gm6xube024Wto54WpuCXieyVc94WpxzjjjDBYsWNAt2SxYsIDvfe97RZ1/99137/T3vvPOO/nkJz/ZlWy+9a1v7fRn7a6yTDZTgVdy3jcARxdqExFtkjYBE9L9j/U4dyp09ZiWAgcA10TE4/3EcSBQLmkRMAb494jo9dispPOA8wD233//Ii7PbGBUlZey/4RR7D+h7+KGN7a0sL5zGqHOqYTSpPT860089PwbNLX0Lm4YV13erVfU2UvKnefOq8nCaaedxmWXXcb27dupqKhgzZo1vPbaa3z4wx/mi1/8IkuWLGHbtm2cdtppXHHFFb3Onz59OvX19UycOJHvfve73HTTTUyaNIn99tuPI488EkieoZk/fz7bt2/ngAMO4JZbbmHZsmUsXLiQ3//+93znO9/h9ttv59vf/jaf/OQnOe2007jvvvv46le/SltbG0cddRTXXXcdlZWVTJ8+nbPOOotf/vKXtLa28t///d8cfPDBBa9vqJciGHYFAhHRDhye3h77haT3RcTTfZxSRjImdAJQDTwq6bGI+FOPz50PzIdkBoFsojfbOSU5E66+n8LFDU0tbd3GkbpKwNPbd8+sbeSNpt4TrlaUlXTdsussauhZ5DBpMCdcvecSWPfHgf3Mye+Hk/+54OG9996b2bNnc8899zB37lwWLFjAX//1XyOJ7373u+y99960t7dzwgknsHz5cg477LC8n7N06VIWLFjAsmXLaGtr4wMf+EBXsjn11FP5/Oc/D8Bll13GDTfcwJe+9CVOOeWUruSSq7m5mbPPPpv77ruPAw88kDPPPJPrrruOL3/5ywBMnDiRJ554gmuvvZarrrqK66+/vuD1DfVSBFkmm1fp/qjetHRfvjYNksqAcSSFAv2eGxFvS3oAOAnoK9k0ABsjYguwRdKDwCzgT32cYzYs1VSWccCkGg6YVLi4obW9g9c3txQsAV/2ytusW9HM9gKryb5zy64yfWi2+wwONcN4wtXOW2mdyeaGG24A4NZbb2X+/Pm0tbWxdu1annnmmYLJ5qGHHuJTn/oUo0YlPdVTTjml69jTTz/NZZddxttvv01TU1O3W3b5rFy5khkzZnDggQcCcNZZZ3HNNdd0JZtTTz0VgCOPPJI77rijz88a6qUIsvxXsQSYKWkGSaKYRzJ+kmshcBbwKHAacH9EhKSFwM8k/StJgcBMYLGkWqA1TTTVJMUHV/YTx/+QjAWVARUkt/L+bUCu0GwYKi8tYer4aqb2s5rs21tbu83YkNtjanhrK/UvvcnbeYobairLklkbxlUxeWx1zlLn1eltvEomjq7se8LVPnogWZo7dy5f+cpXeOKJJ9i6dStHHnkkL774IldddRVLlixhr7324uyzz6a5uXmnPv/ss8/mzjvvZNasWdx4440sWrRol+LtXKZgV5YoGKylCDJLNukYzAXAvSSlzz+OiBWSvgXUR8RC4AbglrQA4E2ShETa7laSYoI24PyIaJc0BbgpHbcpAW6NiLsAJF0IfA2YDCyXdHdEfC4inpX0a2A50EFSgt1XT8hsjyeJvUZXsNfovleT3ba9vWtOu66F+3J6TI+88Aavb26hPc9qsslzR+9MH3TytHbe3rqdstISyktFeUnJoM8AXlNTw3HHHcc555zTtUpmY2Mjo0ePZty4caxfv5577rmn4Do2AMcccwxnn302l156KW1tbfzyl7/smt9s8+bNTJkyhdbWVn760592LVcwZswYNm/e3OuzDjroINasWcOqVau6xng+8pGde3JjqJciyLS/GxF3A3f32PeNnNfNwKcLnPtd4Ls99i0HjijQ/mrg6gLHvg98f0diN7P+VVeUMn3iaKb3M+Hqxs4JV3PGkTp7Sc+t28yilRv40IRJvPxm97GBshKlySdNQDlfy0pLKC8RpSUa0OKGM844g0996lMsWLAAgFmzZnHEEUdw8MEHs99++zFnzpw+z//ABz7A6aefzqxZs5g0aVK3ZQK+/e1vc/TRR1NbW8vRRx/dlWDmzZvH5z//ea6++mpuu+22rvZVVVX85Cc/4dOf/nRXgcDf/d3f7dR1DfVSBF5iIA8vMWA2uCKCZ559lvfMPIjW9g5a24PW9g7acl63tgdtHb0fki2RKC/Nk5R6JKo9vdpuoHmJATMbdiRRIlFVXtrnc0EdEb0SUG5S2rq9jdb2oOcv0YKu23NlJSWUlyXJqDMRdSYlPySbHScbMxs2SiQqykrpaxLuiKC9o3syam1Pk1RHsL29gy3b23qNI0HyoGx5SQllXbfsut++KystoWyAb9vtKZxszGy3ERG7/INcEmVpYihcb5eMJSU9oiQJdUtK7UFTSxtt7R30TEmSunpFfSWlkhGckHZm+MXJxsx2C1VVVWzcuJEJEyYMSs+htESUlpRS2cdtu4igrT1o7ejocdsu6SE1t3awubmNjjw/fMtKcosZeielslJRquHXS4oINm7cSFVV1Q6d52RjZruFadOm0dDQwIYNG4Y6lB1SCii9dde1pe87uu3rfW6JkqRXIqXJL91y3pdI7G75qKqqimnTpu3QOU42ZrZbKC8vZ8aMGUMdRmZyV5NduylnvaRNzaxLZwZ/fXNzr9VkS0tEbU1lMqfd2JyphMZVpg/NJjM4VFfs3hOuOtmYmQ2CYleT3bhle7fpg3LnuVu1oYmHV73B5jwTro6tKkumDxpXxeSxlV0zNkwe1zkzePWQTrjqZGNmtpsoKRG1YyqpHVPcarI9J13tnOfuubWNbCgw4eo+Y3OnD6rsmvm7c867SWOyWU3WycbMbJgpZjXZ1vYONmxu6baC7PrGd3pLyxve5jebmmnpMeHquX8+g69/8pABj9nJxsxsBCovLWHf8dXs28+Eq5u2tXYbR5q5z5hM4nGyMTPbQ0li/KgKxo+q6HM12YEwSCshmZnZnszJxszMMudkY2ZmmXOyMTOzzDnZmJlZ5pxszMwsc042ZmaWOScbMzPLXKbJRtJJklZKWiXpkjzHKyX9PD3+uKTpOccuTfevlHRiuq9K0mJJT0laIemKnPYXpO1D0sSc/cdK2iRpWbp9I8trNjOz3jKbQUBSKXAN8DGgAVgiaWFEPJPT7FzgrYg4QNI84ErgdEmHAPOAQ4F9gd9JOhBoAY6PiCZJ5cAfJN0TEY8BDwN3AYvyhPNQRHwymys1M7P+ZNmzmQ2siojVEbEdWADM7dFmLnBT+vo24AQl81/PBRZEREtEvAisAmZHoiltX55uARART0bEmgyvx8zMdlKWyWYq8ErO+4Z0X942EdEGbAIm9HWupFJJy4DXgd9GxONFxPJn6a23eyQdmq+BpPMk1UuqH24rBZqZ7e6GXYFARLRHxOHANGC2pPf1c8oTwLsiYhbwH8CdBT53fkTURURdbW3twAZtZraHyzLZvArsl/N+WrovbxtJZcA4YGMx50bE28ADwEl9BRERjZ233iLibqA8t4DAzMyy12+ykfQXknYmKS0BZkqaIamCZMB/YY82C4Gz0tenAfdHRKT756XVajOAmcBiSbWSxqdxVZMUHzzXT/yT03EgJM0mueaNO3E9Zma2k4pJIqcDz0v6nqSDi/3gdAzmAuBe4Fng1ohYIelbkk5Jm90ATJC0CrgYuCQ9dwVwK/AM8Gvg/IhoB6YAD0haTpLMfhsRdwFIulBSA0kvaLmk69PvcRrwtKSngKuBeWlCMzOzQaJifu5KGgucAXyWpPrrJ8B/RcTmbMMbGnV1dVFfXz/UYZiZDSuSlkZEXb5jRd0ei4hGktLkBSS9i08BT0j60oBFaWZmI1YxYzanSPoFycOS5STPu5wMzAL+V7bhmZnZSFDMDAJ/BfxbRDyYuzMitko6N5uwzMxsJOk32UTEWWlF1ykk4zVLImJdeuy+rAM0M7Phr5jbaOcCi4FTSSq7HpN0TtaBmZnZyFHMbbSvAUdExEYASROAR4AfZxmYmZmNHMVUo20EckucN+OHIs3MbAcU07NZBTwu6X9Ixmzmkjw0eTFARPxrhvGZmdkIUEyyeSHdOv1P+nXMwIdjZmYjUTHVaFcASKpJ3zf1fYaZmVl3xVSjvU/Sk8AKYIWkpYXWhDEzM8unmAKB+cDFEfGuiHgXyawBP8o2LDMzG0mKSTajI+KBzjcRsQgYnVlEZmY24hRTILBa0teBW9L3/y+wOruQzMxspCmmZ3MOUAvcAdwOTEz3mZmZFaXPno2kUuCOiDhukOIxM7MRqM+eTbo6ZoekcYMUj5mZjUDFjNk0AX+U9FtgS+fOiLgws6jMzGxEKSbZ3JFuufpfS9rMzCxVTLIZHxH/nrtD0kUZxWNmZiNQMdVoZ+XZd3YxHy7pJEkrJa2SdEme45WSfp4ef1zS9Jxjl6b7V0o6Md1XJWmxpKckrZB0RU77C9L2IWlinu91lKQ2SacVE7uZmQ2cgj0bSWcAfwPMkLQw59AY4M3+PjitZLsG+BjQACyRtDAinslpdi7wVkQcIGkecCVwuqRDgHnAocC+wO8kHQi0AMdHRJOkcuAPku6JiMeAh4G7gEUFYrkS+E1/cZuZ2cDr6zbaI8Bakudq/iVn/2ZgeRGfPRtYFRGrASQtIFmeIDfZzAUuT1/fBvxQktL9CyKiBXhR0ipgdkQ8SlKwAFCebgEQEU+m3ydfLF8ieUboqCLiNjOzAVYw2UTES8BLwJ/t5GdPBV7Jed8AHF2oTUS0SdoETEj3P9bj3KnQ1UtZChwAXBMRj/cVhKSpwKeA4+gj2Ug6DzgPYP/99+/n0szMbEcUM+vzqZKel7RJUqOkzZIaByO4fCKiPSIOB6YBsyW9r59TfgD8Q0R09PO58yOiLiLqamtrBypcMzOjuGq07wF/ERHP7uBnvwrsl/N+WrovX5sGSWXAOJIlp/s9NyLelvQAcBLwdB9x1AEL0ttrE4GPS2qLiDt38HrMzGwnFVONtn4nEg3AEmCmpBmSKkgG/Bf2aLOQd6rdTgPuj4hI989Lq9VmADOBxZJqJY0HkFRNUnzwXF9BRMSMiJgeEdNJxoX+3onGzGxwFdOzqZf0c+BOkmowACKi54Oe3aRjMBcA9wKlwI8jYoWkbwH1EbEQuAG4JS0AeJMkIZG2u5WkmKANOD8i2iVNAW5Kx21KgFsj4i4ASRcCXwMmA8sl3R0Rnyv+j8LMzLKipCPRRwPpJ3l2R0SM2Jmf6+rqor6+fqjDMDMbViQtjYi6fMf67dlExGcHPiQzM9uTFFONdqCk+yQ9nb4/TNJl2YdmZmYjRTEFAj8CLgVaASJiOenYipmZWTGKSTajImJxj31tWQRjZmYjUzHJ5g1J7yGdFiadyHJtplGZmdmIUkzp8/nAfOBgSa8CLwKfyTQqMzMbUYqpRlsNfFTSaKAkIjZnH5aZmY0kxfRsAIiILf23MjMz662YMRszM7Nd4mRjZmaZK+ahzk9LGpO+vkzSHZI+kH1oZmY2UhTTs/l6RGyW9OfAR0kmz7wu27DMzGwkKSbZtKdfPwHMj4hfARXZhWRmZiNNMcnmVUn/P3A6cLekyiLPMzMzA4pLGn9NsibNiRHxNrA38L8zjcrMzEaUYp6zmQL8KiJaJB0LHAbcnGlUZmY2ohTTs7kdaJd0AMm0NfsBP8s0KjMzG1GKSTYdEdEGnAr8R0T8b5LejpmZWVGKSTatks4AzgTuSveVZxeSmZmNNMUkm88CfwZ8NyJelDQDuCXbsMzMbCTpN9lExDPAV4E/Snof0BARVxbz4ZJOkrRS0ipJl+Q5Xinp5+nxxyVNzzl2abp/paQT031VkhZLekrSCklX5LS/IG0fkibm7J8rabmkZZLq04dTzcxsEBUzXc2xwPPANcC1wJ8kHVPEeaXpOScDhwBnSDqkR7Nzgbci4gDg34Ar03MPIVl6+lDgJODa9PNagOMjYhZwOHCSpA+mn/UwyQwHL/X4HvcBsyLicOAc4Pr+Yjczs4FVzG20fwH+n4j4SEQcA5xIkhj6MxtYFRGrI2I7sACY26PNXOCm9PVtwAmSlO5fEBEtEfEisAqYHYmmtH15ugVARDwZEWt6BhERTRER6dvRne3NzGzwFJNsyiNiZeebiPgTxRUITAVeyXnfkO7L2yateNsETOjrXEmlkpYBrwO/jYjH+wtE0qckPQf8iqR3k6/NeelttvoNGzYUcXlmZlasYpLNUknXSzo23X4E1GcdWCER0Z7eEpsGzE7Hkfo75xcRcTDwl8C3C7SZHxF1EVFXW1s7sEGbme3hikk2fwc8A1yYbs8AXyzivFdJHgDtNC3dl7eNpDJgHLCxmHPTqXMeIBnTKUpEPAi8O7eAwMzMstdnskkH5Z+KiH+NiFPT7d8ioqWIz14CzJQ0Q1IFyYD/wh5tFgJnpa9PA+5Px1cWAvPSarUZwExgsaRaSePT2KqBjwHP9XMNB6TjQKTr8FSSJDQzMxskfc6NFhHtaenx/hHx8o58cES0SbqAZBLPUuDHEbFC0reA+ohYSLI2zi2SVgFvkiQk0na3kvSi2oDz01imADelSbAEuDUi7gKQdCHwNWAysFzS3RHxOeCvgDMltQLbgNNzCgbMzGwQqL+fu5IeBI4AFgNbOvdHxCnZhjZ06urqor5+yIalzMyGJUlLI6Iu37FiZn3++gDHY2Zme5hiks3LwNqIaIausZJ9Mo3KzMxGlGKq0f4b6Mh5357uMzMzK0oxyaYsnQEAgPR1RXYhmZnZSFNMstkgqasYQNJc4I3sQjIzs5GmmDGbvwN+KumH6fsG4G+zC8nMzEaafpNNRLwAfFBSTfq+qZ9TzMzMuimmZwM4yZiZ2c4rZszGzMxslzjZmJlZ5opZqXOppPMl7TUYAZmZ2chTTM/mdGBfYImkBZJO7JxF2czMrBj9JpuIWBUR/wQcCPwM+DHwkqQrJO2ddYBmZjb8FTVmI+kw4F+A7wO3A58GGoH7swvNzMxGin5LnyUtBd4mWXvmkpyF0x6XNCfL4MzMbGToM9lIKgFuj4j/L9/xiDg1k6jMzGxE6fM2WkR0AE4oZma2S4oZs/mdpK9K2k/S3p1b5pGZmdmIUcx0NaenX8/P2RfAuwc+HDMzG4mKKX2ekWcrKtFIOknSSkmrJF2S53ilpJ+nxx+XND3n2KXp/pWSTkz3VUlaLOkpSSskXZHT/oK0fUiamLP/M5KWS/qjpEckzSomdjMzGzhFTcQp6X3AIUBV576IuLmfc0qBa4CPkSxLsETSwoh4JqfZucBbEXGApHnAlcDpkg4B5gGHkjxQ+jtJBwItwPER0SSpHPiDpHsi4oHvgYEAABKvSURBVDHgYeAuYFGPUF4EPhIRb0k6GZgPHF3MdZuZ2cAoZrqabwL/kW7HAd8DTunzpMRsYFVErE5X91wAzO3RZi5wU/r6NuCEdHaCucCCiGiJiBeBVcDsSHTOPl2ebgEQEU9GxJqeQUTEIxHxVvr2MWBaEbGbmdkAKqZA4DTgBGBdRHwWmAWMK+K8qcArOe8b0n1520REG7AJmNDXuZJKJS0DXgd+GxGPFxFLp3OBe3agvZmZDYBiks22tAS6TdJYkh/y+2UbVmER0R4Rh5P0UGant/j6Jek4kmTzDwWOnyepXlL9hg0bBi5gMzMrKtnUSxoP/AhYCjwBPFrEea/SPSlNS/flbSOpjKTHtLGYcyPibeAB4KT+Akmn27kemBsRG/O1iYj5EVEXEXW1tbX9faSZme2AYqrR/j4i3o6I/0My2H9WejutP0uAmZJmSKogGfBf2KPNQuCs9PVpwP0REen+eWm12gxgJrBYUm2a+JBUncbzXF9BSNofuAP424j4UxFxm5nZACu2Gm0q8K7O9pKOiYgH+zonItokXQDcC5QCP46IFZK+BdRHxEKS+dZukbQKeJMkIZG2uxV4BmgDzo+IdklTgJvSSrcS4NaIuCuN6ULga8BkYLmkuyPic8A3SMaBrk1XRmiLiLpi/4DMzGzXKelI9NFAupLkwc5ngPZ0d0REMRVpw1JdXV3U19cPdRhmZsOKpKWFfpkvpmfzl8BBObM9m5mZ7ZBiCgRWkzzPYmZmtlOK6dlsBZZJuo/kCX4AIuLCzKIyM7MRpZhks5DeVWRmZmZF6zfZRMRN/bUxMzPrS8FkI+nWiPhrSX8knX8sV0QclmlkZmY2YvTVs7ko/frJwQjEzMxGroLJJiLWpl9f6tyXrhOzMfp7OMfMzCxHwdJnSR+UtEjSHZKOkPQ08DSwXlK/85GZmZl16us22g+BfySZHPN+4OSIeEzSwcB/Ab8ehPjMzGwE6OuhzrKI+E1E/DfJWjaPAUREnxNfmpmZ9dRXsunIeb2txzGP2ZiZWdH6uo02S1IjIKA6fU36virzyMzMbMToqxqtdDADMTOzkauYiTjNzMx2iZONmZllzsnGzMwy52RjZmaZc7IxM7PMOdmYmVnmMk02kk6StFLSKkmX5DleKenn6fHHJU3POXZpun+lpBPTfVWSFkt6StIKSVfktL8gbR/phKGd+w+W9KikFklfzfJ6zcwsv8ySjaRS4BrgZOAQ4AxJh/Rodi7wVkQcAPwbcGV67iHAPOBQ4CTg2vTzWoDjI2IWcDhwkqQPpp/1MPBR4KXu34I3gQuBqwb2Cs3MrFhZ9mxmA6siYnVEbAcWAHN7tJkLdK4EehtwgiSl+xdEREtEvAisAmZHoiltX55uARART0bEmp5BRMTrEbEEaB3YyzMzs2JlmWymAq/kvG9I9+VtExFtwCZgQl/nSiqVtAx4HfhtRDw+EMFKOk9SvaT6DRs2DMRHmplZatgVCEREe0QcDkwDZkt63wB97vyIqIuIutra2oH4SDMzS2WZbF4F9st5Py3dl7eNpDKStXM2FnNuRLwNPEAypmNmZruxLJPNEmCmpBmSKkgG/Bf2aLMQOCt9fRpwf7rk9EJgXlqtNgOYCSyWVCtpPICkauBjgNfXMTPbzfW1xMAuiYg2SRcA9wKlwI8jYoWkbwH1EbEQuAG4RdIqkqqxeem5KyTdCjwDtAHnR0S7pCnATWllWglwa0TcBSDpQuBrwGRguaS7I+JzkiYD9cBYoEPSl4FDIqKRgdZQD49cDWP2hbFTkq9jJsPYfWHMFKgYNeDf0sxsOFDSkbBcdXV1UV9fv+MnPv9b+PWlsHktbG/qfbxqXJJ0xkx5JwGNndJ93+haKPHqDmY2/EhaGhF1+Y5l1rPZI838WLIBNDcmSWfzWmhcC5tfS7+m2wsroWk9RHv3z1Bp0hsaM7l7UupKTGmvqXLM4F+fmdlOcrLJStXYZKs9qHCbjnZoej0nKb2Wvl6XvN64Cl58CFo29T63oqZ3AurZa6rZB0r9V2xmQ88/iYZSSWmSJMZO6bvd9i3vJKCupLTund7SSw8n+zvaepwoqJlUoIfUuW8yVI0HKbPLNDNzshkOKkbDhPckWyEdHbB1Y87tute6J6i3XoKXH4Vtb/U+t6y6Rw9pcu8ihzFToKwiu2s0sxHNyWakKCmBmtpkmzKrcLvW5h637dZ1v4XXsCRJVu0tvc8dNTF/lV1nD2nMvjBqb/eSzKwXJ5s9TXkV7D0j2QqJSHpAXWNIPYscXoPXnoAteab1Ka3MSUST84wnpV/Lq7O7RjPb7TjZWG9S0kMZtTdM7mM2oLbt0LSue5Vd17jSWli7HP50L7Ru7X1u9V45PaQCRQ6jJiY9NjMb9pxsbOeVVcD4/ZOtkAho3tR3Gfj6FbDldYiO7ueWlBdXBl4xOtvrNLNd5mRj2ZKgenyyTXpv4XbtbclzR7lVdrlJacNz8MIDsH1z73Mrx/XRQ0r31Uzyw7JmQ8jJxnYPpWUwbmqycWThdi2b85SB5/Sa3vh9crzXw7IlyXNH3XpIPYsc0odlXeBgNuCcbGx4qRyTbBNnFm7T0Q5b3sjTQ0p7TRtfgDUPJbf3eiof3fs5pNwy8LGdD8uWZ3eNZiOQk42NPCWlMGafZNv3iMLttm/tMZbUo8jhpUfTh2V7LvKqZA67vGXgOfuq93IvySzlZGN7ropRxT0su+3NwmXgm16BhsXJA7U9lVX3KAOfkufZpClQVpndNZrtJpxszPpSUgKjJybblMMKt2trKdxDalwLrz6RvG5r7n3uqAlFlIFPcC/JhjUnG7OBUFYJe01PtkI6H5bteesut9e09qn0YdkeS3+UVrzTE+prrjs/LGu7KScbs8GS+7DsPocWbtfemhYzFCgDX/90snZS65be51aN72M28HTf6Fo/LGuDzsnGbHdTWg7j90u2QiKgpTH/bbvO0vDXn03XTOr5sGxZThl4H0tUVNZke522R3GyMRuOpGTl16pxMOngwu3a25Lbcrk9o9wlKjb8CVb/PklcPVWM6VEGnmf2htGTvGaSFcX/SsxGstKyd9ZMmtpHu5am3rftch+effGhZB68nmsmqSRJOP0tUVE1zgUOezgnGzNLbplVHgATDyjcpqMDtr5ReBG/t16Elx/Jv2ZS+ajeJd89y8BrJnvNpBEs02Qj6STg34FS4PqI+OcexyuBm0nmJ9kInB4Ra9JjlwLnAu3AhRFxr6Qq4EGgMo39toj4Ztr+AuDLwHuA2oh4I92vNIaPA1uBsyPiiSyv22xEKilJ5pirmQQcXrhd67Y8ZeA5RQ6vLE72tW/vfe7o2sKzN3Q+s+SHZYelzJKNpFLgGuBjQAOwRNLCiHgmp9m5wFsRcYCkecCVwOmSDgHmAYcC+wK/k3Qg0AIcHxFNksqBP0i6JyIeAx4G7gIW9QjlZGBmuh0NXJd+NbMslFfD3u9OtkIiYOubvWcA7+o1vQoN9UlPqqeyqvxl4LlFDmOmJGs32W4jy57NbGBVRKwGkLQAmAvkJpu5wOXp69uAH6Y9kbnAgohoAV6UtAqYHRGPAk1p+/J0C4CIeDL9Pj3jmAvcHBEBPCZpvKQpEbF2IC/WzHaABKMnJNvk9xdu19byzmqyvZaoWAdrl8HKe6BtW+9zq/fu/RxSzyKHURNcBj5Iskw2U4FXct430LtH0dUmItokbQImpPsf63HuVOjqMS0FDgCuiYjHdyKOqUC3ZCPpPOA8gP3372N9FjMbPGWVsNe7kq2QCGh+O/9tu84ih3V/hKbX6fWwbNeaSf2UgVeMyvQy9wTDrkAgItqBwyWNB34h6X0R8fQAfO58YD5AXV1d9NPczHYXUjKOU70X7HNI4Xbtre+smZSvyGH9M7DqPtje1PvcynF5ekg9lqgYXes1k/qQZbJ5Fch9Km1aui9fmwZJZcA4kkKBfs+NiLclPQCcBPSVbIqJw8xGutJyGDct2frS3Nh3GfgLK9OHZXuumVSaPCzb1xIVYyZD1djsrnE3lmWyWQLMlDSD5If7POBverRZCJwFPAqcBtwfESFpIfAzSf9KUiAwE1gsqRZoTRNNNUnxwZX9xLEQuCAdMzoa2OTxGjMrqGpsstUeWLhNR3vysGyvHlL6euOq5NmkljxrJlXU9F6OIl8Z+Ah7WDazq0nHYC4A7iUpff5xRKyQ9C2gPiIWAjcAt6QFAG+SJCTSdreSFBO0AedHRLukKcBN6bhNCXBrRNwFIOlC4GvAZGC5pLsj4nPA3SRlz6tISp8/m9U1m9keoqQ07bVM7rvd9i3de0U9ixxeejg5nm/NpJpJBXpIOUUOVeOHTRm4kiIty1VXVxf19fVDHYaZ7Qk6OpL1kAqWgafvt73Z+9yy6h49pDxFDmOmDNrDspKWRkRdvmMjq59mZjbclJRATW2yTZlVuF1r8zuJKN8SFa/Ww7Nrob2l97mjJuavsutWBr53pr0kJxszs+GgvAr2npFshXSumdRzKqHcXtNrT6ZrJvVQWpn0jmafBx+6YMDDd7IxMxspctdMmvy+wu3aticTq+YrA6/ZJ5PQnGzMzPY0ZRUwfv9kGySep8HMzDLnZGNmZplzsjEzs8w52ZiZWeacbMzMLHNONmZmljknGzMzy5yTjZmZZc4TceYhaQPw0k6ePhHIs3D6iOZr3jP4mvcMu3LN74qI2nwHnGwGmKT6QrOejlS+5j2Dr3nPkNU1+zaamZllzsnGzMwy52Qz8OYPdQBDwNe8Z/A17xkyuWaP2ZiZWebcszEzs8w52ZiZWeacbHaSpB9Lel3S0wWOS9LVklZJWi7pA4Md40Aq4no/k17nHyU9IqmPxdSHh/6uOafdUZLaJJ02WLFlpZhrlnSspGWSVkj6/WDGl4Ui/m2Pk/RLSU+l1/zZwY5xoEnaT9IDkp5Jr+miPG0G9GeYk83OuxE4qY/jJwMz0+084LpBiClLN9L39b4IfCQi3g98m5ExsHojfV8zkkqBK4HfDEZAg+BG+rhmSeOBa4FTIuJQ4NODFFeWbqTvv+fzgWciYhZwLPAvkioGIa4stQH/KyIOAT4InC/pkB5tBvRnmJPNToqIB4E3+2gyF7g5Eo8B4yVNGZzoBl5/1xsRj0TEW+nbx4BpgxJYhor4Owb4EnA78Hr2EWWviGv+G+COiHg5bT/sr7uIaw5gjCQBNWnbtsGILSsRsTYinkhfbwaeBab2aDagP8OcbLIzFXgl530Dvf8yR6pzgXuGOoisSZoKfIrh32vdEQcCe0laJGmppDOHOqBB8EPgvcBrwB+BiyKiY2hDGjiSpgNHAI/3ODSgP8PKdvZEs3wkHUeSbP58qGMZBD8A/iEiOpJfevcIZcCRwAlANfCopMci4k9DG1amTgSWAccD7wF+K+mhiGgc2rB2naQakp75l7O+Hieb7LwK7Jfzflq6b8SSdBhwPXByRGwc6ngGQR2wIE00E4GPS2qLiDuHNqxMNQAbI2ILsEXSg8AsYCQnm88C/xzJQ4mrJL0IHAwsHtqwdo2kcpJE89OIuCNPkwH9GebbaNlZCJyZVnR8ENgUEWuHOqisSNofuAP42xH+W26XiJgREdMjYjpwG/D3IzzRAPwP8OeSyiSNAo4mud8/kr1M0pND0j7AQcDqIY1oF6XjTzcAz0bEvxZoNqA/w9yz2UmS/oukMmWipAbgm0A5QET8H+Bu4OPAKmAryW9Hw1YR1/sNYAJwbfqbfttwny23iGsecfq75oh4VtKvgeVAB3B9RPRZGr67K+Lv+dvAjZL+CIjk1ulwX3ZgDvC3wB8lLUv3/SOwP2TzM8zT1ZiZWeZ8G83MzDLnZGNmZplzsjEzs8w52ZiZWeacbMzMLHNONmZDQFJ7OnNy53bJAH729P5mqjYbbH7OxmxobIuIw4c6CLPB4p6N2W5E0hpJ30vXBVos6YB0/3RJ96frityXztiApH0k/SJda+UpSR9KP6pU0o/StUp+I6l6yC7KDCcbs6FS3eM22uk5xzal6wL9kGSyT4D/AG6KiMOAnwJXp/uvBn6frrXyAWBFun8mcE265szbwF9lfD1mffIMAmZDQFJTRNTk2b8GOD4iVqcTJa6LiAmS3gCmRERrun9tREyUtAGYFhEtOZ8xHfhtRMxM3/8DUB4R38n+yszyc8/GbPcTBV7viJac1+14fNaGmJON2e7n9Jyvj6avHwHmpa8/AzyUvr4P+CIkS1RLGjdYQZrtCP+2YzY0qnNm2wX4dUR0lj/vJWk5Se/kjHTfl4CfSPrfwAbemYH3ImC+pHNJejBfBEbsUhY2fHnMxmw3ko7Z1I2AKezNuvFtNDMzy5x7NmZmljn3bMzMLHNONmZmljknGzMzy5yTjZmZZc7JxszMMvd/AQh2jWoM5QdEAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zuhx9G99Qwrp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "ff4ab076-f7f8-4b2e-dbfd-d2a330434074"
      },
      "source": [
        "plt.plot(range(1, NUM_EPOCHS+1), train_acc_lst, label='Training accuracy')\n",
        "plt.plot(range(1, NUM_EPOCHS+1), valid_acc_lst, label='Validation accuracy')\n",
        "plt.legend(loc='upper left')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()\n",
        "print(f'Training accuracy: {train_acc_lst[-1]:.2f}%')\n",
        "print(f'Validation accuracy: {valid_acc_lst[-1]:.2f}%')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Training accuracy: 0.00%\n",
            "Validation accuracy: 0.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63zWuzIsYLvW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "b4526018-443f-4136-fa18-7905123a9c2e"
      },
      "source": [
        "test_acc, test_loss, test_f_measure = compute_accuracy_and_loss(model, valid_loader, DEVICE)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([122, 4250, 300])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0nW19O9Qwrt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "4d97c6dd-4695-44cb-b845-5d7e831dd014"
      },
      "source": [
        "model.eval()\n",
        "with torch.set_grad_enabled(False): # save memory during inference\n",
        "    test_acc, test_loss, test_f_measure = compute_accuracy_and_loss(model, test_loader, DEVICE)\n",
        "    print(f'Test accuracy: {test_acc:.2f}%')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([128, 4250, 300])\n",
            "torch.Size([122, 4250, 300])\n",
            "Test accuracy: 0.00%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKc4w-vAQwrx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}